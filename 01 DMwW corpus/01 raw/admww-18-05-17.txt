<-- 1.0 Todo -->
Time series forecasting 
Introduction
This course aims to lift you to the "wizard" level of skill in data mining with Weka. You'll learn to use a variety of popular packages that extend Weka's capabilities in a wide range of areas.
1.1
What will you learn?
video (03:39)
1.2
About this course
article
1.3
Welcome! Please introduce yourself
discussion
1.4
Are you ready for this? 
quiz
1.5
Rogues gallery
article
How can you use data mining to foretell the future?
Each week we’ll focus on a “Big Question” relating to data mining. This is the Big Question for this week.
1.6
How can you use data mining to foretell the future?
article
Time series: linear regression with lags 
The simplest kind of forecasting is linear regression, "Date" attributes must be normalized in order to interpret the linear model. Linear predictions can be improved by adding lagged copies of variables.
1.7
Time series: linear regression with lags 
video (11:10)
1.8
Time series forecasting with SMO 
quiz
Using the time series forecasting package 
The time series forecasting package automatically produces lots of lagged variables
... and lots of other variables too, which can lead to overfitting. Beware of evaluation based on training data: simpler models can be better!
1.9
Using the time series forecasting package 
video (09:48)
1.10
Evaluating on a test set 
quiz
Looking at forecasts 
Weka's time series forecasting package includes many options for visualizing predictions. It can graph future predictions for any number of steps ahead, and show comparisons of predictions for different numbers of steps
1.11
Looking at forecasts
video (08:42)
1.12
Looking at the textual output
quiz
Lag creation and overlay data 
There are many different parameters and options for deriving time-dependent attributes.
More than one target can be predicted, and "Overlay data" can be specified whose future values are known.
1.13
Lag creation and overlay data 
video (09:20)
1.14
How do you find stock market data?
article
1.15
Forecasting wine sales 
quiz
Analyzing infrared data from soil samples 
Here are six challenges for data mining applications. Inferring properties of soil samples from infrared data can save significant dollars.
Remember, data mining is an experimental science!
1.16
Analyzing infrared data from soil samples 
video (10:37)
1.17
Challenges for machine learning applications
discussion
1.18
Analyzing a soil sample 
quiz
1.19
Avenues for further investigation
article
1.20
Reflect on this week’s Big Question
discussion
1.21
Index
article

<-- 1.1 Video -->
What will you learn?
This video welcomes you to the course, which – unlike earlier courses in this series – is given by the entire data mining team at the University of Waikato in NZ. It summarizes what you will learn. As well as a host of new techniques, the course also includes descriptions of some fielded applications of Weka. Ian reminds you what you surely know already, that New Zealand is at the top of the world, and has a cool bird called a weka.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello and welcome to Advanced Data Mining with Weka. I’m Ian Witten, and I’m going to be giving some of the lessons in this course, which is brought to you by the entire data mining team at the University of Waikato in New Zealand. This is the third in a series of online courses on practical data mining and the use of the Weka workbench, and this course is going to look at the use of some popular packages in Weka, specialist packages for specialist jobs in data mining. And also, we’re going to look at some specific application areas in this course to give some example applications.
As you know, a weka is a bird found only on the islands of New Zealand, but, as far as this course is concerned, it’s a data mining workbench – the Waikato Environment for Knowledge Analysis – that contains a bunch of machine learning algorithms for various data mining tasks like classification, preprocessing, feature selection, clustering, association rule mining – things like that. So what are you going to learn in this course? Well, we’re going to look at the time series forecasting package and how to do time series forecasting with Weka. We’re going to look
at data stream mining: incremental classifiers in Weka, and we’re looking at the MOA system for Massive Online Analysis. There’s a MOA package for Weka, which we’ll install and look at, and then also the MOA system itself, you’ll install that – it’s got a very similar interface to Weka – and look at some its facilities for stream-oriented data mining. We’re going to be looking at Weka’s interface to the R data mining system. You can use facilities in R, which is a pretty advanced data mining system from Weka, getting all that extra functionality right inside your Weka. We’re going to look at distributed processing using the Apache SPARK system. We’re going to be looking at scripting Weka in Python.
There’s a package which allows you to script Weka right from the Explorer. You can write little Python scripts, and also you can install the Python Weka wrapper, where you install a full version of Python with access to all of the things that Python gives you access to plus Weka besides. And we’re going to look at some applications. We’re going to look at analyzing soil samples. We’re going to look at neuroimaging with functional MRI data. We’re going to look at classifying tweets and classifying images and signal peptide prediction. The aim of this course is to equip you to use Weka on your own data and most importantly to understand what it is that you’re doing. This is the team.
These are the Weka people at Waikato, and you’ll meet all of these people as we go through this course. They’re all experienced Weka users. They’ve lived with Weka for many years. Before I go, let me just show you that this is where I am at the moment. I’m sitting here in New Zealand. This is the world as we see it. New Zealand’s at the top, in the middle, where the red arrow is, and you’re probably down at the bottom somewhere. This is where Weka is from. I’ve turned this map of New Zealand around so that north is at the top, which is probably more familiar to you, and we’re right there.
The home of Weka at the University of Waikato in the center of the North Island of New Zealand. I’m really looking forward to giving this course, and looking forward to seeing you in the next lesson. Bye for now!
<End Transcript>

<-- 1.2 Article -->
About this course
This course will lift you to the wizard level of skill in data mining with Weka.
It shows you how to use popular packages that extend Weka’s functionality, following on from Data Mining with Weka and More Data Mining with Weka. You’ll learn about forecasting time series and mining data streams. You’ll connect up the popular R statistical package and learn how to use its extensive visualisation and preprocessing functions from Weka. You’ll script Weka in Python – all from within the friendly Weka interface. And you’ll learn how to distribute data mining jobs over several computers using Apache SPARK.
Course structure
Teachers open the door. You enter by yourself. (Chinese proverb)
This is structured as a five week course:
  Week 1: Time series forecasting
  Week 2: Data stream mining
  Week 3: Reaching out to other data mining packages
  Week 4: Distributed processing
  Week 5: Scripting Weka
In addition to these topics, and in response to popular demand, at the end of each week we describe an actual application of Weka (not necessarily relating to that week’s topic):
  Analyzing infrared data from soil samples
  Signal peptide prediction
  Analyzing functional MRI neuroimaging data
  Processing images with different feature sets
  Data mining challenges
Each week focuses on a “Big Question.” For example, Week 1’s is: How can you use data mining to foretell the future? The week includes a handful of activities that together address the question. Each activity comprises:
  5-10 minute video
  Quiz. But no ordinary quiz! In order to answer the questions you have to undertake some practical data mining task. You don’t learn by watching someone talk; you learn by actually doing things! The quizzes give you an opportunity to do a lot of data mining.
I hear and I forget. I see and I remember. I do and I understand. (Confucius)
Tests:
  Mid-class test at the end of Week 2
  Post-class test at the end of Week 5
This week …
In Week 1 you will experience the surprising power of linear regression with lagged variables to model cyclic phenomena. Having become frustrated with all the steps that are involved in adding such variables manually, you will install the time series forecasting package and learn how to use it. You will analyze historical airline passenger data, and wine sales. (Unfortunately you do not get to drink the wine.) At the end of the week you will know how to use data mining to forecast the future! And, in addition, you will learn about major challenges for data mining applications, and how to infer properties of soil samples from infrared data.
Right now …
Please take the time to fill in the pre-course survey.
Teaching team
  Lead educator, Ian Witten
  Course team (in order of appearance): Geoff Holmes, Albert Bifet, Bernhard Pfahringer, Tony Smith, Eibe Frank, Pamela Douglas, Mark Hall, Mike Mayo, Peter Reutemann
  Educator, David Nichols
  Educator, Jemma König
Production team
  Video editing, Louise Hutt
  Captions, Jennifer Whisler
  Music: Improvisations on Dizzy Gillespie’s A night in Tunisia, by Ian Witten
Support
  Share what you are learning, including difficulties, problems and solutions, with others in the class in a weekly discussion focused on the Big Question of the week and what you have learned
  Other discussions from time to time
  Transcripts are supplied for all videos as PDF files
  Slides for all videos can be downloaded as PDF files
Software requirements
Before the course starts, download the free Weka software. It runs on any computer, under Windows, Linux, or Mac. It has been downloaded millions of times and is being used all around the world.
(Note: Depending on your computer and system version, you may need admin access to install Weka.)
Prerequisite knowledge
You should have completed Data Mining with Weka and More Data Mining with Weka – or be an experienced Weka user. If you can do the Are you ready for this? quiz at the end of this Activity, you’ll be fine!
Although the course includes some scripting with Python and Groovy, you need no prior knowledge of these languages.
You will have to install and configure some software components. We provide full instructions, but you may need to be resourceful in sorting out configuration problems.

<-- 1.3 Discussion -->
Welcome! Please introduce yourself
Remember me? I’m Ian, in New Zealand. I hope you do remember me from the courses Data Mining with Weka and More Data Mining with Weka, because you should have completed those (or have equivalent knowledge) before embarking on this course.
But just to remind you: formally I’m Professor Ian Witten from the University of Waikato in New Zealand, but everyone calls me Ian. I grew up in Northern Ireland, studied at Cambridge University, and worked at the Universities of Essex in England and Calgary in Canada before moving to paradise (aka New Zealand) 25 years ago. I became interested in data mining way back when, and also in open source software and open education. Data, they say, is the “new oil”: it affects all of us economically, socially, and politically.
I made this course because everyone needs to know about the data revolution (which far outshines the “computer revolution”) and what you can do with data – and what they can do with data. That’s why I’m so glad to have you here and learning!
Who are you? Where are you from? What do you do? Why are you here? What are you hoping to learn? What sort of data do you have and why do you need to analyze it? Can you share an example?
See what other learners say and let them know if you share their interests (use ‘Reply’).
(We find the diversity of backgrounds, interests and locations of the people who join this course absolutely fascinating.)
You can make and reply to comments on almost every step of the course by clicking the pink “+” icon. There will also be specific discussion steps from time to time (like this one and the regular Weekly Reflections steps). Please join in!
There’s more information on FutureLearn discussions here.

<-- 1.4 Quiz -->
Are you ready for this? 
Question 1
The sentiment.arff dataset contains 4995 tweets, classified as pos (2481 instances) and neg (2514 instances) according to whether they were followed by a positive smiley, e.g. :-), or a negative one, e.g. :-(. It has already been processed by the StringToWordVector filter; the attributes are word occurrences (1 or 0) and the last attribute (sentiment) is the class.
It is hard to improve much on the baseline accuracy of 50%; Multinomial Naive Bayes achieves 64%. In this series of questions you will produce a manipulated dataset of exactly the same size on which ZeroR and OneR perform the same as before, but Multinomial Naive Bayes achieves a substantially higher success rate.
Imagine a corrupt data miner, encouraged to improve upon the above-mentioned 64% under pressure from his boss.
To do this he cheats by using the RemoveMisclassified filter, setting maxIterations to 1, the classifier to NaiveBayesMultinomial, and leaving the other parameters at their default values. This reduces the dataset to 3233 instances.
What is the performance of Multinomial Naive Bayes on this reduced dataset?
57%
64%
84%
89%
95%
99%
100%
---
Correct answer(s):
95%

<-- 1.4 Quiz -->
Are you ready for this? 
Question 2
Further lured by promises of lucrative bonuses, our (anti-)hero re-applies RemoveMisclassified with maxIterations set to 0, which reduces the dataset to 3056 instances.
What is the performance of Multinomial Naive Bayes now?
57%
64%
84%
89%
95%
99%
100%
---
Correct answer(s):
99%

<-- 1.4 Quiz -->
Are you ready for this? 
Question 3
Fearing that his boss will smell a rat because the new dataset is much smaller than the one he was given, our (anti-)hero uses the Resample filter with a carefully calculated sampleSizePercent to restore the dataset to the original 4995 instances (he has to think about how to set the noReplacement parameter to achieve this).
How many pos instances are there in this dataset?
1528
1882
2896
3056
---
Correct answer(s):
1882
---
Feedback correct:
Use the Resample filter with noReplacement set to False and sampleSizePercent to 163.45% (4995/3056).
Setting noReplacement to True and trying to increase the sample size by setting sampleSizePercent greater than 100% causes an error message when you when click Apply.

<-- 1.4 Quiz -->
Are you ready for this? 
Question 4
ZeroR and OneR yield 62% and 67% accuracy on this new dataset, very different from the 50% and 52% that they achieved on the original one, and our (anti-)hero fears that, noting the phenomenal accuracy of Multinomial Naive Bayes, his boss will run benchmark tests and notice the discrepancy.
He hits on a plan to create a new dataset with exactly 2481 pos and 2514 neg instances, just as in the original dataset, which will therefore yield the exact same result from ZeroR. He starts with the 3056-instance dataset generated in Q.2, uses a suitable filter to remove all neg instances and then another filter to increase the instances to the desired number, and saves the dataset. He repeats the procedure, this time removing all pos instances; and then combines the two datasets using a text editor.
Applying ZeroR to the resulting dataset yields exactly the same result as for the original one. But OneR does not. What is its performance?
57%
64%
84%
89%
95%
99%
100%
---
Correct answer(s):
57%
---
Feedback correct:
Here’s what I did, starting with the 3056-instance dataset:
Apply RemoveWithValues, attrIndex “last”, nominalIndices “last”
[yields a dataset with 1148 “pos” instances only]
Apply Resample with sampleSizePercent 216.12% (i.e. 2481/1148)
[yields a dataset with 2481 "pos" instances only]
Save the dataset as temp.pos.arff
Undo (twice), to return to the 3056-instance dataset
Apply RemoveWithValues, attrIndex “last”, nominalIndices “first”
[yields a dataset with 1908 “neg” instances only]
Apply Resample with sampleSizePercent 131.77% (i.e. 2514/1908)
[yields a dataset with 2514 "neg" instances only]
Save the dataset as temp.neg.arff
In a text editor, append data from temp.neg.arff to temp.pos.arff
(the data is all lines that follow the “@data” line)
Save the result as sentiment.manipulated.arff
Load sentiment.manipulated.arff
It has 2481 “pos” instances and 2514 “neg” ones
ZeroR gives 50.3303% correct (like sentiment.arff)
OneR gives 56.7568% correct

<-- 1.4 Quiz -->
Are you ready for this? 
Question 5
To reduce OneR’s performance to that on the original dataset, our (anti-)hero decides to identify the attribute that OneR chooses for the new dataset, remove it, and continue until its performance (rounded to the nearest integer) is 52%.
In fact, he has to remove 8 attributes. What are they (in order of removal)?
good, user, url, love, happy, lol, great, awesome
lol, good, great, url, happy, love, work, awesome
great, work, lol, awesome, user, love, happy, url
work, happy, user, url, love, awesome, lol, great
lol, good, awesome, great, user, url, love, work
---
Correct answer(s):
good, user, url, love, happy, lol, great, awesome

<-- 1.4 Quiz -->
Are you ready for this? 
Question 6
Multinomial Naive Bayes’s performance deteriorates substantially on this latest dataset – though it’s still a great deal better than the original 64%.
What is it?
69%
73%
79%
84%
87%
89%
---
Correct answer(s):
84%
---
Feedback correct:
Congratulations!
You’ve created a manipulated dataset of exactly the same size as the original on which ZeroR and OneR perform the same as before, but Multinomial Naive Bayes achieves a substantially higher success rate – 84% instead of the original 64%.

<-- 1.4 Quiz -->
Are you ready for this? 
Question 6
Multinomial Naive Bayes’s performance deteriorates substantially on this latest dataset – though it’s still a great deal better than the original 64%.
What is it?
69%
73%
79%
84%
87%
89%
---
Correct answer(s):

<-- 1.5 Article -->
Rogues gallery
These are the experts you will meet on this course.

<-- 1.6 Article -->
How can you use data mining to foretell the future?
I’m sorry, but you can’t really use data mining to foretell the future. ☹️
What we’re going to look at is a particular kind of data called “time series data”. This involves an attribute or attributes, typically numeric, whose value evolves over time. Each instance has a time-stamp – which may or may not be spaced at regular intervals.
We often use linear regression to extrapolate the attribute’s value to future instances. You’ll be asking, isn’t that a bit mundane? – straight-line extrapolation. And you’ll probably be surprised (as I was) to learn that there are simple ways of augmenting the dataset to allow it to model cyclic phenomena using linear regression. Augmenting datasets manually is a frustrating experience, and we’ll learn how to use Weka’s time series forecasting package, which automates this and other related functions.  During the week you’ll analyze historical airline passenger data, and wine sales. (Unfortunately you do not get to drink the wine.)
This week our end-of-week example of a data mining application is about inferring properties of soil samples from infrared data. We’ll also look at some general challenges for data mining applications.
At the end of this week you will be able to explain the role of “lagged variables” in time-series analysis. You’ll be experienced in the use of Weka’s time series forecasting package, and be able to work with data that varies on an hourly, daily, weekly, monthly, and yearly basis. You’ll understand that the standard holdout and cross-validation methods simply do not work for time series, and know what to do about it. And you’ll be able to explain what “overlay data” is and how valuable it can be. Oh yes, and you’ll have even more experience of that perennial problem of overfitting, and how to detect it.

<-- 1.7 Video -->
Time series: linear regression with lags 
The simplest kind of forecasting is linear regression. Although this sounds mundane and not very useful – we rarely expect time series simply to be linearly increasing or decreasing – adding lagged copies of variables increases its power enormously by allowing cyclic models. Weka’s internal date format needs to be converted into something more sensible in order to be able to interpret the linear time-based models. Also, using a lagged variable means that early instances have missing values, which need to be removed.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
In this lesson, we’re going to start talking seriously about time series forecasting. We’re going to look at linear regression with lags. We’re not going to use the time series forecasting package yet; we’ll start that in the next lesson. We’re going to load a time series data set here. We’re going to go to the Explorer. I’m going to load airline. This is where my Weka datasets are. I don’t know where yours are. I’m going to load airline.arff. Here it is. I’m going to just have a look at this data with the edit button. You can see that there’s a passenger_numbers attribute and then a Date attribute that goes from the first of January 1949 through to the first of December 1960.
So this is ancient airline passenger data. We’re going to go to Classify here, and we’re going to predict with linear regression in the functions category. This is important. We’re going to predict passenger_numbers. It’s the first attribute, so we need to set it here from the default, because Weka by default predicts the last attribute. I’m going to just click Start. We’re going to be looking at the root-mean-squared error here. 46.6 is what we get. We could look at the classifier errors. Now, this is a linear regression, so we’re expecting a linear line here. That’s what linear regression predicts.
On the y-axis I’m going to put the predicted passenger numbers; on the x-axis I’m going to put the date, and there we have it. This is the predicted line. The size of these crosses incidentally indicates the size of the error at that point, but for our purposes here, it’s a linear regression. Not really very interesting. One thing that’s a little bit surprising is the model is 0 times date plus this constant and that would be a horizontal line if it was really true. There’s something a little bit funny about this. What is funny about it is the date. If I go back and look here, the date attribute has got values ranging from these numbers here. Is that 662 billion?
– 662 billion here. And that’s because these dates are measured in milliseconds since January 1, 1970. So I’m going to convert them into months since the beginning of the dataset. I’m going to do that with a filter. There’s different ways of doing this, but I’m going to use the AddExpression filter, and I’m going to make an expression that takes the second attribute, the date attribute, that’s a2. And I’m going to divide that by – that’s in milliseconds. I’m going to make it seconds, and then I’m going to make it minutes, and then I’m going to make it hours, and then I’m going to make it days. Then I’m going to make it years. 365¼ days in a year.
I’m going to add 21 to get from 1949 to 1970. I’m going to make this in months. It took me a little bit of a while to figure this out. I hope it’s going to work. I’m going to call that attribute NewDate. Let’s see what happens here. I’m going to apply the filter, and now I’ve got NewDate, which goes from round about 0 to about 143. Now, there’s a little issue here with leap years, right? I’m using this figure of 365.25 days in a year, which is pretty accurate on average, but I should really take into account exactly which years are leap years and so on, so there’s a bit of inexactness going on here. But never mind.
It’s just a bit approximate. I’m going to delete the Date attribute, remove the Date attribute. I’m going to look at the model again. I’m going to remember every time – this is a bit of a nuisance – every time I’ve got to remember to predict passenger_numbers. And if I run that, then we’re getting this model 2.66 times the NewDate plus 90. It’s the same model as before, but we’ve kind of scaled NewDate, so now this coefficient, which used to be rounded down to 0, is something more sensible. OK. So far so good, and so far not very interesting. Here is the regression line, and you can see the data. The data’s kind of cyclic when you look at it.
Passenger numbers, it depends on the month, you know, and yet the regression line is just a straight linear prediction. Not so interesting. Let’s do something a little bit more interesting. I’m going to copy the passenger_numbers attribute. We’re going to add a delayed version of passenger_numbers. I’m going to use the Copy filter to create a new attribute. I’m going to copy the first attribute and apply that. And here we’ve got Copy of passenger_numbers. I’m going to take this attribute and subtract 12, I’m going to lag it. I’m going to delay it by 12 months, so it’s going to contain last year’s value for that month. I’m going to do that with a TimeSeriesTranslate. I’m going to configure that.
I’m going to translate the third attribute. I’m going translate it by 12 months, subtract 12 months from that. I think that’s ok. And then I need to actually – this particular filter doesn’t work on the class, so I’m going to set the class back to passenger_numbers, and then I’m going to run it and see what happens here. If I go to Edit, now I can see this is my new attribute, and you can see that that 112 is this 112 here. In fact, this is a delayed version of this attribute. This gives for this month, month number 13, this gives the figure for the year before and these are unknown values. Terrific! That’s what I wanted to do.
Then I’m going to go back and predict this with linear regression. I need to remember to predict passenger_numbers. There we go, and now I get a different model and a better root-mean-squared error, 31.7. This is a model that uses the date and then a little bit of the 12-month-before copy.Now actually, this is not a very good model. It’s a little bit crazy, and the reason it’s a little bit crazy is because of those missing values. We’ve got missing values at the beginning of the dataset, and we’re going to get much better results if we delete those instances with missing values. I’m going to do that with a filter.
I’m going to do that with an instance filter called RemoveRange and I’m going to remove instances from 1–12. And if I apply that, then now if I look at my data, I don’t have missing values. This starts out with the 112 data which is 12 months before, and this starts out on the 13th month of the original data, which is what I want. So I’m then going to go now and classify that with linear regression. Don’t forget to predict passenger_numbers. There we go. And now I get a much smaller root-mean squared-error of 16, and I’m getting quite a sensible model. This says passenger numbers increase a little bit.
Take the passenger_number of the year before, add 7% and then just a little offset here. I could try and visualize this model. I’ll just show you. If I do it this way, it’s not really very informative, because this is predicted passenger_numbers on the y-axis against Date on the x-axis. And you can’t see any pattern here, there is actually a cyclic pattern, but it’s completely obscured by the size of these x’s, which are not very interesting for our purposes at the moment. In order to get a better look at that I’m going to use the AddClassification filter. I’m going to add a classification. It’s supervised attribute filter, AddClassification. I’m going to add the classification created by linear regression.
Output the classification, and I need here to say what we’re going to be predicting, which is passenger_numbers. I’m going to apply this filter, and now I get a new attribute, classification, which I can then visualize. So I’m going to look at classification against NewDate. And this shows you this cyclic prediction that we’re getting here. So adding this delayed attribute gives us a cyclic prediction.Let’s go back to the slide and have a look at this. We have a graph here, which shows the prediction with lag_12. There is no prediction for the first 12 instances, I deleted those.
So these are the predictions, this cyclic wave, and you can see this fits pretty well the actual values of passenger_numbers, which are the black dots here. It’s a much better fit, this cyclic prediction, than the original rather boring red linear prediction, and these are the two equations of those lines. So adding this simple lag variable allows us to break away from the linear paradigm, even though we’re using linear regression, and get nonlinear predictions. I think that’s pretty exciting, actually. I’ve done a lot of things rather quickly here, and you’re going to be redoing them yourself with a different classifier. I’ve got a list of some of the pitfalls that I’ve done, and you might want to refer back to this.
Just to summarize. We’ve learned that linear regression can be used for time series forecasting and that lagged variables yield much more complex models than straight line ones. In this case, we chose the appropriate lag by eyeballing the data and noticing that it varied in an annual cycle. And we can include more than one lagged variable with different lags, and we could think about seasonal effects, you know. We could think about yearly, quarterly, daily, hourly data. Of course, doing all of this manually is a pain, adding these variables. So the time series forecasting package helps you do this in a much easier, quicker, more convenient way.
<End Transcript>

<-- 1.8 Quiz -->
Time series forecasting with SMO 
Question 1
Open the airline.arff dataset. The Date attribute is expressed in msec since Jan 1, 1970.
Which of the values below is closest to its minimum value, as shown in the Preprocess panel?
35150317071
–286718400000
–474793200000
–662731200000
---
Correct answer(s):
–662731200000
---
Feedback correct:
Note: the value is negative for this dataset, because it precedes 1970. Also, the answer depends on the time zone you are in, because your operating system takes the time zone into account when translating dates and times. A 12-hour difference between your time and New Zealand’s would add 126060*1000 = 43200000 to the answer.
---
Feedback incorrect:
You may find
1.1 What will you learn?
useful.

<-- 1.8 Quiz -->
Time series forecasting with SMO 
Question 2
Which of the values below is closest to its maximum value?
35150317071
–286718400000
–474793200000
–662731200000
---
Correct answer(s):
–286718400000
---
Feedback correct:
Note: the value is negative for this dataset, because it precedes 1970. Also, the answer depends on the time zone you are in, because your operating system takes the time zone into account when translating dates and times. A 12-hour difference between your time and New Zealand’s would add 126060*1000 = 43200000 to the answer.

<-- 1.8 Quiz -->
Time series forecasting with SMO 
Question 3
Use the AddExpression filter to re-map the date in the airline dataset into months since Jan 1, 1949.
Call the new date NewDate, and remove the old Date attribute.
Use SMOreg (in the “functions” category) to predict passenger_numbers. What is the RMS error?
16.7
33.9
40.5
48.5
---
Correct answer(s):
48.5
---
Feedback correct:
The appropriate expression parameter for the AddExpression filter, used in the lesson video, is (a2/(1000*60*60*24*365.25) + 21)*12.
---
Feedback incorrect:
You get this if you predict NewDate instead of passenger_numbers. Change the Classify panel to predict passenger_numbers.

<-- 1.8 Quiz -->
Time series forecasting with SMO 
Question 4
If you look in the Classifier output at the model produced, you will see that SMOreg has normalized the NewDate parameter. To make a comparison with the linear regression model produced in the lesson video, you need to disable this normalization operation. Do so.
What is the model produced?
0.36*passenger_numbers – 26.98
0.65*NewDate + 0.00
2.66*NewDate + 90.44
2.41*NewDate + 99.00
---
Correct answer(s):
2.41*NewDate + 99.00
---
Feedback incorrect:
Are you predicting NewDate instead of passenger_numbers?
---
Feedback incorrect:
You need to look at the filterType parameter of SMOreg, and change it to No normalization/standardization.
---
Feedback incorrect:
This model is the one produced by LinearRegression; you can compare it with SMOreg.

<-- 1.8 Quiz -->
Time series forecasting with SMO 
Question 5
Now add a lagged variable.
In the Preprocess panel, copy the passenger_numbers attribute and apply the TimeSeriesTranslate filter with a parameter of –12 months to the new attribute. Classify with SMOreg. (You should be predicting the future from the past, not the other way round, so think carefully which version of the passenger_numbers attribute should be the class.)
What is the RMS error?
0%
14.6%
23.7%
36.3%
38.8%
---
Correct answer(s):
38.8%
---
Feedback correct:
Many things can go wrong here.
Use the Copy filter to copy the attribute. Here you need to specify which attribute to copy.
In the TimeSeriesTranslate filter, specify –12 as the instanceRange and 3 (the copied attribute) as the attributeIndices. Be sure to check that the translation has been done correctly using the Preprocess panel’s Edit button. If it hasn’t, note that TimeSeriesTranslate doesn’t apply translation to what it thinks is the “class” attribute—by default, the last attribute.
---
Feedback incorrect:
Many things can go wrong here.
Use the Copy filter to copy the attribute. Here you need to specify which attribute to copy.
In the TimeSeriesTranslate filter, specify –12 as the instanceRange and 3 (the copied attribute) as the attributeIndices. Be sure to check that the translation has been done correctly using the Preprocess panel’s Edit button. If it hasn’t, note that TimeSeriesTranslate doesn’t apply translation to what it thinks is the “class” attribute—by default, the last attribute.
Perfect prediction? Zero error? You’re probably predicting a class value that is already present as an attribute in the dataset!
---
Feedback incorrect:
Many things can go wrong here.
Use the Copy filter to copy the attribute. Here you need to specify which attribute to copy.
In the TimeSeriesTranslate filter, specify –12 as the instanceRange and 3 (the copied attribute) as the attributeIndices. Be sure to check that the translation has been done correctly using the Preprocess panel’s Edit button. If it hasn’t, note that TimeSeriesTranslate doesn’t apply translation to what it thinks is the “class” attribute—by default, the last attribute.
You’re probably predicting the time-shifted attribute from the original one – in other words, predicting the past from the future!
---
Feedback incorrect:
Many things can go wrong here.
Use the Copy filter to copy the attribute. Here you need to specify which attribute to copy.
In the TimeSeriesTranslate filter, specify –12 as the instanceRange and 3 (the copied attribute) as the attributeIndices. Be sure to check that the translation has been done correctly using the Preprocess panel’s Edit button. If it hasn’t, note that TimeSeriesTranslate doesn’t apply translation to what it thinks is the “class” attribute—by default, the last attribute.

<-- 1.8 Quiz -->
Time series forecasting with SMO 
Question 6
The model is not a good one because the first 12 instances have missing values for the lagged parameter, and SMOreg replaces these by their average value over the dataset, which is inappropriate.
Delete the first 12 instances using the RemoveRange instance filter.
What is the RMS error of SMOreg now?
12.6
14.7
15.9
16.4
---
Correct answer(s):
16.4
---
Feedback correct:
Specify 1–12 as the instancesIndices parameter of the RemoveRange filter
---
Feedback incorrect:
Specify 1–12 as the instancesIndices parameter of the RemoveRange filter
---
Feedback incorrect:
Specify 1–12 as the instancesIndices parameter of the RemoveRange filter
You’re probably predicting the copied parameter, i.e. the time-shifted version of passenger_numbers – again!

<-- 1.8 Quiz -->
Time series forecasting with SMO 
Question 7
Check the model produced to see whether the parameters have been normalized, which means that you cannot compare it with the linear regression model produced in the lesson video. If so, you should prevent this normalization operation. Do so.
What is the model produced? (In the answers below, Lag_12 is the new attribute, i.e. the lagged version of passenger_numbers.)
0.07 * NewDate + 1.07 * Lag_12 + 11.62
1.07 * Lag_12 + 12.67
0.05 * NewDate + 0.90 * Lag_12 + 0.02
0.19 * NewDate + 0.85 * passenger_numbers + 5.29
---
Correct answer(s):
0.07 * NewDate + 1.07 * Lag_12 + 11.62
---
Feedback correct:
Note that suppressing normalization has changed the RMS error slightly, from 16.4 to 16.5
---
Feedback incorrect:
This is the model produced by LinearRegression. It’s interesting that the coefficient of the lagged variable is the same.
---
Feedback incorrect:
Make sure that SMOreg’s filterType is set to No normalization/standardization.
---
Feedback incorrect:
Are you predicting the time-shifted version of passenger_numbers? – again!

<-- 1.8 Quiz -->
Time series forecasting with SMO 
Question 8
So, which is better in terms of RMS error?
SMOreg
LinearRegression
---
Correct answer(s):
LinearRegression
---
Feedback correct:
In the video, the RMS error obtained for LinearRegression was 16.0, whereas here we have found 16.4 for SMOreg (16.5 without normalization).
This is surprising, because support vector machines are more sophisticated learning structures than linear regression. However, all this evaluation has been done on the training set – which is always misleading. In the next section we will see what happens on independent test sets.

<-- 1.8 Quiz -->
Time series forecasting with SMO 
Question 8
So, which is better in terms of RMS error?
SMOreg
LinearRegression
---
Correct answer(s):

<-- 1.9 Video -->
Using the time series forecasting package 
Dealing manually with time series is a pain, as we learned in the last lesson. Weka’s time series forecasting package automatically produces lagged variables, plus many others – perhaps too many! It transforms the data by adding a large number of attributes, which, unfortunately, invites overfitting. This is indicated by a large discrepancy between error on the training set and error on independent test data. You can configure Weka to reduce the number of added attributes.
<Start Transcript>
Hello again, and welcome back to Advanced Data Mining with Weka. We’re going to look at the time series forecasting package now to do roughly what we did in the last lesson without the time series forecasting package. I’ve got the airline data loaded here. The time series package has given me this additional Forecast tab. I’m going to go straight to that, and without any more ado I’m just going to click Start and see what happens. Well, the time series package transforms the data into a large number of attributes. Unfortunately, you don’t get to see the attributes in the Preprocess panel. We still just have those two attributes there. You don’t see the generated attributes there.
You have to go to the Forecast panel and look here. Here’s the original attributes, and here’s transformed
training data: passenger_numbers; we’ve got month, quarter, date-remapped. The date-remapped is like what we did for the date in the last lesson. We did it manually, which changed it from milliseconds since January 1, 1970 into something more sensible. This actually does a better job, because it takes proper account of which years are leap years and which years aren’t leap years. Then we’ve got these lagged variables. The passenger_numbers lagged by – we just had 12 before – but now we’ve got the lags by 1, 2, 3, right up to 12 for 12 months, I guess.
We’ve got the square of the date-remapped and the cube of the date-remapped, in case you need those, and a bunch of other things, the date-remapped times these lagged variables. That’s a lot of variables. Underneath here is the generated model, which is very complicated. Let’s see how well it does. Actually, it doesn’t show here how well it does. To see that, we have to turn on Perform Evaluation. Let me click that here. Run it again, and we get a root-mean-squared
error of 10.6 on the training set, which looks good: last time we got 16.0. That was the best figure we got. But remember, this is the error on the training set. That’s always very misleading. Let’s make a simpler model. There’s a lot of attributes here. We can’t edit the generated attributes, like I said, but we can apply a filter. So I’m going to go to Advanced Configuration, and for my base learner, I’m going to choose the FilteredClassifier. And in the FilteredClassifier, I’m going to specify linear regression just like we had before, and for the filter, I’m going to choose the Remove attribute filter. Here it is.
I’m going to configure that to remove attributes number 1, 4, and 16, which I happen to know the correct ones. I’m sorry. I’m going to leave attributes 1, 4, and 16, and I’m going to set invertSelection to True. So these are the three attributes that I leave. Well, let’s just see what happens. Go back and look at my attributes, and here’s the generated attributes that we saw before. Now here’s the filtered attributes. We’ve got passenger_numbers, we’ve got date-remapped, and we’ve got this lag by 12. This is what we did in the last lesson, remember? Let’s see how we get on here. We got a root-mean-squared error of 27.8.
Actually, we got that on the last lesson, but we got even better results by deleting the first 12 instances. Remember the first 12 instances have got lagged values with unknown values, and linear regression does bad things with unknown values, at least as far as time series are concerned. So I want to delete the first 12 instances.
Now, I could do that by applying two filters: removing attributes and removing instances and I could use the multifilter. But actually on the time series forecasting panel, there’s an easy way of doing that, which you really need to learn, because you’re going to be doing it a lot. In Advanced Configuration, we’re going to look at Lag creation and the More options. We’re going to say remove leading instances with unknown lag values. Let me run that, and now I get a root-mean-squared error of 15.8, and a model which is exactly the same
as the model we got on the last lesson: 1.07 times lag_passenger_numbers plus 12.7. That’s what we got before. Now, let’s just return to this full model that we had. We won’t use the filtered classifier; we’ll just use linear regression. Here it is. Now, we get a Root mean squared error of 8.7. It looks fantastic. But the model looks extremely complicated. We looked it before. Here it is again. Look at the complexity of this model. So it’s probably overfitted. What we’d like to do is to evaluate this on held out training data. We can do that with the Evaluation panel.
I’m going to evaluate on – we can either have a fraction here or a number of instances – I’m going to evaluate on 24 instances, that is two years’ worth of instances and run that. I get an error on the test data of 59. That’s huge. The error on the training data is only 6.4. So let’s just have a look at this on the slide. With the full model, all the attributes, we’ve got this enormous gap between the training error and the test error. And with this simple model, with just two attributes there, there’s a little gap, but not very big. So we could try reducing the attributes in other ways. We could actually use the AttributeSelectedClassifier.
I won’t do that for you, but to do that I’d have to choose the metalearner AttributeSelectedClassifier and specify linear regression as the base learner and then specify some attribute selection method. If I left that at all the defaults, I would in fact get four attributes selected. And I’d get a training and test error of 11 and 19. Still some indication of overfitting. The gap between these two figures really indicates overfitting. Now, we reduced the model to two attributes using a filter, the Remove filter. But actually there is a simpler way of doing that, which you need to learn, in the Forecast panel.
If you go to Lag creation, it’s going to create lags between 1–12 – we saw those – but if you use custom lag lengths, we can increase that to 12, and now it’s only going to create a lag length of 12. I can remove the powers of time. Remember we had the time squared and the time cubed. We can remove the product of time and lagged variables. And if I go to periodic attributes here and click Customize, then I can include whichever ones of these attributes it wants to generate. Now, I’m not going to include any of those. So that will get us the simplest attribute set. I’ll just run that, and let’s look
now at the attributes being used, just three of them: passenger_numbers, date-remapped, and this lag by 12. Down here, of course, we’ve got the same result as we got before. We’ve got the same model and the same training and test errors. If we plot these things, this is the training data. Now remember we’re ignoring the first 12 instances at the beginning because we have unknown values for the lagged variable, and we’re reserving 24 instances at the end for testing. So if we look now at the full model, we get this red line, and you can see that the predictions over the test data are starting to vary from those data points.
If you look at the simple model, the one with just two attributes, then we get a more accurate line. Here they are, in fact, both together, and you can see the blue one from the simple model is more accurate than the red one for the full model. We’re using one-step-ahead predictions to evaluate the error here, which means that they can propagate. If you look at the solid red line toward the end, the first of those big dips is an error, and then the second sort of ‘double dip’ is an error that’s propagated from the first error.
Once it starts making an error in this kind of evaluation, when we’re evaluating the one step ahead each time, the errors are going to propagate. So it’s a pretty bad thing once you start making errors, they get worse and worse. OK. That’s it. Weka’s time series forecasting package makes it easy to experiment with lagged variables and other kinds of things like that. It automatically generates many attributes, perhaps too many attributes, so it’s a good idea to always try simpler models. You can use the Remove filter which we did at first, or you can choose which attributes you want using the Lag creation and Periodic attributes tabs under Advanced Configuration.
As always in data mining, you need to be wary of evaluation based on the training data, and you can hold data out using the Evaluation tab. Finally, we’re evaluating time series using repeated one-step-ahead predictions, which means that errors propagate.
<End Transcript>

<-- 1.10 Quiz -->
Evaluating on a test set 
Question 1
Open the airline.arff dataset. Go to the Forecast panel and check Perform evaluation.
In Advanced configuration, ensure that LinearRegression (the default) is chosen as the base learner. Under Lag creation, More options, check Remove leading instances.
What is the RMS error of LinearRegression on the training data?
6.9
8.4
8.7
10.6
---
Correct answer(s):
8.7

<-- 1.10 Quiz -->
Evaluating on a test set 
Question 2
Go to the Evaluation panel and hold out 24 instances for evaluation.
What is the RMS error on the test data?
27.1
32.9
52.9
58.0
---
Correct answer(s):
58.0

<-- 1.10 Quiz -->
Evaluating on a test set 
Question 3
What is the RMS error on the training data?
5.3
5.5
6.4
8.7
---
Correct answer(s):
6.4
---
Feedback correct:
The training error was 8.7 before, but has changed because the training set is different – it has 24 fewer instances

<-- 1.10 Quiz -->
Evaluating on a test set 
Question 4
What is the most likely reason for the huge disparity between test and training set errors you have just found?
The test data represents values at later times than the training set
The difference is due to statistical variation
The training set is far larger than the test set
The model is overfitted to the training data
---
Correct answer(s):
The model is overfitted to the training data
---
Feedback correct:
The disparity is so huge that it seems overwhelmingly likely that the model overfits the training data
---
Feedback incorrect:
This is certainly a possibility. A fundamental assumption of machine learning is that both training and test data represent the same fundamental statistical distribution, which is unlikely to be the case for time series, which are generally either increasing or decreasing. However, one of the other choices identifies what is probably a far stronger effect.
---
Feedback incorrect:
The difference could conceivably be due to statistical variation, but there’s a much more likely answer

<-- 1.10 Quiz -->
Evaluating on a test set 
Question 5
Now change the base learner to SMOreg.
What is the RMS error on the test data and training data?
Select all the answers you think are correct.
Test data: 6.1
Test data: 8.9
Test data: 22.8
Test data: 27.5
Training data: 6.1
Training data: 8.9
Training data: 22.8
Training data: 27.5
---
Correct answer(s):
Test data: 27.5
Training data: 8.9

<-- 1.10 Quiz -->
Evaluating on a test set 
Question 6
What do you think about the figures for SMOreg compared with the corresponding ones for LinearRegression?
Neither SMOreg nor LinearRegression overfit very much
Overfitting occurs for LinearRegression but not for SMOreg
Overfitting occurs for SMOreg but not for LinearRegression
Overfitting is present for SMOreg, but to a lesser degree than for LinearRegression
---
Correct answer(s):
Overfitting is present for SMOreg, but to a lesser degree than for LinearRegression
---
Feedback incorrect:
The difference between test and training errors, which indicates overfitting, is substantial for both SMOreg and LinearRegression
---
Feedback incorrect:
The difference between test and training errors, which indicates overfitting, is substantial for SMOreg as well as for LinearRegression
---
Feedback incorrect:
The difference between test and training errors, which indicates overfitting, is substantial for LinearRegression as well as for SMOreg

<-- 1.10 Quiz -->
Evaluating on a test set 
Question 7
If you look at the model that SMOreg has just created you will see that it involves a linear sum with 43 terms (plus a constant). Let’s make a simpler model.
On the lag creation panel:
    use custom lag lengths, setting both minimum and maximum to 12
    under More options, turn off both powers of time and products of time and lagged variables
On the Periodic attributes panel:
  check Customize but leave all the other boxes unchecked.
What is the RMS error of SMOreg on the test data and training data?
Select all the answers you think are correct.
Test data: 15.0
Test data: 16.9
Test data: 17.0
Test data: 17.9
Training data: 12.2
Training data: 15.0
Training data: 15.9
Training data: 16.9
---
Correct answer(s):
Test data: 16.9
Training data: 15.9
---
Feedback correct:
Overfitting is greatly reduced
---
Feedback incorrect:
Overfitting is greatly reduced

<-- 1.10 Quiz -->
Evaluating on a test set 
Question 8
With the current settings, what is the RMS error of LinearRegression on the test data and training data?
Select all the answers you think are correct.
Test data: 12.4
Test data: 15.6
Test data: 16.6
Test data: 18.7
Training data: 12.4
Training data: 15.6
Training data: 16.6
Training data: 18.7
---
Correct answer(s):
Test data: 18.7
Training data: 15.6
---
Feedback correct:
Comparing the performances of SMOreg and LinearRegression, SMOreg is a little better than LinearRegression on the test data; they are about the same on the training data. LinearRegression is still perhaps slightly overfitted to the training data.
---
Feedback incorrect:
Comparing the performances of SMOreg and LinearRegression, SMOreg is a little better than LinearRegression on the test data; they are about the same on the training data. LinearRegression is still perhaps slightly overfitted to the training data.

<-- 1.10 Quiz -->
Evaluating on a test set 
Question 8
With the current settings, what is the RMS error of LinearRegression on the test data and training data?
Select all the answers you think are correct.
Test data: 12.4
Test data: 15.6
Test data: 16.6
Test data: 18.7
Training data: 12.4
Training data: 15.6
Training data: 16.6
Training data: 18.7
---
Correct answer(s):

<-- 1.11 Video -->
Looking at forecasts
Weka’s time series forecasting package includes options for visualizing predictions for any number of steps ahead, as well as performance on the training data. As well as visualizing future predictions, you can hold out the last few instances of the dataset for testing and visualize performance on these. Errors accumulate on multi-step future predictions, and you can assess the effect of this by looking at 1-step-ahead – or indeed any-number-of-steps-ahead – predictions on held-out test data.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello, and welcome back to New Zealand for another few minutes of Advanced Data Mining with Weka. We’re going to continue our exploration of the time series forecasting package. In the last lesson I showed you some graphs, which I actually made with Excel for the purposes of presentation, but the time series forecasting package can make such graphs itself, and we’re going to show you how to look at the output of the package. I think you should restart the Explorer, just to reinitialize all of the options in the time series forecasting stuff, and load airline.arff. I’ve done that. I’m going to go to Forecast
and click Start here, and we get this output, which we haven’t looked at before: “Train future predictions” it’s called; and you can see this is a graph actually of passenger numbers, and if you look very carefully, you can see that these are square data points, and the very last one is a round data point. That’s the predicted passenger number. We’re only predicting one time unit here, but we can change that. Let’s go up to the interface and change the number of time units to forecast to, say, 12, and try again. Now you can see that we’ve got these 12 predicted points and a dashed line. So we’re forecasting ahead, from the end of the training data.
Let’s go to the Lag creation panel, and remember we removed the leading instances with unknown lag values. That will remove the first 12 instances, and we can do that again. Actually, it doesn’t affect the graph. We still get the same graph, but we know that the first 12 instances are not being used to create the model. Coming back to the slide, think about the timeline like this. Here’s the dataset, that top line, and underneath we’ve got the dashed line with the leading instances, 12 of them, and then the training data for future predictions, and then the future predictions leading ahead after the end of the dataset. All right. Now let’s do some evaluation here.
We’re going to evaluate on the training data and on 24 held-out instances. I’m going to go to the Evaluation panel and evaluate on the training data and 24 held-out instances, two years worth. Run that. Now I get the “train future predictions” output here, which ends at the end of the training data and then shows us the future for 12 future predictions from that point. Coming back to the slide, we’ve got the dataset. We’ve got the training data now, which is all of the dataset except for the last 24 instances, and the future predictions from the training data is the dashed line there.
Then if we look at the other output here – going back to Weka – ”Test future predictions”, you can see now that we’ve got the test data here and future predictions from the end of the test data, this dashed line with the round points. Coming back to the slide, we’ve got the whole dataset, then we’ve got the training data, and then we’ve got the test data and future predictions from the end of the test data, that is, after the end of the dataset. Now it would be nice to see the one-step-ahead estimates for the test data. There are a lot of graphing options here.
First of all, I’m going to turn off the evaluation on training, because that’s going to give us too much data to look at. Let’s just look at evaluating on the test data. I’m not going to graph the future predictions at all. Now if I run this, I get no graphical output. There’s nothing. Let’s turn on “Graph the predictions at step 1” and run it. Now you can see here the test predictions for the target. You can see in blue the predicted passenger numbers and in red the actual passenger numbers. So we can see there the discrepancy on the test data between the one-step-ahead predictions and the actual data itself. We’re going to do a little bit more on this panel.
We’re going to graph the predictions at step 12, that is, 12-step-ahead predictions, and then we’re going to compare 1-step-ahead, 6-steps-ahead, and 12-steps-ahead predictions. Let’s go back here. I’m going to graph the predictions at step 12. Now, I of course get worse predictions, because we’re predicting 12 steps ahead. You’d expect that to get worse. There’s a consistent error, where they undershoot the actual data values because, of course, with multi-step-ahead predictions, with any step-ahead predictions, once you make an error on the first prediction, then that error continues to propagate through the future predictions. Let’s graph the target. We’ve only got one possible target here.
If we had other attributes, we could graph them, but we’re just going to graph passenger_numbers at step 12, and actually that’s going to give the same result.
I’ve got two graphs here: the one we had before, and the new one, which looks exactly the same. However, you can do better things here. I’m going to turn the old one off just to stop too much confusion, and I’m going to graph – we can put in a comma-separated list of numbers here – so I’m going to graph 1-step-ahead, 6-steps-ahead, and 12-steps-ahead predictions. Now you can see them in different colors. The difference between 1-step-ahead predictions, the most accurate, that’s the blue line; 6-steps-ahead predictions, which is the green line; and, yellow, which is considerably worse, and 12-steps-ahead predictions, which is a
bit worse still: the yellow line. You can compare predictions at different points ahead. I’m just going to improve these predictions to finish off. I’m going to go to my base learner and change it from linear regression to SMO. Let’s have a look at that. You can see those predictions are quite a bit better than they were with linear regression. Let’s go and change – we’re using this large model with a large number of attributes here – I’m going to reduce the number of attributes. I’m going to just use a lag of 12, and then I’m going not to include powers of time. I’m not going to include products of time and lag variables.
I’m going here, and I’m going to customize this by not including any of these periodic attributes. If I run this again, well I’ve got a much simpler model here. This is the model based on just the date and the lag by 12. Now if I look at those graphs that I saw before. Well, you can’t see them. You can’t see them, because they’re all on top of each other. It’s plotting the red and the blue last, and the green and the yellow are hidden underneath the 1-step-ahead predictions. I’ve shown you several different options for visualizing time series predictions. We
talked about the need to distinguish different parts of the timeline: the initialization part with the leading instances, which contain unknown values for the lag variables; extrapolation past the end of the dataset into future predictions; the full training data; the test data, if evaluation is specified; and the training data with the test data held out; and we extrapolate past the end of that for so-called “future predictions” based on the training data. We showed how you can look at different numbers of steps ahead when making predictions. You can read more about this in a document about the time series analysis and forecasting package with Weka, referenced there at the bottom.
<End Transcript>

<-- 1.12 Quiz -->
Looking at the textual output
Question 1
Go to the Forecast panel and forecast 12 time units ahead, using default values for all other parameters.
What’s the maximum passenger estimate over the 12 predicted values, and what month and year does it occur in?
464, in December 1961
661, in August 1961
690, in June 1961
690, in July 1961
691, in January 1962
710, in January 1962
---
Correct answer(s):
690, in July 1961

<-- 1.12 Quiz -->
Looking at the textual output
Question 2
Now remove leading instances with unknown lag values, and repeat.
What’s the maximum passenger estimate over the 12 predicted values, and what month and year does it occur in?
690, in July 1961
710, in July 1961
710, in August 1961
892, in October 1961
892, in December 1961
905, in July 1962
---
Correct answer(s):
710, in July 1961

<-- 1.12 Quiz -->
Looking at the textual output
Question 3
Now evaluate on the training data and 24 test instances.
What’s the maximum future passenger estimate from the end of the training data, and what month and year does it occur in? (Note: be sure to look from the end of the training data, not the test data.)
490, in June 1959
490, in August 1959
543, in July 1961
610, in September 1959
---
Correct answer(s):
490, in August 1959
---
Feedback incorrect:
Are you looking at predictions from the test data instead of the training data?

<-- 1.12 Quiz -->
Looking at the textual output
Question 4
What’s the maximum passenger estimate from the end of the test data, and what month and year does it occur in?
490, in August 1959
438, in June 1961
438, in July 1961
543, in July 1961
---
Correct answer(s):
543, in July 1961
---
Feedback incorrect:
Are you looking at predictions from the training data instead of the test data?

<-- 1.12 Quiz -->
Looking at the textual output
Question 5
You can change the textual options using the “Output options” (as opposed to “Graphing options”). The results all appear in the single textual output stream, rather than in different tabs as with the Graphing options.
By setting the “Output options” appropriately, what is the largest error in the training data for 1-step-ahead predictions, and what instance number does it occur on? (Note: the largest error may be negative.)
–12, on instance 42
–9, on instance 14
15, on instance 30
21, on instance 62
332, on instance 120
---
Correct answer(s):
21, on instance 62

<-- 1.12 Quiz -->
Looking at the textual output
Question 6
What is the largest error in the test data for 1-step-ahead predictions, and what instance number does it occur on? (Note: the largest error may be negative.)
–106, on instance 136
–91, on instance 134
–67, on instance 131
–56, on instance 122
336, on instance 121
---
Correct answer(s):
–106, on instance 136

<-- 1.12 Quiz -->
Looking at the textual output
Question 6
What is the largest error in the test data for 1-step-ahead predictions, and what instance number does it occur on? (Note: the largest error may be negative.)
–106, on instance 136
–91, on instance 134
–67, on instance 131
–56, on instance 122
336, on instance 121
---
Correct answer(s):

<-- 1.13 Video -->
Lag creation and overlay data 
There are many parameters and options for deriving time-dependent attributes, such as which attribute holds the timestamp and what is the periodicity of the data. Periodicity affects the lagged variables that are generated. Weka interpolates instances for missing dates, which you can suppress manually if you wish. You can predict several variables, and Weka generates lagged values of each one. You can incorporate “overlay data” – additional data that might be relevant to the prediction that is available for the future (e.g., weather forecasts).
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! Welcome back to New Zealand for some more Advanced Data Mining with Weka. This is the last lesson on the time series forecasting facilities. We’re going to look at some features that we haven’t looked at so far. First of all, the timestamp. Any attribute of type “date” is used as the timestamp by default, but you can change this under the basic configuration parameters. I’ve loaded the airline data once again, and if I go to the Forecast panel, it’s going to use Date as the timestamp, but I could change that to another attribute if I wanted. Also, the periodicity. We’ve been detecting the periodicity automatically.
This data is monthly; I think there are 143 monthly instances, but we can specify something else if we prefer. We could actually specify, let’s say, weekly. This is not necessarily a very sensible thing to do, but what would happen if we specified weekly? First of all, it affects the lagged variables, the variables that are generated. Now we’ve got a large number of lagged variables. Actually, with weekly data, we’ve got 52 lagged variables generated. 52 weeks in a year, a whole year’s worth. As well as that, Weka inserts interpolated instances for the missing values.
So if we’re trying to do this weekly and the data was only monthly, then there’s a whole lot of weeks which need to be interpolated, and these are them. These weeks, and there’s a long list of weeks here, have been interpolated into the data. Then, of course, in order to get values for the training instances, they’re all missing values, so Weka interpolates the values for all of the attributes. These values have been interpolated. In this case, the airline data monthly is 144 instances. Weekly, we’ve got 573 instances here, and if I were to specify hourly we’d have 104,000 instances. The periodicity, as
I said, determines what attributes are created: different numbers of lagged variables depending on whether it’s monthly, weekly, daily or hourly. If it’s daily, then we include a Day-of-the-week attribute and Weekend attributes. If it’s hourly, we include a Morning or Afternoon attribute. Of course, you can override all of these attributes using the Advanced Configuration panel. I bet you’re tired of the airline data now. I’m going to open another dataset, the Apple stocks data. We need to find this data. When you install a package in Weka, it installs the package information in your home folder, so I’m going to go to my home folder, wekaFiles / Packages / timeseriesForecasting package, and here I’ve got some sample data, time series forecasting data.
I’m going to open appleStocks. Now, this data contains more than one thing to predict. It’s actually got the daily high, low, opening, and closing values for Apple stocks in the year 2011, plus the sales volume. I’m going go here – I need to tell it what to forecast. I’m going to forecast Close. Let me just see what happens. It’s generated lags here. It’s generated 12 lags. I think I want to tell it this data is weekly actually. I don’t think it’s figured that out. The Periodicity is weekly. No, I’m sorry, the periodicity is daily for this data. Let me do that, and now I’ve got 7 lagged variables, so a whole week’s worth of lagged variables.
There were some missing values, and instances were inserted, a few instances. Those were mostly weekends, actually, those instances. That’s what the skip list is for. I don’t really want to include weekends, because the stock market is closed. If I type “weekend” here, and do it again, then I will have reduced the number of interpolated instances. There are still a few of them – 5 of them – and those correspond to holidays when the stock exchange was closed. I can actually specify a list of dates here, as well as the word “weekend”. Let’s specify a list of dates in the format that’s on the slide. Let me just try that. Now I’m hoping for no interpolated instances. Yep, there’s none there.
I think what I’d like to do is to specify under the lags, I want to use maybe 2 weeks worth – that would be 10 working days. Let’s up that number to 10. OK, that’s the data prepared. Now, let’s do some evaluation on this data. First of all, I’m going to remove the leading instances, which are the ones with unknown lag values, which is a good idea. And then we’re going to hold out some of the instances. Let’s go and Remove leading instances, and then go to Evaluation. We’re going to evaluate on training and test, and I’m going to leave this at 30%. We’re going to use 30% of the dataset for testing. OK.
I’m going to look here at the mean absolute error. We’ve got these numbers here, 7.7 on the slide, you can see that since we’ve removed the leading instances, we get slightly better results than if we hadn’t done that. We can predict more than one target with this data, and if we do that we’re going to get lagged versions of each of the targets, and that might help. Let’s go and predict Close and High. We’re going to get lagged values of both of these variables, and it’s possible that we might get better predictions. Well, actually, we don’t.
These are the values we get: 8 on the test data and 3.4 on the training data, slightly worse than before. If we were to select all of the variables as targets, we’d get even worse results. We get quite bad overfitting here, with a much smaller training error, 2.5, than the test error, 9.6.Now, another thing that you need to know about is overlay data. Overlay data is additional data that might be relevant to the prediction. It’s not to be forecast. It can’t be predicted, and it’s available in the future. Overlay data is available in the future.
We don’t have overlay data for the Apple stocks problem, but I’m going to cheat by using one of the existing attributes as though it were overlay data, as though we knew it even in the future. Let me just predict Close. I’m going to go and specify some overlay data. We’re going to use Open as overlay data, and I can then see what happens. I got a complaint here from Weka. It’s unable to generate a future forecast because there’re no future values available for the overlay data. Well, let’s just stop it trying to generate future forecasts. If I just take out these output future predictions and do it again, then I won’t get that error message.
Back on the slide, we can see that the overlay data has improved things quite a bit. By including Open, the test error has got down to 5.9, and if we include High as well, it gets down even further. And although I won’t do this for you, if I were to change the base learner to SMO, a better learner, I would get even better results, down to a very small error on the test data, 2.4.In fact, I would get these graphs if I looked at the predictions. Again to save time I won’t do that, but you can see the prediction on the training data, the prediction on the test data. We’re getting very good predictions using this overlay data.
Well, we’ve covered quite a few options in the time series forecasting package. When you’re starting with a new dataset, you should start by getting the time axis right. Don’t forget that missing instances are automatically interpolated, and you can select the periodicity yourself if you like, and there’s a skip facility to ensure that time increases linearly. Then you need to select your target, what you’re going to predict (or targets). Overlay data can help a lot, obviously. If you can get hold of it, that’s always wonderful.
<End Transcript>

<-- 1.14 Article -->
How do you find stock market data?
You might be interested in using Weka to analyze other stock market data.
For example, suppose you wanted daily financial data for Google (GOOG) for the period 1 Jan 2015 – 31 Dec 2015. It’s easy!
    Go to http://finance.yahoo.com
    Search for GOOG
    Click Historical Data
    Specify the time period and click Done, and then Apply
    Click Download Data to get a CSV file that you can load into Weka.
Enjoy!

<-- 1.15 Quiz -->
Forecasting wine sales 
Question 1
Throughout this activity we will use mean absolute error as the evaluation criterion, rather than root mean squared error as before. Which is best in practice depends on what is most appropriate for the problem at hand in the real world.
There are important differences between mean absolute error and root mean squared error. Which of these statements do you agree with?
Select all the answers you think are correct.
Mean absolute error weights all individual differences equally
Root mean squared error gives relatively high weight to large errors
Mean absolute error never exceeds root mean squared error
Root mean squared error punishes large errors more severely
If both measures have the same value, all errors have the same size
Neither root mean squared error nor mean absolute error can be negative
---
Correct answer(s):
Mean absolute error weights all individual differences equally
Root mean squared error gives relatively high weight to large errors
Mean absolute error never exceeds root mean squared error
Root mean squared error punishes large errors more severely
If both measures have the same value, all errors have the same size
Neither root mean squared error nor mean absolute error can be negative

<-- 1.15 Quiz -->
Forecasting wine sales 
Question 2
Use linear regression to forecast the volume of red wine, holding out one year’s worth of data for evaluation.
But first, check Remove leading instances under Lag creation, More options. Leave all other parameters at their default values. Note that Weka correctly identifies the last attribute as the date to be used for forecasting.
What is the mean absolute error on the test data and the training data?
Select all the answers you think are correct.
Test data: 173
Test data: 187
Test data: 233
Training data: 131
Training data: 135
Training data: 173
---
Correct answer(s):
Test data: 233
Training data: 131

<-- 1.15 Quiz -->
Forecasting wine sales 
Question 3
What mean absolute error values do you get if the linear regression model uses only two attributes, Date and Lag_Red-12 (the sales of red wine for the same month last year)?
Select all the answers you think are correct.
Test data: 218
Test data: 225
Test data: 242
Training data: 175
Training data: 183
Training data: 242
---
Correct answer(s):
Test data: 242
Training data: 183
---
Feedback correct:
Examine the attributes used in the linear regression model near the top of the output panel. To remove superfluous lagged variables, click “Use custom lag lengths” in the Lag creation panel and select a minimum lag of 12. To eliminate Month attributes from the model, check “Customize” in the Periodic attributes panel – but leave the other boxes there unchecked. To eliminate the products of time and lagged variables, and the powers of time, from the model, uncheck the corresponding boxes under More options in the Lag creation panel.
Now the linear regression should use just two attributes: Date-remapped and Lag_Red-12.
---
Feedback incorrect:
Examine the attributes used in the linear regression model near the top of the output panel. To remove superfluous lagged variables, click “Use custom lag lengths” in the Lag creation panel and select a minimum lag of 12. To eliminate Month attributes from the model, check “Customize” in the Periodic attributes panel – but leave the other boxes there unchecked. To eliminate the products of time and lagged variables, and the powers of time, from the model, uncheck the corresponding boxes under More options in the Lag creation panel.
Now the linear regression should use just two attributes: Date-remapped and Lag_Red-12.

<-- 1.15 Quiz -->
Forecasting wine sales 
Question 4
Suppose you use the three attributes Date, Lag-12, and Lag-24.
What is the mean absolute error on the test data and the training data?
Select all the answers you think are correct.
Test data: 187
Test data: 221
Test data: 242
Training data: 170
Training data: 172
Training data: 175
---
Correct answer(s):
Test data: 221
Training data: 172
---
Feedback correct:
On the Lag creation panel, select the minimum and maximum lags to be 12 and 24 respectively, and type 12,24 into the “Fine tune lag selection” box.
---
Feedback incorrect:
On the Lag creation panel, select the minimum and maximum lags to be 12 and 24 respectively, and type 12,24 into the “Fine tune lag selection” box.

<-- 1.15 Quiz -->
Forecasting wine sales 
Question 5
Keeping the same parameter settings, predict both Dry-white and Red.
How many attributes are there in the transformed training data?
7
9
12
24
---
Correct answer(s):
7
---
Feedback correct:
The attributes in the transformed training data are Dry-white, Red, Date-remapped, Lag_Dry-white-12, Lag_Dry-white-24, Lag_Red-12 and Lag_Red-24

<-- 1.15 Quiz -->
Forecasting wine sales 
Question 6
In terms of the mean absolute error, does including the lagged variables for Dry-white make the prediction for Red …
better
exactly the same
worse
better on the training data, worse on the test data
better on the test data, worse on the training data
---
Correct answer(s):
exactly the same
---
Feedback correct:
The mean absolute error values for Red wine on the test data and training data are still 221 and 172 respectively, just as they were before. (They are much worse for Dry-white.)

<-- 1.15 Quiz -->
Forecasting wine sales 
Question 7
Instead of predicting Dry-white as well as Red, use Dry-white as overlay data.
(You won’t be able to generate a future forecast, so turn this off in the Output panel.)
How many attributes are there in the transformed training data?
3
5
7
12
---
Correct answer(s):
5
---
Feedback correct:
The attributes in the transformed training data are Dry-white, Red, Date-remapped, Lag_Red-12 and Lag_Red-24

<-- 1.15 Quiz -->
Forecasting wine sales 
Question 8
In terms of the mean absolute error, does using Dry-white as overlay data make the prediction for Red …
better
exactly the same
worse
better on the training data, worse on the test data
better on the test data, worse on the training data
---
Correct answer(s):
better
---
Feedback correct:
The mean absolute error values on the test data and training data are now 216 and 163, somewhat less than they were before (namely 221 and 172 respectively)

<-- 1.15 Quiz -->
Forecasting wine sales 
Question 9
Think about the differences between using an attribute as overlay data and using it as an additional target for prediction. Which of these statements do you agree with?
Select all the answers you think are correct.
An attribute’s lagged values can be used in the model if it is overlay data
An attribute’s lagged values can be used in the model if it is a target for prediction
The values of overlay data must be available for the instances that are being predicted
---
Correct answer(s):
An attribute’s lagged values can be used in the model if it is a target for prediction
The values of overlay data must be available for the instances that are being predicted

<-- 1.15 Quiz -->
Forecasting wine sales 
Question 10
Does adding all the other types of wine as overlay data improve the prediction for Red?
Yes
Only if you include Red as overlay data
No
---
Correct answer(s):
Yes
---
Feedback correct:
The mean absolute error values for test data and training data are now 207 and 145, less than they were before.
Of course, it would greatly improve the result if you could use the target variable as overlay data—because then prediction would be trivial! However, Weka will not allow you to select Red as overlay data—the target variable doesn’t appear in the list.

<-- 1.15 Quiz -->
Forecasting wine sales 
Question 10
Does adding all the other types of wine as overlay data improve the prediction for Red?
Yes
Only if you include Red as overlay data
No
---
Correct answer(s):

<-- 1.16 Video -->
Analyzing infrared data from soil samples 
Some feel that data miners focus too much on new methods and tiny improvements in accuracy, instead of on applications that will make a real difference in practice. Geoff Holmes discusses six major challenges that have been proposed for data mining applications: one is to save $100M through improved decision making. Inferring properties of soil samples from infrared data can save significant sums of money, because it can enable expensive wet chemistry to be replaced by an automatic process. However, achieving sufficient accuracy is a real challenge.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! My name is Geoff Holmes, and today’s lesson, is infrared data from soil samples. Before starting to talk about the actual application we’ll develop, I thought I’d just mention something about application development in general. The top academic conference in machine learning is called ICML, International Conference on Machine Learning. This is where all the top people in the field present their work. In 2012, a paper was published at this conference which was something of a wake-up call to the machine learning community. The author was Kiri Wagstaff from the Jet Propulsion Lab in Pasadena, CA, and the paper, which is accessible to anyone with an interest in machine learning, is called Machine Learning that Matters.
The URL there on the slide will enable you to download it and read it. What the paper does is it points out that the field is focusing too much on new methods and on the accuracy of those methods and less on the kind of application that will really make a difference. What Kiri did was to suggest six challenges for machine learning applications. I’m not going to go through all the six that are listed there on the slide. I just
want to talk about the highlighted one: $100M saved through improved decision making provided by an ML system. Now, believe it or not, you can develop an ML system using near-infrared data on soil samples that will be something that could save $100 million. This lesson is a starting point for such a system, but it is possible. Before we do that, let’s just take a moment to think about what machine learning requires in order for us to develop an application of any kind. Well, it needs input and output in its training phase.
In our case, we need a set of samples – those are going to be soil samples in some form, and you’ll see that in a while – and an output target value. In our case, this is going to be a real valued number, and will represent a property of interest of the soil. That could be organic carbon, organic nitrogen, available nitrogen, potassium. Something that we’re interested in predicting from the input. Our problem, of course, is to learn a mapping that describes the relationship between the input and the output. We refer to this mapping as a “model”.
We build the model on our training data, and then we use that model on unseen observations – new soil, if you like – in order to apply the model to the new soil in order for it to predict the target soil property of that soil that we’re interested in, such as the organic carbon. Now we need to think about where we’re going to get X and Y from for this particular application. Traditionally, soil samples are processed using techniques called “wet chemistry” techniques, and what those wet chemistry techniques are trying to do is determine the properties of the soil, such as available nitrogen, organic carbon and so forth. They will result in the Y values that we’re interested in.
What we need for this application is for a number of samples to have been processed using wet chemistry to determine these Y values for us. Let’s say we’re interested in available nitrogen. We need, let’s say 50 or 100 different soil samples to have been processed using wet chemistry to produce 50 to 100 Y values. We need to take a portion of each of those samples from, let’s say, a thing called a “soil bank”. Suppose we’ve got a soil bank. We divide our soil sample into half. We send half off to the wet chemistry lab to get the property determined, and with the other half, we put that through a near-infrared device. That will produce the X values for our input.
Now, the near-infrared device produces a signature, if you like, for the soil sample. I’ve got an example of one there below on the slide. These values will form the input. In the sense of an ARFF file, they represent the values or reflectance values for a given wavelength band. You’ll see in the ARFF file produced for the [quiz] that that starts at around 350 nanometers – that’s the first attribute. The next one might be 370 nanometers, 390 – or 80, 90, 400, 410, and so on.
The number of attributes we have, as you’ll see in the example, is something like 200 for each of those spectral wavelength bands, and then the values are numeric values, which are the amplitudes, if you like, of the spectrum, just the reflectance values that you get from the device. So as I said, you need a few hundred samples, so it’s not cheap, because you’ve got to send off – whatever number of samples you’ve got, it’s very cheap to get the X, but it’s expensive to get the Y, because you’ve got to send those off for wet chemistry analysis. So to put together a decent training set is expensive.
Given that, why would you bother doing that for the soil in this particular application? Well, once you’ve, let’s say, got your 50–100 samples and you’ve built your model, and if a farmer comes in with a new soil sample and says “I want to know what the available nitrogen is”, we just get out our available nitrogen model that we built and we get the NIR spectra for that new sample – that represents new X, if you like – we run it through the model, and it will produce an estimate of Y for that soil signature. We’ll be able to tell the farmer “for your soil sample, the available nitrogen is 4.3” (or whatever that estimated Y value is).
Instead of days for the wet chemistry to take place, we’re talking about milliseconds for the NIR device to produce the signature for us to run through the model and get the estimate of Y. That’s the first thing that makes it useful. It’s very fast. Second thing that makes it useful is that we can produce, for the same input, if we’ve got enough models, an estimate for a number of soil properties, not just one. If we’ve got, for example, wet chemistry which has determined the potassium, available nitrogen, the organic carbon, the organic nitrogen, and so on, then we can build models for each of those and for the same X value, we can produce predictions for each of those soil properties.
So we can tell the farmer with the soil sample in very short order – of the order of milliseconds – what the values are for each of those soil properties. All right, so that’s the value of it. How do we actually go about doing the modeling? Well, the training set, remember, let’s imagine it’s an ARFF file. The right-most column, or the class column, would be a set of numeric values, so we’re talking about a regression problem. Then the attributes are all these reflectance values at various wavelengths. They’re all numeric values, as well. We’ve got X numeric values, and so is Y.
The classifiers of interest are things such as LinearRegression, RepTree, model tree M5 prime, RandomForest, support vector machine regression, GaussianProcesses, and so on. What I’ve done there is lined up the algorithms in terms of their processing speed. What you’ll do in the [Quiz] activity is you’ll process the data using the first four, because you’ll see that it’s quite a large dataset, and the other two take too long really to be useful. We’ll be saying more about that later. The big thing – message – though is that pre-processing can make a big difference to a classifier’s performance. What you’ll do is process the data raw, and then you’ll see what happens to the results when you start applying the pre-processing techniques.
The classifiers respond in different ways to the different pre-processing techniques. Some get better, some get worse, some stay the same. One thing that’s worth bearing in mind is that you’re about to enter experimental machine learning, where you’re going to have lots of results, because the [Quiz] activity takes you through the first four classifiers on the previous slide, but all in default mode. Now, each of them has parameters that can be tweaked, and so can each form the basis for a separate experiment. You’ll be using four pre-processing methods, one of which is to do nothing, just use the raw spectrum. Now, some of those methods themselves have parameters, as well. Of course, you can combine the pre-processing methods, as well.
So the space of experiments is extremely large. From all of that, you’ll be able to produce some pretty good results. Now, what you’ll be looking at is particularly the correlation coefficient. So how well does the predicted value match the known value from the training data using cross-validation? That will give you some idea of how close you are, and what want, of course, is to produce models that get you close to 1.0, the perfect correlation with what you’ve seen in training data previously. Now, you’ll see that that’s not possible, because there’s too much error in the data typically. But it will be a starting point.
You’ll mainly see the improvement you can get from that baseline or benchmarking that you do with the raw data to what happens when you apply various pre-processing techniques. I hope you enjoy that. I hope it wets your appetite for machine learning application development.
<End Transcript>

<-- 1.17 Discussion -->
Challenges for machine learning applications
At the beginning of the last video Geoff introduced six challenges for machine learning applications:
  A law passed or legal decision made that relies on the result of an ML analysis
  $100M saved through improved decision making provided by an ML system
  A conflict between nations averted through high-quality translation provided by an ML system
  A 50% reduction in cybersecurity break-ins through ML defences
  A human life saved through a diagnosis or intervention recommended by an ML system
  Improvement of 10% in one country’s Human Development Index (HDI).
What do you think of these? Can you come up with other important challenges?

<-- 1.18 Quiz -->
Analyzing a soil sample 
Question 1
The dataset has been converted into an ARFF file called org_c_n.arff.
Load it into the Weka Explorer. The instances represent 4439 samples of soil that have been processed by a NIR (near-infrared) device. Most of the 220 attributes are wave bands, and contain the reflectance values produced by the device. For our purposes the dataset should contain only the wave bands plus the class we are interested in, and for this activity we will concentrate on organic carbon.
Remove the unnecessary attributes from the dataset. How many attributes remain?
131
172
217
254
---
Correct answer(s):
217
---
Feedback correct:
You should remove Batch_Labid, ISO, and OrganicNitrogen
---
Feedback incorrect:
You should remove Batch_Labid, ISO, and OrganicNitrogen

<-- 1.18 Quiz -->
Analyzing a soil sample 
Question 2
There is still a problem with the dataset.
If you click on the class attribute, OrganicCarbon, you will see that 12% of the values are missing. These are samples for which there was no wet chemistry reference, and are useless for our purpose. Use an appropriate Weka instance filter to remove all instances whose class attribute is missing.
How many instances remain?
528
1119
1872
3911
---
Correct answer(s):
3911
---
Feedback correct:
Use the RemoveWithValues filter
528 instances with missing values for OrganicCarbon are removed
---
Feedback incorrect:
Use the RemoveWithValues filter

<-- 1.18 Quiz -->
Analyzing a soil sample 
Question 3
We now set about benchmarking.
The class is numeric, making this a regression problem. A simple classifier for regression problems is LinearRegression. Choose this in the Classify panel, along with 10-fold cross-validation (the default).
What correlation coefficient does LinearRegression achieve?
0.2547
0.3951
0.4215
0.4789
---
Correct answer(s):
0.3951

<-- 1.18 Quiz -->
Analyzing a soil sample 
Question 4
Next we investigate the performance of some more sophisticated classifiers: M5P, REPTree and RandomForest. (There are other possibilities, but they are all slower.)
Run these three with default settings, and record the resulting correlation coefficients. What is the best correlation coefficient achieved?
0.3951
0.4284
0.6871
0.7412
---
Correct answer(s):
0.6871
---
Feedback correct:
The largest correlation coefficient is 0.6871, achieved by RandomForest

<-- 1.18 Quiz -->
Analyzing a soil sample 
Question 5
We now examine the effect of preprocessing the data, using the results of these classifiers as a benchmark.
We investigate three commonly used techniques for NIR data: downsampling, row normalisation and a signal smoothing method called Savitzky-Golay.
Downsampling is a simple method that can accelerate processing with little loss in accuracy (this may also allow slower classification methods to be applied without too much delay). By hand, remove every second attribute, W350, W370, … W2490. The resulting dataset will have 109 attributes including the class (you may wish to save it). Run the benchmark classifiers (again with default settings), along with 10-fold cross-validation. You will probably notice that they are faster than before.
We will continue to use the correlation coefficient as the measure of success. Which methods perform better on the downsampled version than on the original data?
Select all the answers you think are correct.
LinearRegression
M5P
REPTree
RandomForest
---
Correct answer(s):
LinearRegression
M5P
REPTree
RandomForest
---
Feedback correct:
Linear regression: 0.400 on the downsampled data; 0.395 on the original
M5P: 0.630 on the downsampled data; 0.603 on the original
REPTree: 0.654 on the downsampled data; 0.653 on the original
RandomForest: 0.695 on the downsampled data; 0.687 on the original
---
Feedback incorrect:
Linear regression: 0.400 on the downsampled data; 0.395 on the original
---
Feedback incorrect:
M5P: 0.630 on the downsampled data; 0.603 on the original
---
Feedback incorrect:
REPTree: 0.654 on the downsampled data; 0.653 on the original
---
Feedback incorrect:
RandomForest: 0.695 on the downsampled data; 0.687 on the original

<-- 1.18 Quiz -->
Analyzing a soil sample 
Question 6
Downsampling has improved both speed and accuracy for all these classifiers.
Let’s keep going: make the dataset half the size again! Construct a new dataset with 55 attributes: the class and wavebands W380, W420, W460, … W2500. Run the benchmark again.
Which methods now perform better than they did on the first downsampling data?
Select all the answers you think are correct.
LinearRegression
M5P
REPTree
RandomForest
---
Correct answer(s):
M5P
REPTree
RandomForest
---
Feedback correct:
M5P: 0.690 on the new data; 0.630 on the previous data
REPTree: 0.658 on the new data; 0.654 on the previous data
RandomForest: 0.707 on the new data; 0.695 on the previous data
---
Feedback incorrect:
Linear regression: 0.391 on the new data; 0.401 on the previous data
---
Feedback incorrect:
M5P: 0.690 on the new data; 0.630 on the previous data
---
Feedback incorrect:
REPTree: 0.658 on the new data; 0.654 on the previous data
---
Feedback incorrect:
RandomForest: 0.707 on the new data; 0.695 on the previous data

<-- 1.18 Quiz -->
Analyzing a soil sample 
Question 7
Next we look at row normalization, a “scatter correction” technique that is designed to address the problem of baseline effects to which all NIR devices are susceptible.
Unfortunately, Weka does not (yet!) have a filter for row normalization, so we provide a new dataset, org_c_no_missing-rn.arff. Load it into Weka and run the benchmark again. Note that this method does not remove data, so we are back to 217 attributes (including the class).
Row normalization improves results for two of the methods, compared to the original (non-downsampled) result. Which ones?
Select all the answers you think are correct.
LinearRegression
M5P
REPTree
RandomForest
---
Correct answer(s):
LinearRegression
M5P
---
Feedback correct:
LinearRegression: 0.513 on the filtered data; 0.395 on the raw data
M5P: 0.703 on the filtered data; 0.603 on the raw data
---
Feedback incorrect:
LinearRegression: 0.513 on the filtered data; 0.395 on the raw data
---
Feedback incorrect:
M5P: 0.703 on the filtered data; 0.603 on the raw data
---
Feedback incorrect:
REPTree: 0.441 on the filtered data; 0.653 on the raw data
---
Feedback incorrect:
RandomForest: 0.624 on the filtered data; 0.687 on the raw data

<-- 1.18 Quiz -->
Analyzing a soil sample 
Question 8
The spectral derivative is a third preprocessing tool: it smooths the spectral signal.
One of the most prominent methods, called Savitzky-Golay, corrects (smooths) each point using a fixed-width window centered on the point; the window’s width is a parameter. Again, this method is not in Weka, so we have produced datasets that smooth the signal using windows of two different sizes, 7 points (3 either side) and 11 points (5 either side): org_c_no_missing-sg7.arff and org_c_no_missing-sg11.arff. Upon loading them you will notice that the wave bands have been replaced with generic names because the technique has altered the original attributes. Run the benchmark once more.
What is the best technique when Savitzky-Golay preprocessing is used?
LinearRegression
M5P
RepTree
RandomForest
---
Correct answer(s):
RandomForest
---
Feedback correct:
Savitzky-Golay achieves a correlation coefficient of 0.865 for the 7-point-window dataset and 0.8556 for the 11-point-window dataset, both of which exceed the results from the other methods

<-- 1.18 Quiz -->
Analyzing a soil sample 
Question 9
One of these window sizes is better across all classifiers. Which one?
7
11
---
Correct answer(s):
7

<-- 1.18 Quiz -->
Analyzing a soil sample 
Question 10
We have seen that preprocessing can make a big difference to the performance of a classifier.
So far, three different techniques have been applied independently. What if we combine them? We downsampled the original file by removing every second attribute, then applied Savitzky-Golay, then row normalization, to produce org_c_no_missing-d2sg7rn.arff. Load this dataset and re-run the benchmark.
For one of the classifiers, this combination produces the best result of all preprocessing techniques. Which one?
LinearRegression
M5P
RepTree
RandomForest
---
Correct answer(s):
LinearRegression
---
Feedback correct:
It gives a correlation coefficient of 0.6734, which is the best result we have obtained for LinearRegression. (However, it’s not a very good result compared to what the other classifiers achieve.)

<-- 1.18 Quiz -->
Analyzing a soil sample 
Question 10
We have seen that preprocessing can make a big difference to the performance of a classifier.
So far, three different techniques have been applied independently. What if we combine them? We downsampled the original file by removing every second attribute, then applied Savitzky-Golay, then row normalization, to produce org_c_no_missing-d2sg7rn.arff. Load this dataset and re-run the benchmark.
For one of the classifiers, this combination produces the best result of all preprocessing techniques. Which one?
LinearRegression
M5P
RepTree
RandomForest
---
Correct answer(s):

<-- 1.19 Article -->
Avenues for further investigation
You can perform much more experimentation in search of a good model!
For example, we have not examined the effects of parameter changes in either the classifiers or the preprocessing techniques (except for the Savitzky-Golay window size).
One problem faced in all application development is knowing when a result is good enough to be useful in practice. In our experience, the correlation coefficient needs to increase to 0.95–0.99 for this problem. Our best result in this activity is 0.87, still a long way off. Another important factor that we have not explored is the effect of outliers in regression problems. Filtering out outlier instances can make a huge difference to performance.

<-- 1.20 Discussion -->
Reflect on this week’s Big Question
This week’s Big Question is, “How can you use data mining to foretell the future?”
We promised that by the end you’d be able to explain the role of “lagged variables” in time-series analysis. You’d be experienced in the use of Weka’s time series forecasting package, and be able to work with data that varies on an hourly, daily, weekly, monthly, and yearly basis. You’d understand that the standard holdout and cross-validation methods simply do not work for time series, and know what to do about it. And you’d be able to explain what “overlay data” is and how valuable it can be. Oh yes, and you’ll have even more experience of that perennial problem of overfitting, and how to detect it.
Well, what do you think now? Time series forecasting is foretelling the future, though admittedly in a rather limited sense. Though we have used the standard prediction methods of linear regression and SMO, time series require different attributes and different evaluation methods. Cross-validation is no longer useful. Continued extrapolation is, but its accuracy deteriorates as the forecasting period increases.

<-- 1.21 Article -->
Index
At the end of each week is an index of topics covered that week.
A full index to the course appears under DOWNLOADS, below.
      Topic
      Step
      Datasets
      airline
      1.7, 1.8, 1.9, 1.10, 1.11, 1.12, 1.13
      appleStocks
      1.13
      Financial data for GOOG
      1.14
      Infrared data from soil samples
      1.16, 1.18
      sentiment
      1.4
      Soil dataset (org_c_n)
      1.18
      wine
      1.15
      Classifiers
      LinearRegression
      1.7, 1.9, 1.10, 1.11, 1.15, 1.18
      M5P
      1.18
      NaiveBayesMultinomial
      1.4
      OneR
      1.4
      RandomForest
      1.18
      REPTree
      1.18
      SMOreg
      1.8, 1.10, 1.11
      ZeroR
      1.4
      Metalearners
      FilteredClassifier
      1.9
      Filters
      AddExpression
      1.8
      Copy
      1.7
      Remove
      1.9
      RemoveMisclassified
      1.4
      RemoveRange
      1.7, 1.8
      Resample
      1.4
      TimeSeriesTranslate
      1.7, 1.8
      Packages
      timeseriesForecasting
      1.9
      Plus …
      Downsampling
      1.18
      Forecast panel
      1.9, 1.10, 1.11, 1.12
      International Conf. on Machine Learning (ICML)
      1.16
      Lagged variables
      1.6–1.15
      Overlay data
      1.13
      Row normalization
      1.18
      Savitzky-Golay smoothing
      1.18
      Time series analysis
      1.6–1.15

<-- 2.0 Todo -->
Data stream mining 
How can you mine continuous data streams?
This week's Big Question!
2.1
How can you mine continuous data streams?
article
Incremental classifiers in Weka
Weka contains some classifiers that operate incrementally rather than in batch mode, like the Hoeffding Tree method for decision trees. Incremental classifiers don’t have to store the training data in memory.
2.2
Incremental classifiers in Weka
video (05:12)
2.3
Three incremental classifiers
quiz
Weka's MOA package 
Weka’s MOA package gives access to classification algorithms from the MOA system, which can deal with huge datasets and handle evolving data streams where the concept drifts over time.
2.4
Weka's MOA package 
video (03:30)
2.5
Using MOA from Weka
quiz
The MOA interface 
The MOA system, a sister of Weka, operates fully incrementally, on data streams. Different evaluation methods are needed for stream data.
2.6
The MOA interface 
video (05:33)
2.7
Using the MOA interface
article
2.8
Using MOA
quiz
Dealing with change
An adaptive sliding window algorithm can detect change and update statistics from a data stream. The Hoeffding adaptive tree constructs “alternative branches” to prepare for changes. Bagging can be adapted for stream data.
2.9
MOA classifiers and streams 
video (08:14)
2.10
Streaming classifiers and evolving streams
quiz
Classifying tweets 
Twitter is BIG data! One application is sentiment analysis: does a tweet convey a positive or negative feelings? The Kappa statistic is often a more robust and useful measure than percent accuracy.
2.11
Classifying tweets 
video (05:45)
2.12
Sentiment analysis
discussion
2.13
Classifying tweets
quiz
Signal peptide prediction 
What are important problems in bioinformatics? Signal peptide prediction is one. 
How can features be created from raw DNA sequence data?
And how can we evaluate the results?
2.14
Signal peptide prediction
video (17:20)
2.15
Signal peptide prediction
quiz
2.16
Reflect on this week’s Big Question
discussion
How are you getting on?
We're well into the course now. Let's just take stock.
2.17
Mid-course assessment 
test
This is a test step, it helps you verify your understanding. If you want to earn a Certificate of Achievement on this course you need to complete this test and any others, scoring an average of 70% or above.
To take tests you need to upgrade this course.  It costs $94, which also gets you:
Unlimited access to the course for as long as it exists on FutureLearn, so you can learn at your own pace
A Certificate of Achievement when you’re eligible, to prove what you’ve learned
Upgrade
Find out more
2.18
How are you getting on?
discussion
2.19
Index
article

<-- 2.1 Article -->
How can you mine continuous data streams?
This week we look at mining streams of data in real time.
Imagine taking input from a sensor, or from a webcam, or from a Twitter feed. It keeps coming in, continually, inexorably – forever! And a data mining system must keep up with this, creating and maintaining a model that you can inspect at any time – but which changes continuously as new data comes in. There’s no limit. This is bigger than big data: it’s potentially infinite.
How can you deal with this? First, let’s make clear what we will not be tackling. We won’t be talking about how to connect sensors to a data mining program, nor how to deploy models that affect the world on a continuous basis. And we’ll introduce algorithms that are real-time in principle, ones that update their models in a time that is independent of the volume of data already seen – but we won’t concern ourselves with implementing them to work fast enough to keep up with the data produced by any particular sensor.
That still leaves plenty of open questions. For example, can you make a stream-oriented decision tree classifier? You could force J48 to work incrementally by re-running it from scratch as each new data sample arrives. But that couldn’t possibly work in real time, because no matter how quickly it operates there will come a point where the volume of data overwhelms its ability to produce a new model before the next data sample arrives. And anyway, data streams tend to evolve as time goes on, so it’s probably inappropriate to build a model of the entire stream right back to the beginning of time. And what about evaluation? Our trusty method of cross-validation won’t work any more, because you will never have the full dataset. What can take its place?
Our end-of-week example is an application of data mining to bioinformatics. We focus on signal peptide prediction. What’s the problem? And how can appropriate features be created from raw DNA?
At the end of this week you will be able to explain, at a high level, how decision trees can be modified incrementally, and compare the performance of incremental and non-incremental decision tree algorithms. You’ll be able to use Weka’s MOA (Massive Online Analysis) package. And you’ll be able to use the MOA system itself, which not only contains stream-oriented implementation of many packages, but allows different evaluation techniques designed for incremental operation. You’ll know the difference between holdout and prequential evaluation. You’ll know about adaptive windowing and how to use it for change detection. You’ll also have some experience of sentiment analysis using Twitter data.

<-- 2.2 Video -->
Incremental classifiers in Weka
Albert Bifet introduces data stream mining. It requires incremental operation rather than the batch mode used so far. Weka includes many different incremental methods. Updating decision trees presents an interesting challenge that is solved using the “Hoeffding bound” to estimate how many instances should be examined before deciding whether to split a node. Incremental methods like this typically require more training data to reach a given level of accuracy than batch-mode ones, but they can be very fast, and use much less memory.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello! My name is Albert Bifet. I’m a member of the Weka machine learning group and I work at Telecom ParisTech in Paris, France. We’re going to talk about data stream mining in Weka and MOA. Data stream mining is a way of doing real-time analytics, so this is going to be very, very important for big data and the Internet of Things. As you know, in Weka usually what we do is that we store all the dataset in memory and then what we do is that we build our classifier using this dataset that is stored in memory. This is what is called a batch setting.
In the incremental setting, what we do is that we update our classifier one instance by one instance. There’s a huge difference between the two settings. Let’s look at the incremental setting with more detail. So the idea is that, what we do is we process an example every time, so only one example, not a dataset of examples. We use a limited amount of memory, because we don’t store all the dataset in memory. We work in a limited amount of time, so we need to be very, very fast. And we are ready to predict at any point. That’s the main difference.
So in the batch setting what we do is that we process all the dataset and then we are ready to predict at the end of building the classifier. Weka has many different incremental methods. To know which ones are incremental, we need to look that they should implement the UpdateableClassifier interface. We can find many different methods, as NaiveBayes, NaiveBayesMultinomial, k-nearest neighbors, Stochastic Gradient Descent, and also some decision trees, such as the Hoeffding Tree. As you know, the standard decision tree is not incremental, so we need to have all the dataset in memory. The Hoeffding Tree is the first – is a state-of-the-art incremental decision tree learning.
The Hoeffding Tree was proposed by Pedro Domingos and his group around 2000, and the difference with the standard decision trees and in the standard decision tree what we do is that when we need to decide when we want to split or not, what we do is that we look at the data that we have memory and then we compute the statistics and then we decide if we split or not. But, in the incremental setting, we don’t store the dataset in memory, so then, what we need to do if we need to decide if we want to split or not, we need to wait for new instances to arrive.
So how many instances do we need to decide if we need to split or not? This is something that is computed using the Hoeffding bound, and this is why the Hoeffding tree has this name.Another interesting thing of the Hoeffding Tree is that it has theoretical guarantees that if the number of instances that we are using to build the model is large enough, the decision tree is going to be similar to a decision tree built using the standard decision tree method. Let’s see an example in Weka. We’re going to generate a dataset and then we’re going to evaluate using a Hoeffding Tree. Let’s start generating the data. We are going to use the Random radial basis function generator.
This is a generator that generates data by creating for each class a random set of centers. In this case, we’re going to use 50 centroids, 10 attributes, 2 classes. We’re going to create 1000 instances. Let’s generate the data. Now we have the data, 1000 instances. Now, let’s classify it using the Hoeffding Tree. Let’s choose the classifier. Hoeffding Tree. We’re going to run a 10-fold cross-validation. We see this is very fast, and we get an accuracy of 71%.Now, let’s do it again but generating 100,000 instances. We change this parameter. We generate the data. Then again we run the 10-fold cross-validation. We see that it takes more time, but still it’s really fast.
At the end, what we get is the accuracy goes to 89%. With 100,000 instances, we get 89% and with 1000 instances, we were getting only 71%. Increasing the number of instances that the Hoeffding Tree processes that allows us to have a much better accuracy. In this lesson, we have seen the two settings of Weka, the batch setting and the incremental setting. In the batch setting what we did is that we stored all of the dataset in memory. In the incremental setting, what we did is that we built the classifier one instance by one instance. The nice thing about the incremental setting is that we can be much more efficient and that we use less memory and we are much faster.
<End Transcript>

<-- 2.3 Quiz -->
Three incremental classifiers
Question 1
Create a stream of data using the LED24 generator that is accessible from the Generate button in WEKA’s Preprocess panel.
Configure the generator to create 10,000 instances. Run a 10-fold cross-validation on this data using the J48 and HoeffdingTree classifiers.
What are the percentages of correctly classified instances for J48 and HoeffdingTree?
Select all the answers you think are correct.
66.0% for both
71.3% for Hoeffding Tree
72.7% for J48
73.8% for J48
73.8% for Hoeffding Tree
73.9% for Hoeffding Tree
---
Correct answer(s):
72.7% for J48
73.8% for Hoeffding Tree

<-- 2.3 Quiz -->
Three incremental classifiers
Question 2
Generate a stream of 1,000,000 instances from the same source.
To save time, perform a percentage split evaluation (default parameter) instead of cross-validation.
Repeat the experiment with J48 and Hoeffding Tree. Which is faster, and which is more accurate?
Select all the answers you think are correct.
J48 is faster
Hoeffding Tree is faster
J48 is more accurate
Hoeffding Tree is more accurate
---
Correct answer(s):
Hoeffding Tree is faster
Hoeffding Tree is more accurate

<-- 2.3 Quiz -->
Three incremental classifiers
Question 3
Repeat the experiment (1,000,000 instances, percentage split evaluation) with NaiveBayesUpdateable and IBk.
Which is faster?
NaiveBayesUpdateable
IBk
---
Correct answer(s):
NaiveBayesUpdateable
---
Feedback correct:
They both build the model quickly, but IBk takes a long time to evaluate test instances – so long that I’m sure you didn’t wait for it to terminate!

<-- 2.3 Quiz -->
Three incremental classifiers
Question 4
IBk builds the model quickly because it just stores all instances in memory, but is slow at predicting because for each prediction it calculates the distance to all the instances.
One way to accelerate this is to retain in memory only a “window” of instances, rather than the full dataset. The windowSize parameter sets the maximum number of instances allowed in the training pool – adding further instances simply removes older ones. The default value of 0 signifies no limit to the number of training instances stored.
Repeat the first experiment (10,000 instances, 10-fold cross-validation) using IBk with window sizes 100, 1000, and 10,000. Which gives best performance in terms of accuracy?
100
1000
10,000
---
Correct answer(s):
10,000
---
Feedback correct:
This yields an accuracy of 57.2%

<-- 2.3 Quiz -->
Three incremental classifiers
Question 5
Which is fastest?
100
1000
10,000
---
Correct answer(s):
100
---
Feedback correct:
Duh! I guess that’s kinda obvious …

<-- 2.3 Quiz -->
Three incremental classifiers
Question 6
With IBk you can specify different nearest neighbor search algorithms.
Repeat the last experiment (window size of 10,000) with KDTree as the search algorithm. Does this increase accuracy?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
The accuracy stays exactly the same, at 57.2%

<-- 2.3 Quiz -->
Three incremental classifiers
Question 7
The covtypeNorm dataset gives the forest cover type for 30 x 30 meter cells obtained from the US Forest Service Resource Information System.
It contains 581,012 instances and 54 attributes, and has been used in several papers on data stream classification.
Load covtypeNorm.arff into the Weka Explorer and run a percentage split (default value) evaluation with Hoeffding Tree. What is the percentage of correctly classified instances?
73.8%
74.3%
74.4%
74.5%
91.8%
93.6%
---
Correct answer(s):
74.4%
---
Feedback correct:
(More precisely, 74.3693.) This is not a very good result. HoeffdingTree is very conservative and need lots of data before it yields reasonable accuracy.
---
Feedback incorrect:
That’s the result using 10-fold crossvalidation (bet it took a long time!)
---
Feedback incorrect:
That’s the result using J48. My computer took 10 mins to build the model (vs 1 min for HoeffdingTree), and the tree is huge (30,000 nodes vs 450 for HoeffdingTree).

<-- 2.3 Quiz -->
Three incremental classifiers
Question 7
The covtypeNorm dataset gives the forest cover type for 30 x 30 meter cells obtained from the US Forest Service Resource Information System.
It contains 581,012 instances and 54 attributes, and has been used in several papers on data stream classification.
Load covtypeNorm.arff into the Weka Explorer and run a percentage split (default value) evaluation with Hoeffding Tree. What is the percentage of correctly classified instances?
73.8%
74.3%
74.4%
74.5%
91.8%
93.6%
---
Correct answer(s):

<-- 2.4 Video -->
Weka's MOA package 
MOA is open source software that is specifically designed for mining data streams. It can handle evolving data streams – ones generated by mechanisms that change, or drift, over time. Like Weka, it contains many data mining methods. Weka’s MOA package gives access to these methods through the Weka Explorer. By the way, the moa was a huge flightless bird that lived in New Zealand until about 500 years ago.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
What is MOA? MOA is open source software that is specifically designed for mining data streams. The most important thing about MOA is that it can handle evolving data streams, data streams that are changing, data streams with concept drift. MOA has many methods – from classification, regression, clustering, frequent pattern mining, outlier detection, concept drift – and is very easy to use and very easy to extend. MOA can be used alone or can be used with Weka or can be used with ADAMS and with MEKA. ADAMS is a very nice workflow engine where you can develop your workflows, and MEKA is a software specific for multi-label learning. MOA runs on a single computer.
In the case that you need to data stream mining in a cluster of computers, like the ones that are used in the Hadoop ecosystem, then Apache SAMOA could be the right tool for you. SAMOA allows you to run stream mining jobs on Apache Storm, Apache S4, Apache Samza, and nowadays Apache Flink. As you know, New Zealand is very famous for birds, so the weka is a native bird of New Zealand. The moa is also a native bird of New Zealand. It’s also flightless as the weka, but nowadays is extinct. As you can see in these pictures, the moa was a very large bird.
If you compare it here with the size of a weka or a kiwi, we see that the moa was really large, or here comparing with the size of a human person. Let’s see how to install the MOA package in Weka. We’re going to go to Tools. We’re going to open the Package Manager. From there, we are going to look for the Massive Online Package. Let’s look for it. We should go to the “m”. MassiveOnlineAnalysis. Then we install it. We say “Yes”. OK. And once it’s installed, we go to the Explorer, and then we can see the objects of MOA inside Weka. So there are two types. One is the data generators and the other are the classifiers.
For example, if we went to generate data now, we can look at the MOA generator, and from the MOA generator we can access all the data stream generators inside MOA. OK. Let’s choose one for example. I will generate some data. The other important thing is that we can have access to all the MOA classifiers. So we go inside classifiers inside meta we’ll find this MOA classifier, and from this MOA classifier, we can get access to all the classifiers in MOA. Here are all the classifiers in MOA. Let’s choose, for example, a meta classifier. We can choose the online bagging that’s called OzaBag. And that’s all. In this lesson, we have seen how to install the MOA package in Weka.
We have seen that MOA is open source software specifically designed for data stream mining. It handles evolving data streams, and it has many, many different methods for clustering, classification, regression, frequent pattern mining, outlier detection, and concept drift. It is very easy to use and very easy to extend.
<End Transcript>

<-- 2.5 Quiz -->
Using MOA from Weka
Question 1
Find the MOA data generator, which now appears when you click the Generate button in WEKA’s Preprocess panel.
Create 1,000,000 instances using MOA’s hyperplane generator (click the Edit button beside “generator” and select HyperplaneGenerator from the dropdown list at the top).
MOA’s classifiers are in WEKA’s meta category. First select MOA, and then configure it, selecting the Hoeffding Tree classifier – again by clicking the Edit button and selecting from the dropdown list at the top.
Perform a percentage split evaluation (default parameter) using MOA’s Hoeffding Tree classifier. Repeat the evaluation using WEKA’s Hoeffding Tree classifier (in WEKA’s trees category).
Which is faster, and which is more accurate?
Select all the answers you think are correct.
WEKA’s Hoeffding Tree is faster
MOA’s Hoeffding Tree is faster
WEKA’s Hoeffding Tree is more accurate
MOA’s Hoeffding Tree is more accurate
---
Correct answer(s):
MOA’s Hoeffding Tree is faster
WEKA’s Hoeffding Tree is more accurate
---
Feedback correct:
Weka’s HoeffdingTree gives 90.2715% classification accuracy; Moa’s gives 89.7818%
(However, the difference is unlikely to be significant.)
---
Feedback incorrect:
Weka’s HoeffdingTree gives 90.2715% classification accuracy; Moa’s gives 89.7818%
(However, the difference is unlikely to be significant.)

<-- 2.5 Quiz -->
Using MOA from Weka
Question 2
The elecNormNew dataset contains data collected from the Australian New South Wales Electricity Market, where prices change every 5 minutes according to supply and demand.
It contains 45,312 instances and 9 attributes, including the class label, which indicates whether the price is UP or DOWN relative to a moving average over the past 24 hours.
Load elecNormNew.arff into the Weka Explorer and run a 10-fold cross-validation using MOA’s Hoeffding Tree. What is the percentage of correctly classified instances?
73.8%
74.3%
74.4%
76.6%
76.8%
76.9%
---
Correct answer(s):
76.6%
---
Feedback correct:
(More precisely, 76.6199%)
---
Feedback incorrect:
That’s the result using Percentage split, not cross-validation
---
Feedback incorrect:
That’s the result using Weka’s Hoeffding Tree. You should instead select meta > MOA, and then configure the MOA wrapper to use moa.classifiers.trees.HoeffdingTree instead of the default, which is trees.DecisionStump.

<-- 2.5 Quiz -->
Using MOA from Weka
Question 3
Hoeffding option trees are regular Hoeffding trees equipped with additional “option” nodes that allow several tests to be applied.
They consist of a single structure that efficiently represents multiple trees: a particular example can travel down several paths, contributing, in different ways, to different options. Run a 10-fold cross-validation using MOA’s HoeffdingOptionTree classifier. What is its accuracy?
76.4%
76.5%
76.6%
76.8%
76.9%
77.0%
---
Correct answer(s):
77.0%
---
Feedback correct:
(More precisely, 77.0458%)
---
Feedback incorrect:
That’s the result using Percentage split, not cross-validation

<-- 2.5 Quiz -->
Using MOA from Weka
Question 4
The MOA classifier called meta.OzaBag (its full name is moa.classifiers.meta.OzaBag) is an online bagging method for Hoeffding trees.
Use this classifier with HoeffdingTree as the base learner (the default) to bag 10 trees (the default). How does the accuracy (again evaluated using 10-fold cross-validation) compare with that of a single Hoeffding option tree?
Better
Worse
---
Correct answer(s):
Better
---
Feedback correct:
OzaBag (with 10 regular Hoeffding trees) gets 77.6%, whereas Hoeffding option trees get 77.0%.

<-- 2.5 Quiz -->
Using MOA from Weka
Question 5
Let’s see how the performance of these online methods improves with more data. Check the accuracy of MOA’s HoeffdingOptionTree under these two conditions:
    Apply Weka’s Resample filter (unsupervised version) to make an oversampled version of the elecNormNew dataset, increasing its size 10 times (use a 1000% sample size, with replacement)
    Increase the original dataset’s size 40 times (undo the previous filtering operation and repeat it for a 4000% sample)
What are the two accuracies?
Select all the answers you think are correct.
81.6% with a ×10 dataset
81.6% with a ×40 dataset
87.3% with a ×10 dataset
87.3% with a ×40 dataset
88.3% with a ×40 dataset
---
Correct answer(s):
81.6% with a ×10 dataset
87.3% with a ×40 dataset

<-- 2.5 Quiz -->
Using MOA from Weka
Question 6
An alternative to oversampling is to bag online classifiers using the original dataset.
With the original elecNormNew dataset (without oversampling), use MOA’s online classifiers to bag several Hoeffding option trees (note: option trees – you will need to change the base learner in OzaBag), first 10, and then 40 of them (again, with 10-fold cross-validation).
What is the accuracy of online bagging of 40 Hoeffding option trees?
77.61%
77.84%
78.30%
78.32%
---
Correct answer(s):
78.32%
---
Feedback incorrect:
Are you sure you used 40 trees, and Hoeffding option trees rather than Hoeffding trees?
---
Feedback incorrect:
Are you sure you used Hoeffding option trees rather than Hoeffding trees as the base learner?
---
Feedback incorrect:
This is the result for 10 trees: are you sure you used an ensemble size of 40?

<-- 2.5 Quiz -->
Using MOA from Weka
Question 7
In this case, does increasing the number of instances by oversampling the dataset by a certain factor (10 or 40) yield better performance than increasing the number of classifiers by the same factor (10 or 40) and bagging them?
No
Yes, when the factor is 10 (not 40)
Yes, when the factor is 40 (not 10)
Yes, for both values
---
Correct answer(s):

<-- 2.5 Quiz -->
Using MOA from Weka
Question 7
In this case, does increasing the number of instances by oversampling the dataset by a certain factor (10 or 40) yield better performance than increasing the number of classifiers by the same factor (10 or 40) and bagging them?
No
Yes, when the factor is 10 (not 40)
Yes, when the factor is 40 (not 10)
Yes, for both values
---
Correct answer(s):
Yes, for both values

<-- 2.6 Video -->
The MOA interface 
We download MOA and run it. Incremental data stream mining calls for different evaluation methods from batch operation. One possibility is to interleave training and testing by periodically holding out some test data from the input stream (called “periodic” evaluation); another is to test the current classifier on each new instance in the stream before using it for training (called “prequential” evaluation). MOA, which can be invoked through its interactive interface or from the command line, includes many data stream generators. Here we run the HoeffdingTree algorithm on data from the HyperplaneGenerator, and evaluate it both periodically and prequentially.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
MOA can be used in three different ways, using the graphical user interface, the command line, or the Java API. Let’s start with classification evaluation. In batch setting,
we have two different types of evaluation: holdout, when we have different data for testing and training, or 10-fold cross-validation, when we are using the same data for testing and training.
In the incremental setting, what we have is that we have two types: holdout evaluation and also prequential evaluation. Let’s look at these two types of evaluation. In the holdout evaluation, what we are doing is that we are training our model one instance by one instance and then, periodically, we are doing an evaluation testing using different instances. In the prequential evaluation, what we are doing is that we are using the same data for testing and training. In that sense, what we are doing is that we are testing and training every one of the instances of the stream. Every time a new instance arrives, first we test and then we train. Let’s look at the MOA interface.
First, we’re going to download the software, so let’s go to the MOA webpage [moa.cms.waikato.ac.nz]. From the MOA webpage, what we are going to do is go to Downloads, and from there, we are going to download the last release.
OK. Once we have the – once MOA is downloaded, we can run it from the bin folder if it’s in Windows using moa.bat and, if not, using moa.sh. Let’s run it. What we see is that we have several tabs. One is for classification, the other is for regression, also for clustering, outliers, and concept drift. Let’s start with classification. Let’s run a task. We’re going to run an evaluation task. Let’s start with a holdout evaluation, with EvaluatePeriodicHeldOutTest. We need to specify the learner, in this case it’s going to be the HoeffdingTree. What’s the stream? In this case, we’re going to select the HyperplaneGenerator.
OK, and then how many instances we want to use for testing, in that case we say that we want to use 1000 instances and we want to train 1,000,000 instances. And we want to see the results every 10,000 instances. OK. That’s the definition of the task. We see that it’s here specified, EvaluatePeriodicHeldout, and then we run it. We see that here, we have all the results. And here we see that there’s a plot of these results where we have also the different measures, like accuracy, kappa. OK, now let’s run a prequential evaluation. Again, we change the task. We’re going to change to EvaluatePrequential. We’re going to define again what’s the learner, in this case, it’s going to be the HoeffdingTree.
OK. Then the stream, we’re going to select the HyperplaneGenerator. OK, and then we’re going to train 1,000,000 instances, and we’re going to look at the results every 10,000 instances. Now we run the task. Here we see the results, and here we see the evolution of these measures, and now the nice thing is that we can compare both. If we look at this, we see that one appears in red and the other appears in blue. We can take a look at that and we can also zoom it to look at it in more detail. Another way to use MOA is using the command line.
We can reuse the command line that we have in the graphical user interface, when we were selecting what was the task that we want to run. We can use the same text, and we can put it inside the command line. What we are doing then is that we are executing the task using this moa.DoTask. Then, we need only to specify what is the task, what is the learner that we want to use, what is the stream we want to use, how many instances we want to use. In this lesson, we have seen how to use the MOA interface. We know that there are three different ways. We have the graphical user interface, the command line, and the Java API.
Also, we have seen the two types of evaluation for incremental learning. That is the holdout evaluation and the prequential evaluation.
<End Transcript>

<-- 2.7 Article -->
Using the MOA interface
Download MOA and run it. It’s a Java program, like Weka. If you can run Weka, you can run MOA!
Here’s an example of how to use the MOA interface. You will need to go through these steps before attempting the Quiz that follows.
Click Configure to set up a task. Change the task type in the dropdown menu at the top to LearnModel. As you can see, the default learner is Naive Bayes. You could change it by clicking the Edit button and then selecting from the dropdown menu at the top – classifiers are organized the same way as they are in Weka. However, leave it as Naive Bayes for now.
The default datastream is a Random Tree Generator. Use the corresponding Edit button to change it to the Waveform Generator, which generates instances from a combination of waveforms. Change the number of instances to generate from 10,000,000 to 1,000,000.
Finally, specify a taskResultFile, say “modelNB.moa”, where MOA will output the model.
Now click OK, and then Run to launch this task. Textual output appears in the center panel; in this case every 10,000 steps. Various evaluation measures appear in the lower panel, and are continuously updated until the task completes. MOA can run several tasks concurrently, as you will see if you click Run twice in quick succession. Clicking on a job in the top panel displays its information in the lower two panels.
The task you have just run is
LearnModel -l bayes.NaiveBayes -s generators.WaveformGenerator 
    -m 1000000 -O modelNB.moa
– you can see this in the line beside the Configure button – and the Naive Bayes model has been stored in the file modelNB.moa. (Note that parameters that have their default value are not shown in the configuration text.)
Click Configure and change the learner to a Hoeffding Tree with output file modelHT.moa:
LearnModel -l trees.HoeffdingTree -s generators.WaveformGenerator 
    -m 1000000 -O modelHT.moa
and run it. Now we have two models stored on disk, modelNB.moa and modelHT.moa.
We will evaluate the Naive Bayes model using 1,000,000 new instances generated by the Waveform Generator, which is accomplished by the task
EvaluateModel -m file:modelNB.moa 
    -s (generators.WaveformGenerator -i 2) -i 1000000 
The “–i 2” sets a different random seed for the waveform generator. You can set up most of this in the Configure panel. At the top, set the task to EvaluateModel, and configure the stream (which has now changed to Random Tree Generator) to Waveform Generator with an instanceRandomSeed of 2. Frustratingly, though, you can’t specify that the model should be read from a file.
It’s useful to learn how to get around such problems. Click OK to return to the main MOA interface, select “Copy configuration to clipboard” from the right-click menu (on a Mac trackpad, do Alt/Shift/tap), then select “Enter configuration” and paste the clipboard into the new configuration, where you can edit it, and type –m file:modelNB.moa into the command line. This gives the EvaluateModel task the parameters needed to load the Naive Bayes model produced in the previous step, generate a new waveform stream with a random seed of 2, and test on 1,000,000 examples. (Recall that parameters that have their default value are not shown in the configuration text.)
Phew!

<-- 2.8 Quiz -->
Using MOA
Question 1
What is the percentage of correct classifications?
74.2%
77.7%
80.5%
84.5%
85.1%
85.8%
---
Correct answer(s):
80.5%
---
Feedback correct:
(More precisely, 80.465%)

<-- 2.8 Quiz -->
Using MOA
Question 2
Edit the command line to evaluate the Hoeffding Tree model instead of the Naive Bayes model.
What is the percentage of correct classifications?
74.2%
77.7%
80.5%
84.5%
85.1%
85.8%
---
Correct answer(s):
84.5%
---
Feedback correct:
(More precisely, 84.474%)

<-- 2.8 Quiz -->
Using MOA
Question 3
Which model performs best according to the Kappa statistic?
Hoeffding Tree
Naive Bayes
---
Correct answer(s):
Hoeffding Tree
---
Feedback correct:
The Kappa statistic is 76.7% for a Hoeffding tree and 70.7% for Naive Bayes.

<-- 2.8 Quiz -->
Using MOA
Question 4
In MOA, you can nest commands. For example, the LearnModel and EvaluateModel steps can be rolled into one, avoiding the need to create an external file. You can’t do this within the interactive Configure interface; instead you have to edit the Configure command text.
OzaBag is an incremental bagging technique. Evaluate it as follows:
EvaluateModel -m (LearnModel -l meta.OzaBag
-s generators.WaveformGenerator -m 1000000)
-s (generators.WaveformGenerator -i 2) -i 1000000
Do this by copying this command and pasting it as the Configure text using right-click “Enter configuration”.
What is OzaBag’s accuracy?
74.2%
77.7%
80.5%
84.5%
85.1%
85.8%
---
Correct answer(s):
85.8%
---
Feedback correct:
(More precisely, 85.821%)

<-- 2.8 Quiz -->
Using MOA
Question 5
The task EvaluatePeriodicHeldOutTest trains a model while taking performance snapshots at periodic intervals on a held-out test set.
The following command trains the HoeffdingTree classifier on a 100,000,000 samples from the WaveformGenerator data, holding out the first 100,000 samples as a test set. After every 1,000,000 examples it performs a test on this set:
EvaluatePeriodicHeldOutTest -l trees.HoeffdingTree 
-s generators.WaveformGenerator 
-n 100000 -i 10000000 -f 1000000
You can copy this configuration and paste it in (cheating!), or (work harder and learn more) set it up in the interactive Configure interface (test size 100,000, train size 10,000,000, sample frequency 1,000,000).
It outputs a CSV file with 10 rows in the center panel, and final statistics in the lower panel.
What is the final accuracy and Kappa statistic?
Select all the answers you think are correct.
Accuracy: 74.2%
Accuracy: 77.7%
Accuracy: 85.1%
Kappa statistic: 74.2%
Kappa statistic: 77.7%
Kappa statistic: 80.5%
---
Correct answer(s):
Accuracy: 85.1%
Kappa statistic: 77.7%

<-- 2.8 Quiz -->
Using MOA
Question 6
Another evaluation method is “prequential”, which interleaves testing and training.
Individual examples are used to test the model before they are used for training; the accuracy is incrementally updated. The order is important: the model is always tested on examples it has not yet seen. Full use is made of all available data because no holdout set is needed for testing.
Here is a EvaluatePrequential task that trains a HoeffdingTree classifier on 1,000,000 examples of the WaveformGenerator data, testing every 10,000 examples, to create a 100-line CSV file:
EvaluatePrequential -l trees.HoeffdingTree
-s generators.WaveformGenerator
-i 1000000 -f 10000
Set it up in the interactive Configure interface and run it. At the bottom, the GUI shows a graphical display of the results – a learning curve. You can compare the results of two different tasks: click around the tasks and you will find that the current one is displayed in red and the previously selected one in blue.
Compare the prequential evaluation of Naive Bayes with HoeffdingTree. Does the Hoeffding Tree always outperform Naive Bayes in the learning curve display?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
There’s a brief period at the beginning when Naive Bayes is sometimes better.

<-- 2.8 Quiz -->
Using MOA
Question 7
What are the final Kappa statistics?
Select all the answers you think are correct.
70.2% for Hoeffding Tree
70.2% for Naive Bayes
74.2% for Hoeffding Tree
74.2% for Naive Bayes
85.1% for Hoeffding Tree
85.1% for Naive Bayes
---
Correct answer(s):
70.2% for Naive Bayes
74.2% for Hoeffding Tree
---
Feedback correct:
70.25% for Naive Bayes has been rounded to 70.2%, following the convention of rounding to an even number (70.2%) rather than an odd one (70.3%) when both are equally close to the actual value (70.25%)
---
Feedback incorrect:
70.25% for Naive Bayes has been rounded to 70.2%, following the convention of rounding to an even number (70.2%) rather than an odd one (70.3%) when both are equally close to the actual value (70.25%)

<-- 2.8 Quiz -->
Using MOA
Question 8
By default, prequential evaluation displays performance computed over a window of 1000 instances, which creates a jumpy, jagged, learning curve.
Look at the evaluator in the Configuration panel. You can see that the WindowClassificationPerformanceEvaluator is used, with a window size of 1000. Instead, select the BasicClassificationPerformanceEvaluator, which computes evaluation measures from the beginning of the stream using every example:
EvaluatePrequential -l trees.HoeffdingTree 
    -s generators.WaveformGenerator
    -e BasicClassificationPerformanceEvaluator 
    -i 1000000 -f 10000
As you can see, this ensures a smooth plot over time, because each individual example becomes increasingly less significant to the overall average.
Compare again the prequential evaluations of Naive Bayes and Hoeffding Tree using the BasicClassificationPerformanceEvaluator. Does the Hoeffding Tree always outperform Naive Bayes in the learning curve display?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
Again, there’s a brief period at the beginning when Naive Bayes is better.

<-- 2.8 Quiz -->
Using MOA
Question 9
What are the final Kappa statistics?
Select all the answers you think are correct.
70.7% for Hoeffding Tree
70.7% for Naive Bayes
74.2% for Hoeffding Tree
74.2% for Naive Bayes
75.8% for Hoeffding Tree
75.8% for Naive Bayes
---
Correct answer(s):
70.7% for Naive Bayes
75.8% for Hoeffding Tree

<-- 2.8 Quiz -->
Using MOA
Question 9
What are the final Kappa statistics?
Select all the answers you think are correct.
70.7% for Hoeffding Tree
70.7% for Naive Bayes
74.2% for Hoeffding Tree
74.2% for Naive Bayes
75.8% for Hoeffding Tree
75.8% for Naive Bayes
---
Correct answer(s):

<-- 2.9 Video -->
MOA classifiers and streams 
Change is everywhere! – and is a distinguishing feature of data stream mining. Bernhard Pfahringer explains that one way of dealing with change is to use an adaptive windowing method called ADWIN that grows a sliding window on the data stream in times of stability and shrinks it in times of change. The Hoeffding Adaptive Tree grows alternative branches and monitors their performance using ADWIN. Bagging, which involves bootstrap sampling with replacement, can be turned into an online algorithm using a weighting technique, and coupled with ADWIN for explicit change detection. MOA’s data stream generators can simulate change over time, allowing these mining algorithms to be tested and compared.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello! My name is Bernhard Pfahringer. I’m with the Computer Science Department here at the University of Waikato, the home of Weka and MOA. Today, I’m going to tell you a little bit more about MOA. Specifically, we’re going to talk about change. Change is everywhere in data streams. It’s the distinguishing feature of data stream mining. To think about it like you want to predict electricity consumption, there is a big peak in the morning and there’s another peak in the late evening. There’s some level of consumption during the day, and there might be a much lower level of consumption during nighttime. There’s different levels in summer to winter and so on and so forth.
If we look at Tweet streams, for instance, right now the Zika virus is trending. A couple of months ago, at least in New Zealand, it was Rugby World Cup. Again, we see change happening all the time. New topics are coming in, becoming more interested, peak, and then they slowly die away, making place for new, other, topics. How do we deal with change? Some algorithms can just basically, because of the way they work, implicitly adapt to change, but a better way is to do it explicitly. We need a change detector. There are a couple of algorithms that have been described in the literature for that purpose, one of them is ADWIN, which is short for Adaptive Windowing.
It’s used in a number of algorithms inside MOA. It keeps an adaptive sliding window, which tries to estimate the mean of a numeric variable that you’re monitoring. Whenever things are stable, the window grows, but once there seems to be some change, we cut the window into two parts, the old part and the new part. The old part is being discarded. The new part, hopefully, gives us the new, correct mean. Now, this is not just a heuristic algorithm. It comes with guarantees, theoretical guarantees, on the rates of false positives and the rates of false negatives and on the size related to the rate of change.
I don’t have to worry about the size and it being too big because the way it’s stored internally it uses an exponential compression scheme, so that’s fine, as well. Now, ADWIN is being used in a number of algorithms, as I said. One of them is the Hoeffding Adaptive Tree, which, as the name implies, is a variant of the Hoeffding Tree that Albert has explained to you previously. Now, in the Hoeffding Adaptive Tree, you do not just grow the tree on the most promising tests, you also look at the second best ones. So you grow alternative branches, as well, and you monitor the performance of those alternatives over time using ADWIN.
Whenever ADWIN detects that these alternative sections now have become better or are out-performing the official main structure, you replace the main with the alternative. Of course, at the same time, you start growing new alternatives, as well, to be prepared for further change in the future. OK, so the Hoeffding Adaptive Tree was a single classifier. Of course, we all know that, when it comes to utmost predictive accuracy, you want to have ensembles. A very simple ensemble method is bagging. Now, bagging is easily made into an online algorithm, because, just think about it. If you bag, say, an incremental algorithm like the Hoeffding Tree, you have an incremental ensemble immediately. No problem there.
Of course, there’s a little issue, and that is the way bagging distributes examples across the different classifiers. In the batch-learning version, you do bootstrap sampling with replacement. Classifier 1 in our simple example here – which has a dataset which comprises only four examples, A, B, C, and D – might see a sample that is B, A, C, B. Classifier 2 might get D, B, A, D. Classifier 4, a very extreme one, might actually get three copies of B and maybe only one C and never see A or D. How do we solve this issue with getting a procedure that’s basically batch oriented, like bootstrap sampling, and turn it into something incremental? Well, there’re two simple steps to look at here.
First of all, we can actually look at our sub-samples by sorting them. Thus, we restore the original order of the data. A, B, C, D was the original order. Classifier 1 would see A, B, B, C. Classifier 2 would see A, B, D, D. The extreme one would see B, B, B, and C. The next step is to actually look at that as every classifier sees all the examples in order – A, B, C, and D – but with a weight attached. For the first classifier, the weight would be 1 for A, 2 for B, 1 for C, and 0 for D. The extreme classifier would have 0 for A, 3 for B, 1 for C, 0 for D.
Now, that looks promising. In 2001, there was a very interesting paper called “Online Bagging and Boosting” authored by Oza and Russell, where they had this following genius insight. Their insight was that we can actually approximate the binomial distribution with a Poisson distribution where the mean is set to 1. That gives you roughly the same behavior that the binomial distribution in the batch-based bagging has. Roughly about 37% of the time you get a 0 weight. 37% of the time you get a weight of 1. 18% of the time you get a weight of 2, and higher weights are more and more unlikely, but still possible.
Now, using that in online bagging makes it very easy to, whenever an example comes in, you just draw the weight from the Poisson distribution for the first classifier, another weight for the second classifier, and so for all the classifiers. Now, we have a fully incremental setup. In MOA, we have an algorithm called ADWIN Bagging that couples that with ADWIN, as well, for explicit change detection again. We use Poisson with a mean of 1 to weight every example differently for every classifier, but we also detect when things deteriorate. We do that by monitoring the overall performance of the ensemble and whenever things fall below a threshold, we identify – the worst classifier is being removed and replaced by a new one.
Now, in a number of experiments, what we also found is that we can improve on that by playing around with the parameters. For the Poisson distribution, if we use a mean that is larger than 1, like 2 or maybe even up to 6, what we find is that across a large range of benchmark datasets in stream mining, we get better results. We’ve called it leveraging bagging. Again, it’s coupled with ADWIN for explicit change detection. When things get bad, you remove the worst classifier and replace it by a new one. In summary, when looking at change in classifiers and explicit change detection.
Hoeffding Adaptive Tree is a single classifier, and two variants of bagging, ADWIN bagging and leveraging bagging, as an example of ensembles that deal explicitly with change. I invite you to do your own experiments with MOA classifiers and change detection. For that purpose in MOA, you will find that data stream generators also come with a component that also simulates change over time. For instance, in the RandomRBF generator, you get drift by basically having the centers of the kernels move around in space.
<End Transcript>

<-- 2.10 Quiz -->
Streaming classifiers and evolving streams
Question 1
We begin with a non-evolving scenario.
The RandomRBF data generator first creates a random set of centers for each class, each comprising a weight, a central point per attribute, and a standard deviation. It then generates instances by choosing a center at random (taking the weights into consideration), which determines the class, and randomly choosing attribute values and an offset from the center. Finally the overall vector is scaled to a length that is randomly sampled from a Gaussian distribution around the center.
What are the accuracies of Naive Bayes and Hoeffding Tree on a RandomRBFGenerator stream of 1,000,000 instances with default values, using Prequential evaluation and the BasicClassificationPerformanceEvaluator?
Select all the answers you think are correct.
Naive Bayes: 57.6%
Naive Bayes: 72.0%
Naive Bayes: 91.0%
Hoeffding Tree: 57.6%
Hoeffding Tree: 72.0%
Hoeffding Tree: 91.0%
---
Correct answer(s):
Naive Bayes: 72.0%
Hoeffding Tree: 91.0%

<-- 2.10 Quiz -->
Streaming classifiers and evolving streams
Question 2
MOA’s Hoeffding Tree algorithm can use different prediction methods at the leaves.
The default is an adaptive Naive Bayes method, but a majority class classifier can be used instead by specifying
–l (trees.HoeffdingTree –l MC)
You can set this up in MOA’s Configure interface (you need to scroll down).
What is the accuracy of the Hoeffding Tree when a majority class classifier is used at the leaves?
53.1%
57.6%
72.0%
87.8%
91.0%
93.8%
---
Correct answer(s):
87.8%
---
Feedback correct:
(More precisely, 87.84%)

<-- 2.10 Quiz -->
Streaming classifiers and evolving streams
Question 3
What is the accuracy of the OzaBag bagging classifier?
53.1%
57.6%
72.0%
87.8%
91.0%
93.8%
---
Correct answer(s):
93.8%
---
Feedback correct:
(More precisely, 93.83%)

<-- 2.10 Quiz -->
Streaming classifiers and evolving streams
Question 4
Now let’s use an evolving data stream.
The RandomRBFGeneratorDrift generator has a speedChange parameter that controls the rate of movement of the centroids.
What are the accuracies of Naive Bayes, Hoeffding Tree, and OzaBag on a RandomRBFGeneratorDrift stream of 1,000,000 instances with speed change of 0.001, again using Prequential evaluation with the BasicClassificationPerformanceEvaluator?
Select all the answers you think are correct.
Naive Bayes: 53.1%
Naive Bayes: 57.6%
Naive Bayes: 63.5%
Hoeffding Tree: 53.1%
Hoeffding Tree: 57.6%
Hoeffding Tree: 63.5%
OzaBag: 53.1%
OzaBag: 57.6%
OzaBag: 63.5%
---
Correct answer(s):
Naive Bayes: 53.1%
Hoeffding Tree: 57.6%
OzaBag: 63.5%

<-- 2.10 Quiz -->
Streaming classifiers and evolving streams
Question 5
The Hoeffding Adaptive Tree adapts to changes in the data stream by constructing tentative “alternative branches” as preparation for changes, and switching to them if they become more accurate. A change detector with theoretical guarantees (ADWIN) is used to check whether to substitute alternative subtrees.
Two alternative methods for evolving data streams, both of which use ADWIN, are OzaBagAdwin and LeveragingBag.
What are the accuracies of these three techniques in the evolving data stream situation used in the last three questions?
Select all the answers you think are correct.
HoeffdingAdaptiveTree: 66.3%
HoeffdingAdaptiveTree: 68.1%
HoeffdingAdaptiveTree: 81.4%
OzaBagAdwin: 66.3%
OzaBagAdwin: 68.1%
OzaBagAdwin: 81.4%
LeveragingBag: 66.3%
LeveragingBag: 68.1%
LeveragingBag: 81.4%
---
Correct answer(s):
HoeffdingAdaptiveTree: 66.3%
OzaBagAdwin: 68.1%
LeveragingBag: 81.4%
---
Feedback incorrect:
(More precisely, 66.26%)

<-- 2.10 Quiz -->
Streaming classifiers and evolving streams
Question 6
As well as data streams, MOA can also process ARFF files. Select ArffFileStream as the data stream and specify the filename in the arffFile field.
Use the covtypeNorm.arff dataset that we met earlier this week. The default location for files is the top level of the MOA installation folder – you will probably find the model files modelNB.moa and modelHT.moa that you made in the last quiz – so you should either copy the data file there or specify its full pathname.
Determine the accuracy of Naive Bayes, Hoeffding Tree, and LeveragingBag for this dataset, using Prequential evaluation with the BasicClassificationPerformanceEvaluator and a sampleFrequency of 10,000.
Select all the answers you think are correct.
Naive Bayes: 60.5%
Naive Bayes: 80.3%
Naive Bayes: 91.7%
Hoeffding Tree: 60.5%
Hoeffding Tree: 80.3%
Hoeffding Tree: 91.7%
LeveragingBag: 60.5%
LeveragingBag: 80.3%
LeveragingBag: 91.7%
---
Correct answer(s):
Naive Bayes: 60.5%
Hoeffding Tree: 80.3%
LeveragingBag: 91.7%

<-- 2.10 Quiz -->
Streaming classifiers and evolving streams
Question 7
Which method is the fastest, and which is the most accurate?
Select all the answers you think are correct.
Fastest: Naive Bayes
Fastest: Hoeffding Tree
Fastest: Leveraging Bagging
Most accurate: Naive Bayes
Most accurate: Hoeffding Tree
Most accurate: LeveragingBag
---
Correct answer(s):
Fastest: Naive Bayes
Most accurate: LeveragingBag

<-- 2.10 Quiz -->
Streaming classifiers and evolving streams
Question 7
Which method is the fastest, and which is the most accurate?
Select all the answers you think are correct.
Fastest: Naive Bayes
Fastest: Hoeffding Tree
Fastest: Leveraging Bagging
Most accurate: Naive Bayes
Most accurate: Hoeffding Tree
Most accurate: LeveragingBag
---
Correct answer(s):

<-- 2.11 Video -->
Classifying tweets 
Twitter is a vast, continuous, prolific, real time data stream. Sentiment analysis is the task of classifying tweets as positive or negative according to the feelings they express. Emoticons constitute “ground truth” that can serve as training data. Data sets are unbalanced, with far more positives than negatives (which, when you think about it, is a nice comment about the world in general). This presents an evaluation problem that can be addressed using the “Kappa” statistic, which measures the difference between a particular classifier and a random one that uses only the class distribution statistic.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Twitter is a very nice example of a data stream, because it is data that is produced in real time. Twitter is a micro-blogging service that was built to discover what is happening at any moment in time. There are more than 300 million users, more that 2100 million search queries every day, and a very nice thing for us is that the data is public and it can be accessed through a streaming API. In this lesson, we’re going to look at an application of sentiment analysis. Sentiment analysis is the task of classifying messages or tweets into two categories, positive or negative, depending on the feelings that we can see inside the messages. Many times it’s very difficult to get the label data.
In sentiment analysis with Twitter, there is very basic approach, but it works very well. We can get label data using the tweets that have emoticons inside. Many tweets have positive or negative emoticons, and then we can use this information to classify them as positive or negative. We can use all of these tweets to train our
model, and then we can predict using the tweets that don’t have emoticons: we can predict what is the current polarity, what is the current sentiment around any specific product or company or topic. An important thing that we need to look at when we are classifying tweets is that if data is balanced or not. Let’s look at an example. In this simple confusion matrix, what we see is that we are predicting 82% as positive and 18% of the instances as negative. What we see is that we are classifying correctly the positive class 75% of the instances and we are correct on the negative class for 10% of the instances. Our accuracy in this case is 85%.Is this good performance?
To answer this, one way is that we can look at a random classifier. Imagine a random classifier that is predicting randomly but is following the same distribution between the positive class and negative class. This is the confusion matrix in the bottom. There we can see that this classifier is getting also 82% of the instances positives and is predicting as negative 18%. The interesting thing is that it is predicting the positive class correctly 68% of the time and the negative is predicted correctly in 3% of the instances.
That means that the accuracy here is 71%.That means that, if our classifier is predicting with an accuracy higher than this, then we can say that is a good classifier, but if it’s predicting less than this 71%, then our classifier is not doing quite well. To see this, this is, as you may know, there is this kappa statistic measure that is measuring this difference, the difference between the accuracy of our classifier with the accuracy of a random classifier that is predicting using the same distribution of classes. Basically, the kappa statistic computes this difference, then it adds a normalizing factor so we get a value of kappa between 0 and 1.Now let’s look at an application.
There is this Twitter sentiment corpus that was made by students at Stanford that contains tweets that were collected between April 2009 and June 2009. There are 800,000 tweets with positive emoticons and 800,000 tweets with negative emoticons. If we do a prequential evaluation using these tweets and we use a Naive Bayes multinomial classifier, Stochastic gradient descent classifier and a Hoeffding Tree, what we see is that at the end of the stream, the Stochastic gradient descent classifier gets an accuracy of 100%. This is something that is not normal, and then it’s nice to see why it’s happening.
If you look at the kappa statistic, what we see is that at the moment that the accuracy goes up to 100%, the kappa statistic goes down. That means that, in that case, the data at that point starts to be completely unbalanced and only belonging to one class. In this data stream, if we compare accuracy and kappa of the multinomial Naive Bayes, Stochastic gradient descent, and Hoeffding Tree classifier, what we can see is that Stochastic gradient descent is better, but this is something that may not apply to other data streams. What is very interesting is that in data stream mining, we should also not only look at the accuracy, but also look at the resources, at time and memory.
In this lesson, we have seen an application of Twitter classification. Twitter is a micro-blogging streaming service that is built to discover what is happening at any moment in time and, more specifically, what is happening now. Data may be unbalanced in many data streams, so it’s always important to not only look at the accuracy, but also look at other measures such as kappa statistics.
<End Transcript>

<-- 2.12 Discussion -->
Sentiment analysis
In the video, Albert defines sentiment analysis as the task of classifying messages or tweets into two categories, positive or negative, depending on the feelings that we can see inside the messages.
But life is more complex than this! There are many other sentiments, apart from this bipolar positive/negative distinction.
What other sentiments might one want to mine from tweets? In what ways might the results be useful? And – the 64,000 dollar question! – what are possible ways of obtaining training and evaluation data?

<-- 2.13 Quiz -->
Classifying tweets
Question 1
How many instances are there, and how many have the minority class?
Select all the answers you think are correct.
8552 instances
10426 instances
49955 instances
8552 minority class instances
10426 minority class instances
49955 minority class instances
---
Correct answer(s):
49955 instances
8552 minority class instances

<-- 2.13 Quiz -->
Classifying tweets
Question 2
Is the dataset unbalanced?
Yes
No
---
Correct answer(s):
Yes
---
Feedback correct:
One class has 83% of the instances; the other has 17%.

<-- 2.13 Quiz -->
Classifying tweets
Question 3
Run a 10-fold cross-validation with Weka’s NaiveBayesMultinomialText classifier.
In this dataset, the second attribute is a string – the text of the tweet – but this classifier deals with string attributes automatically: it incorporates the StringToWordVector filter that we used in the earlier Weka courses.
What’s the accuracy of NaiveBayesMultinomialText, and what’s its Kappa statistic?
Select all the answers you think are correct.
Accuracy: 80.9%
Accuracy: 82.9%
Accuracy: 83.1%
Kappa statistic: 0%
Kappa statistic: 4.9%
Kappa statistic: 39.2%
---
Correct answer(s):
Accuracy: 80.9%
Kappa statistic: 39.2%

<-- 2.13 Quiz -->
Classifying tweets
Question 4
SGDText is an iterative method that uses stochastic gradient descent to learn either an SVM (default configuration) or a logistic regression model.
Like NaiveBayesMultinomialText, it deals with string attributes automatically.
Configure it for 1 epoch (the default is 500) so that it operates incrementally – it sees instances just once. It’s rather slow, even with just 1 epoch, so use percentage split evaluation rather than cross-validation. What’s its accuracy on the Twitter dataset, and what’s its Kappa statistic?
Select all the answers you think are correct.
Accuracy: 80.9%
Accuracy: 82.9%
Accuracy: 83.1%
Kappa statistic: 0%
Kappa statistic: 4.9%
Kappa statistic: 39.2%
---
Correct answer(s):
Accuracy: 83.1%
Kappa statistic: 4.9%

<-- 2.13 Quiz -->
Classifying tweets
Question 5
Of these two classifiers, NaiveBayesMultinomialText and SGDText, is the one with the larger Kappa statistic also the one with greater accuracy?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
Any Kappa value greater than zero indicates that some learning has taken place, but the value of 5% achieved here is very much worse than the 40% achieved by Naive Bayes – despite the slightly greater accuracy (83% vs 81%).

<-- 2.13 Quiz -->
Classifying tweets
Question 6
Now configure SGDText to use logistic regression by changing the loss function to Log loss.
Does the accuracy increase?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
It decreases slightly, from 83.1% to 82.9%.

<-- 2.13 Quiz -->
Classifying tweets
Question 7
Does the Kappa statistic increase?
Yes
No
---
Correct answer(s):
Yes
---
Feedback correct:
It increases from 4.9% to 5.9%. The Log loss setting gives a slightly lower accuracy, but a slightly higher Kappa.

<-- 2.13 Quiz -->
Classifying tweets
Question 8
Now create an ARFF dataset that you can use from MOA.
In Weka, filter the Twitter dataset with StringToWordVector, setting both IDFTransform and TFTransform to True. Try it.
Oops! – there’s a problem. Check it out. You can solve it by setting StringToWordVector’s attributeNamePrefix to – anything, really.
In MOA, run a prequential evaluation on this dataset using the majority class classifier (functions.MajorityClass) and the BasicClassificationPerformanceEvaluator.
Remember from the last Quiz how to use ARFF files in MOA? – set everything else up in the Configure interface and then insert
-s (ArffFileStream -f filename.arff -c 2)
into the command string. The –c 2 is necessary to signify that the class is the second attribute.
Or you could just copy the following line into MOA’s Command text box, changing the filename appropriately:
EvaluatePrequential -l functions.MajorityClass 
    -s (ArffFileStream -f tweets.arff -c 2) 
    -e BasicClassificationPerformanceEvaluator -i 1000000 -f 1000
What’s the final accuracy and Kappa statistic?
Select all the answers you think are correct.
Accuracy: 19.1%
Accuracy: 80.9%
Accuracy: 82.9%
Kappa statistic: 0%
Kappa statistic: 4.9%
Kappa statistic: 19.1%
---
Correct answer(s):
Accuracy: 82.9%
Kappa statistic: 0%

<-- 2.13 Quiz -->
Classifying tweets
Question 9
The majority class classifier classifies everything as pos, yielding a superficially impressive accuracy of 82.9% …
… which is simply the proportion of pos instances in the dataset, 41403/49955. Not really very good! And this is reflected in a Kappa value of 0, which means that the classifier has learned nothing interesting.
Repeat with the NaiveBayesMultinomial classifier.
What’s the final Kappa statistic?
0%
4.9%
19.1%
28.4%
35.6%
39.2%
---
Correct answer(s):
35.6%
---
Feedback correct:
(More precisely, 35.63%)

<-- 2.13 Quiz -->
Classifying tweets
Question 10
This Kappa value indicates that Bayes does learn something non-trivial.
The Kappa-temporal statistic is similar to Kappa, but instead of comparing with a random classifier it compares with the “no-change” classifier, which simply predicts the class label of the last training instance seen.
What’s the value of the Kappa-temporal statistic in the last experiment?
0%
4.9%
19.1%
28.4%
35.6%
39.2%
---
Correct answer(s):
28.4%

<-- 2.13 Quiz -->
Classifying tweets
Question 11
When the no-change classifier performs well, we say that a data stream has a strong temporal dependence (in other words, significant autocorrelation).
Use MOA to run the no-change classifier (moa.classifiers.functions/NoChange) on the elecNormNew.arff dataset and check the accuracy achieved. Does this dataset exhibit strong temporal dependence?
Yes
Hard to say
No
---
Correct answer(s):
Yes
---
Feedback correct:
The accuracy is 85.8
---
Feedback incorrect:
Well, I kind of agree. The accuracy is pretty good, but is it good enough?  Anyway, go on – take a risk!

<-- 2.13 Quiz -->
Classifying tweets
Question 12
In fact, if you randomize the instances in this dataset (which you can easily do with a Weka filter), the classifier’s accuracy drops from 86% to 51%.
[Would you like a little mathematical challenge? You can show theoretically that for a 2-class dataset with class probabilities p and 1–p, the no-change classifier has a success rate of 1–2p (1–p) if the instances are in random order. For this dataset p = 0.425, and the theoretical success rate is 0.511, or 51%.]
Does the covtypeNorm.arff dataset exhibit strong temporal dependence, according to the same criterion?
Yes
No
---
Correct answer(s):
Yes
---
Feedback correct:
The accuracy is 95.1%.
In fact, if you were to randomize the instances the accuracy would drop from 95% to 38%.
In general, for any dataset, the success rate of the no-change classifier is the sum of the squares of the class probabilities, if the instances are presented in random order. For covtypeNorm.arff, a 7-class dataset, p1^2 + p2^2 + p3^2 + p4^2 + p5^2 + p6^2 + p7^2 works out to 0.377.

<-- 2.13 Quiz -->
Classifying tweets
Question 12
In fact, if you randomize the instances in this dataset (which you can easily do with a Weka filter), the classifier’s accuracy drops from 86% to 51%.
[Would you like a little mathematical challenge? You can show theoretically that for a 2-class dataset with class probabilities p and 1–p, the no-change classifier has a success rate of 1–2p (1–p) if the instances are in random order. For this dataset p = 0.425, and the theoretical success rate is 0.511, or 51%.]
Does the covtypeNorm.arff dataset exhibit strong temporal dependence, according to the same criterion?
Yes
No
---
Correct answer(s):

<-- 2.14 Video -->
Signal peptide prediction
Tony Smith introduces signal peptide prediction, an application of data mining to a problem in bioinformatics. A sequence of amino acids that makes up a protein begins with an initial portion of 20 or 30 amino acids called the “signal peptide” that unlocks a membrane for the protein to pass through. The problem is to determine the “cleavage point” where the signal peptide ends. An important question is whether we seek an accurate prediction or an explanatory model. One potentially useful feature is the length of the signal peptide; another is the amino acids immediately upstream and immediately downstream of the cleavage point. Overfitting is a problem, and domain knowledge from experts is an important ingredient for success – data mining is a collaborative process.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! My name is Tony Smith. In this lesson, we’re going to look at a practical application of data mining in the world of biology. Knowledge discovery with biological data, or so-called bioinformatics. Now, there are many different types of biological problems that we might want to study, many different data types. I’m going to look at a subset that’s quite common, called “sequence analysis”. Sequence of nucleotides that make up genes or sequences of amino acids that make up proteins – in fact, the latter. We're going to look at a very easily stated sequence problem for proteins. It goes like
this: given a freshly produced protein, which portion of it is the signal peptide? Now, what does this mean? Well, you might remember from high school biology that along your DNA there are nucleotide sequences called genes. Genes get copied with messenger RNA to produce a transcript, and the transcript is used to string together amino acids into a polypeptide chain, which is a protein. Proteins perform some function in a cell, and, in order to do that, they have to be transported to where they’re going to perform that function, and, through that transport, they have to pass through a membrane.
In so doing, what happens is the 20 or 30 or so amino acids at the beginning of the protein – called the signal peptide – they open up a translocation channel that allows the protein to pass through the membrane. In so doing, the signal peptide portion gets cleaved off. The signal peptide is kind of like a key that opens a door for a protein, and, if we know what the key is, it give us an idea as to what the function of the protein might be. We want to predict where the signal peptide ends. Where is the cleavage point? We first ask ourselves what’s our general goal? Do we want an accurate prediction or do we want an explanatory model?
Something that gives us some knowledge. We’ll have to ask what features might be relevant in predicting the cleavage site. So what features do we need to generate from the data we’re given? What approach are we going to take? What learning algorithms in Weka we might use, and how are we going to know if the model produced by Weka is any good? How do we know if we’re successful? Here’s some 10 instances or so of new proteins. As you can see, they’re sequences of letters where each letter corresponds to a different type of amino acid. M is Methionine, A is Alanine, S is Serine, and so on.
About 25 or 30 residues along for the beginning of the protein, marked in red here, is the cleavage site. That’s the beginning of the mature protein, the part that survives after cleavage. That’s what we’re trying to predict. Which of those residues is the cleavage site. What properties do we think are relevant? Do we want properties of the entire signal peptide or just properties around the cleavage site? We might get some domain knowledge from a biologist to help us out, or we might do some ad hoc statistical analysis to look for thing that might correlate with the cleavage site.
For example, given the 1400 examples in our dataset, we might find that there’s a very tightly clustered length, with the mean length of 24. Knowing the position of a residue might be useful in predicting whether or not it’s the cleavage site. If we look at the residue at the start of the protein and, perhaps, the three residues immediately upstream of the cleavage site and the three residues downstream from it, there might be some useful information there, some context.
In fact, if we do a histogram of the upstream region of the data we’ve got, we’ll see that is looks like the letter A, Alanine, and perhaps the letter L and maybe S, as well, seem to be quite frequent around the cleavage site. So that could be useful.
When we don’t have much domain knowledge, we might come up with a set of features that include the position of the residue being considered; the residues at each position, three either side of the cleavage point; and then for each residue that we know is the cleavage site, we’ll put that in the class of yes this is the cleavage point; and we’ll just get some negative instances by randomly choosing some other residues and producing the same information. We might do this inside a spreadsheet. Here’s an example. Each column is an attribute and each row is one instance of a residue. We record all this information. This can be saved in a comma-separated version in most spreadsheet packages.
Weka, of course, can load a CSV package. We’re going to go ahead and load in this data into Weka and have a go seeing if we can predict the cleavage site from it. I’ve loaded up the dataset that I just showed you into Weka. We see here we’ve got the features, the length, or the position of the acid in question. Which residue is at the –3 position, –2, –1. The residue at the cleavage site and 1, 2, and 3 upstream. And I’ve recorded whether this is an example of the cleavage site or a randomly chosen other residue that’s not.
Now, if I go straight to classify, I want an explanatory model, so I’m going to go for a C4.5 decision tree. I’ll go down to trees, load up J48, which is C4.5, and, under the default settings of 10-fold cross-validation, I’m just going to go ahead and start up Weka. It comes back pretty quickly. If we look at the accuracy, we’ll see we’ve got 78-79% accuracy. That’s pretty good considering other state-of-the-art software for predicting the signal peptide cleavage point performs at about 80-85% accuracy. So we’ve already done really well, but is this model any good? Now, if we look at the true positive rates for the two classes.
Here we’ve got the Yes and No class, and if we look at the true positive rates, they’re around 80%, so that pretty good. Let’s take a look at the decision tree produced. I’ll just pop up the visualization of it. Enlarge that a little bit. Fit to the Screen. Now, there’s a couple of reasons why this decision tree suggests we haven’t come up with a very good model. One is it’s very wide and very shallow, and it’s highly branching. Each of these tests seems to produce a lot of very small subsets. This suggests that what we’ve done is that we’ve actual found a model that overfits the data. Now, what does that mean? Well, let me give you an example.
Machine learning algorithms are trying their best to get predictive accuracy, and it’s often very easy for learning algorithms to find some model that will work. There are two reasons why we might get good performance for the wrong reasons. One is sparseness of d ata, and another is overfitting the data. Let’s look at each of these problems and see if we can figure out what’s going on with our example here. Data sparseness is another form of overfitting, but it’s specifically because we don’t have enough instances to figure out the true underlying relationship. Consider this very small dataset here. What I’ve done is that I’ve rolled two dice – six-sided game dice – and I’ve tossed a coin. Two dice, one coin.
I’ve recorded the outcomes. I rolled a 3 with one dice, a 5 with another, and a heads with the coin. I did that four times and recorded the four instances here. Now, we know that there are six possible outcomes for rolling a dice. I’ve got two dice. Two outcomes for a coin toss. That’s 6 x 6 x 2. That’s 72 possible instances we could’ve had, but we only have 4. I give these four instances to Weka. I say come up with a rule that allows me to predict the coin toss from the roll of the dice.
It comes up with a model: if Die1 > 2 then the outcome of the coin toss is heads, otherwise it’s tails. That fits the data we’ve got here. 100% correct, but, of course, if we had additional instances, then hopefully Weka would see that there’s no correlation, these are random outcomes. This is the problem of overfitting due to data sparseness. This is a real problem with our signal peptide, because we’ve recorded 7 different residues around the cleavage site, so each of them can be 1 of 20 residues. That’s 20^7 possible patterns. We’ve got the position, there’s about 60 different integers there. The two class values.
That’s 153 billion possible instances of which we have 1400 positive ones and an equal number of negative ones. A tiny fraction. That’s data sparseness. Overfitting, in general, can be indicated when the model is overly complex, such that the tests practically uniquely identify instances. The model splits instances into lots of very small subsets, and a telltale sign of this is the model is complex, highly branching. That’s what we see from our example here. We can usually tell if we’ve been overfitting. If we just get some more data, if we tried to predict it based on the tree we learned, we’d get poor performance. Of course, we don’t often have extra data.
Given these characteristics of an overfitting model, I would look at the decision tree we’ve got here and suggest that it is overfitting. One way to test that is I’ve actually prepared a dataset with three times as many negative instances. I’ll just go back and load up file two here, sigdata2. That’s the same as data1, only with three times as many negative instances. We’ll just go back to Classify under the same default settings. We’ll go ahead and start it up. Now, if we look at the accuracy, we’ll see it’s even gone up, 82.5%.But, if we look at the true positive rate of the cleavage class, it’s actually down to almost 50%.
That is practically a coin toss in its accuracy in predicting the
very thing we’re interested in: is this the cleavage site? This doesn’t look like a very fruitful way of going about trying to predict the cleavage site. Our amino acid context approach appears to be overfitting the data. What else could we try? Well, we might look for a different set of features that capture the more general properties of signal peptides. A more informed approach, which we might learn about by consulting an expert, a biologist, is we assume that the cleavage occurs because of physical forces at the molecular level. That is, amino acids have electro-chemical properties. We might create features that capture those physicochemical properties of amino acids around the cleavage site or of the signal peptide as a whole.
We can get some domain knowledge from the experts. What kind of knowledge would we get? Well, this diagram here shows a distribution of the amino acids at positions relative to the cleavage site. If we look at the –1 position, that’s the amino acids immediately upstream of the cleavage site. Here the size of the letters is proportional to the frequency of the amino acid type at that position. we’ll see at the –1 position, there’s a lot of A’s, quite a few G’s, S’s, some C’s and T’s. At the –3 position, we see A’s, V’s, S’s, and T’s. Also, sort of the region 5 to 15 upstream, we see there’s a lot of L’s, V’s, and A’s. What’s going on here?
What are the electro-chemical properties of A’s and L’s and V’s that we might exploit to capture this non-uniform distribution in these relative positions? It turns out that amino acids have well-known types. They can be molecules that tend to not like being near water. They’re called hydrophobic. You see on the right side of this Venn diagram, we’ve got A, V, P, M, L, F. These are all hydrophobic amino acids. On the other side, we’ve got the hydrophilic ones, the ones that like to be near water. We also have some amino acids that are positively charged and some are negatively charged. This affects whether or not they stick together, of course. And then the rest are not really very charged.
There are residues with small side chains, the bit of the molecule that distinguishes one residue from another. We’ve got A, V, P, G, C, N, S there all have small side chains, and the other ones are somewhat larger. These are the kinds of properties we could record about the molecule around the cleavage site. In fact, biologists know of the physicochemical properties around signal peptides, and they talk about this thing called the C-region, H-region, and the N-region. Now, the C-region is just those 3, 4, 5, 6 residues immediately upstream of the cleavage site. They’re usually uncharged at position –3 and the –1 position are small, have a small side chain. Adjacent to that upstream is the H-region, about 8 residues long.
That was all the L’s and V’s we saw. It tends to be a hydrophobic region. Then, above that, to the beginning of the protein is the N-region, which tends to be positively charged. This is information we can use to construct more informed features. The possible features we might include are the size, the charge, the polarity, and the general hydrophobicity of regions of the signal peptide, especially at position –1 and –3, because they seem to be quite distinct. We might compute the total hydrophobicity in an approximate H-region, about 5 to 15 upstream of the cleavage site. We might look at the total charge, polarity, and hydrophobicity in the C-region and so on. Then record whether or not that’s the cleavage site.
So for a couple of randomly chosen residues which are not the cleavage site, we’ll compute these same features. In fact, I’ve created
a dataset which just includes the following four features: the position, as we had before – the same as the length we had in the previous dataset – the overall hydropathy of the approximate H-region, the side-chain size for the –1 residue, and the charge of the –3 residue. If we go back to Weka here, we’ll just load in file 3, the one I prepared here. I’ll just load it in. Here we can see the position, the charge at the –3 position, whether or not it’s small in the –1 position, and the overall hydophobicity here of the H-region, which you’ll see is a numeric value.
There are charts of general hydrophobicity for amino acids, and I’ve just summed them up for a region upstream of the cleavage site. Let’s go back to J48. It’s still all set up here for 10-fold cross-validation. We’ll start her off under the default settings. If we look at our accuracy here, we’ve got – holy smokes – 91.5% accuracy. That’s great! Now, is this all just because we’re predicting one class? We look at the true positive rate, and we’ll see we’ve got an average true positive rate of almost 92%. That’s quite good. But, we might ask ourselves, are we overfitting the data? Now, if we look at the model, it’s going to be quite small, because we don’t have very many features.
Maybe this is a little on the big side. (Fit to screen here.)We might wonder, are we overfitting the data? Have we got a problem of data sparseness? Well, once again, I can generate three times as many negative instances to see if we’re just getting a sort of random outcome. We’ll go back to Preprocess here, open the file sigdata4. It’s the same as sigdata3, but with three times as many negative instances. I’ll load them all in. We’ve got 5,620 instances. I’ll go back to Classify. Same default settings. Go ahead and start it up, and let’s look at the accuracy first of all. Accuracy has gone up to almost 94%, but let’s look at those true positive rates.
Here, we see that our average true positive rate for our two classes still remains high, 94%. This indicates, in fact, that the model has been relatively good at discriminating between cleavage sites and non-cleavage sites. In fact, if we look at the model, if we visualize the tree, we can see a number of features here. At the top of the tree, it’s looked at the H-region, which we knew was useful in predicting the cleavage site, and then it’s looked at the smallness of the –1 position and so on. Overall, this looks like it might possibly be capturing, in a formal model, the general principles biologists told us all about.
When we’re doing bioinformatics, the considerations we have for doing data mining is we have to ask ourselves what’s our overall goal? Do we want predictive accuracy or explanatory power? How do we prepare the data to generate features which are actually going to be useful for solving our problem? How can we evaluate how good the model is that we get, knowing that Weka’s going to do its best to come up with a highly accurate model, and it may do so under spurious circumstances. Most importantly, bioinformatics is an instance where data mining really is a collaborative experience. So seek expert advice whenever you can.
<End Transcript>

<-- 2.15 Quiz -->
Signal peptide prediction
Question 1
Analysis of signal peptide length reveals a fairly tight distribution around a mean of 24. Let’s see if position information helps when added to the residue context around the cleavage site.
Apply J48 to sigdata1.arff, using default settings for the classifier and 10-fold cross-validation. Compare results when the len feature is included to when it is left out. Does this feature improve the true-positive rate for both classes?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
The true positive rate is not improved for the yes class; it increases from 77.4% to 77.5% when the len feature is omitted.

<-- 2.15 Quiz -->
Signal peptide prediction
Question 2
When the len feature is removed, which of the other features are not used in the final decision tree?
Select all the answers you think are correct.
pos–3
pos–2
pos–1
pos
pos+1
pos+2
pos+3
---
Correct answer(s):
pos
pos+2
pos+3

<-- 2.15 Quiz -->
Signal peptide prediction
Question 3
If len is the only feature available to predict the class, what value does J48 choose as the constant for splitting the data?
13
14
15
16
17
18
---
Correct answer(s):
15

<-- 2.15 Quiz -->
Signal peptide prediction
Question 4
Apply J48 to sigdata2.arff, which has the same features as the previous dataset, sigdata1, but three times as many negative instances as positive ones. Use default settings once again.
Remove the len feature. Does accuracy improve when compared to the same experiment run on sigdata1?
Yes
No
---
Correct answer(s):
Yes
---
Feedback correct:
The proportion of correctly classified instances improves from 78.8% to 82.7%.

<-- 2.15 Quiz -->
Signal peptide prediction
Question 5
Does the true positive rate for the yes class improve with the additional negative instances?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
It drops from 77.5% to 52.3%.

<-- 2.15 Quiz -->
Signal peptide prediction
Question 6
Which features have been used to predict the class in the tree built for the larger dataset?
Select all the answers you think are correct.
pos–3
pos–2
pos–1
pos
pos+1
pos+2
pos+3
---
Correct answer(s):
pos–3
pos–1
pos+1

<-- 2.15 Quiz -->
Signal peptide prediction
Question 7
Does including the len feature significantly improve the true positive rate for this larger dataset?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
Including len changes the true positive rate from 52.3% to 52.8%, which is negligible.

<-- 2.15 Quiz -->
Signal peptide prediction
Question 8
Biologists believe that physicochemical properties of the signal peptide play an important role in determining the cleavage site.
Some of these properties have been recorded in the sigdata3.arff file. Load this data and run J48 with the default settings.
Does the average true positive rate improve when using general physical properties instead of specific amino acids?
Yes
No
---
Correct answer(s):
Yes
---
Feedback correct:
The average true positive rate is 91.6%, much larger than anything we have seen before.

<-- 2.15 Quiz -->
Signal peptide prediction
Question 9
Which physical property offers the most information gain?
pos
charge-3
small-1
h-region
cleave
---
Correct answer(s):
h-region
---
Feedback correct:
The J48 tree splits first on h-region.

<-- 2.15 Quiz -->
Signal peptide prediction
Question 10
How many additional correct classifications do you get when the pos feature is included in the current dataset as opposed to if it is left out?
(Note: this feature is analogous to the len feature in earlier datasets.)
12
13
14
15
16
17
---
Correct answer(s):
12
---
Feedback correct:
Omitting pos reduces the number of correct classifications from 2573 to 2561

<-- 2.15 Quiz -->
Signal peptide prediction
Question 11
Three times as many negative instances are included in sigdata4.arff as were available in the previous file, sigdata3. Load sigdata4.arff into WEKA and run J48 with default settings.
Does the true positive rate for the cleavage site (i.e. yes) drop as significantly as it did when more negative instances were added to the context data (i.e. sigdata2)?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
It drops from 91% to 85%, far smaller than the drop from 78% to 52% that we observed for sigdata2.

<-- 2.15 Quiz -->
Signal peptide prediction
Question 12
Does the pos feature help to significantly increase accuracy when these additional negative instances are added to the physicochemical data?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
Removing pos affects the accuracy only marginally, from 94% to 93%.

<-- 2.15 Quiz -->
Signal peptide prediction
Question 13
How much does the size of the tree (including leaves) reduce when pos is omitted from the sigdata4 feature set?
4
7
11
15
22
29
---
Correct answer(s):
22
---
Feedback correct:
It drops from 29 nodes to 7
---
Feedback incorrect:
That’s the difference in the number of leaves

<-- 2.15 Quiz -->
Signal peptide prediction
Question 14
Apart from the h-region, which other feature offers the most information gain when the pos feature is omitted?
pos
charge-3
small-1
cleave
---
Correct answer(s):
small-1
---
Feedback correct:
The J48 decision tree involves only two attributes, h-region and small-1

<-- 2.15 Quiz -->
Signal peptide prediction
Question 15
Given the four data sets (sigdata1, sigdata2, sigdata3 and sigdata4), and ignoring position/length information, which one yields a decision tree that is least likely to be overfitting the data and the most likely to be a perspicacious characterisation of the cleavage site?
sigdata1
sigdata2
sigdata3
sigdata4
---
Correct answer(s):
sigdata4
---
Feedback correct:
The J48 tree has only 7 nodes (including leaves), whereas for the other three datasets it has 143, 62, and 13 nodes respectively

<-- 2.15 Quiz -->
Signal peptide prediction
Question 15
Given the four data sets (sigdata1, sigdata2, sigdata3 and sigdata4), and ignoring position/length information, which one yields a decision tree that is least likely to be overfitting the data and the most likely to be a perspicacious characterisation of the cleavage site?
sigdata1
sigdata2
sigdata3
sigdata4
---
Correct answer(s):

<-- 2.16 Discussion -->
Reflect on this week’s Big Question
This week’s Big Question is, “How can you mine continuous data streams?”
We promised that by the end you’d be able to explain, at a high level, how decision trees can be modified incrementally, and compare the performance of incremental and non-incremental decision tree algorithms. You’d be able to use both Weka’s MOA package and the MOA system itself, which contains stream-oriented implementation of many packages and allows different evaluation techniques designed for incremental operation. You’d know the difference between periodic holdout and prequential evaluation, and about adaptive windowing and how to use it for change detection. You’d also have some experience of sentiment analysis using Twitter data.
Well, what do you think now? It’s true that we haven’t discussed how to connect sensors to a data mining program, nor how to deploy models that affect the world on a continuous basis. The incremental algorithms that we have described are real-time in principle, because they update their models in a time that is independent of the volume of data already seen, but we haven’t discussed implementation details. Nevertheless you’ve learned a lot about mining continuous data streams, despite the fact that everything we’ve done has used either data generators (which are potentially infinite) or data already stored in computer files (which are not).

<-- 2.17 Quiz -->
Mid-course assessment 
Question 1
Restart the Weka Explorer (to reset the Forecast parameters to default values) and open the appleStocks2011.arff dataset. What is the first date covered by the dataset (in the form yyyy-mm-dd)?
---
Correct answer(s):

<-- 2.17 Quiz -->
Mid-course assessment 
Question 2
With this dataset, go to the Forecast panel. Why is the Start button not activated?
Weka doesn’t know which attribute to predict
Weka doesn’t know which field contains the time stamp
Weka can’t detect the periodicity of the data
No classifier is specified
---
Correct answer(s):
Weka doesn’t know which attribute to predict

<-- 2.17 Quiz -->
Mid-course assessment 
Question 3
Specify Daily periodicity, and predict any attribute.
In the Output panel you will see that several training instances had missing values imputed via interpolation. What is the instance number of the first and last such instance?
---
Correct answer(s):

<-- 2.17 Quiz -->
Mid-course assessment 
Question 4
Most of these missing-value instances correspond to weekends. Remove these by putting an appropriate specification into the Skip list, and re-run.
What is the instance number of the first and last missing-value instance now?
---
Correct answer(s):

<-- 2.17 Quiz -->
Mid-course assessment 
Question 5
Ignore all missing instances by entering this into the Skip list:
weekend,2011-01-17@yyyy-MM-dd,2011-02-21,2011-04-22,2011-05-30,2011-07-04
Predict the attribute “High” (only). Remove leading instances with missing values (under Lag creation, More Options). Evaluate on the training data and on 10 days of held-out data.
The mean absolute error on the training data is 3.1. What is it on the test data?
3.1
3.3
4.1
6.6
6.7
7.9
---
Correct answer(s):
6.7

<-- 2.17 Quiz -->
Mid-course assessment 
Question 6
Configure the Lag creation panel by setting the minimum and maximum lags both to 1, and by removing powers of time and products of time and lagged variables (under More options).
When you activate Start you will get a model that involves 2 attributes.
Reconfigure the Periodic attributes to get a model that involves a single attribute – a lag attribute.
What is the model?
---
Correct answer(s):

<-- 2.17 Quiz -->
Mid-course assessment 
Question 7
Use the dataset of soil samples org_c_n.arff to predict organic nitrogen (rather than organic carbon, as was done in the Analyzing infrared data from soil samples video).
Load the dataset. Remove unnecessary attributes, and remove instances with a missing value for the class.
How many attributes and instances are left?
Select all the answers you think are correct.
217 attributes (including the class)
220 attributes (including the class)
1555 instances
2884 instances
3911 instances
4439 instances
---
Correct answer(s):
217 attributes (including the class)
2884 instances

<-- 2.17 Quiz -->
Mid-course assessment 
Question 8
A standard classifier used in the soil analysis industry is the Partial Least Squares classifier, called PLSClassifier in Weka.
It’s not in the standard distribution, but in a package called partialLeastSquares. Install this into your Weka system.
What correlation coefficient does PLSClassifier achieve when predicting organic nitrogen from the dataset you prepared in the previous question (default parameters, evaluated using 10-fold cross-validation)?
0.0731
0.1976
0.1978
0.4229
0.4239
0.4249
---
Correct answer(s):
0.4249

<-- 2.17 Quiz -->
Mid-course assessment 
Question 9
Open the elecNormNew.arff dataset in the Weka Explorer and evaluate, the performance of these classifiers (using 10-fold cross-validation):
    A: Weka’s Hoeffding tree
    B: MOA’s Hoeffding tree
    C: MOA’s Hoeffding Option Tree
    D: MOA’s Hoeffding Adaptive Tree
(To access the last three, you will need to have installed Weka’s massiveOnlineAnalysis package.)
How does their performance compare, in terms of correctly classified instances?
A < C < D < B
B < A < D < C
B < D < A < C
C < A < D < B
D < C < B < A
---
Correct answer(s):
B < D < A < C

<-- 2.17 Quiz -->
Mid-course assessment 
Question 10
Use MOA to determine the accuracy of HoeffdingTree on a WaveformGenerator stream of 1,000,000 instances, with Prequential evaluation and the BasicClassificationPerformanceEvaluator.
What is the final current accuracy? (Note: current accuracy, not mean accuracy.)
75.71%
75.84%
75.85%
82.98%
83.80%
83.89%
---
Correct answer(s):

<-- 2.17 Quiz -->
Mid-course assessment 
Question 10
Use MOA to determine the accuracy of HoeffdingTree on a WaveformGenerator stream of 1,000,000 instances, with Prequential evaluation and the BasicClassificationPerformanceEvaluator.
What is the final current accuracy? (Note: current accuracy, not mean accuracy.)
75.71%
75.84%
75.85%
82.98%
83.80%
83.89%
---
Correct answer(s):
83.89%

<-- 2.18 Discussion -->
How are you getting on?
By now the course is well underway: two weeks down and three to go.
What do you think of it so far?
Do you want to discuss any issues that have arisen?
This is the place to do it!

<-- 2.19 Article -->
Index
(A full index to the course appears at the end of Week 1.)
      Topic
      Step
      Datasets
      Forest cover type (covtypeNorm)
      2.3, 2.9, 2.12
      labelled_tweets
      2.12
      NSW Electricity Market (elecNormNew)
      2.5, 2.12
      Signal peptide cleavage data (sigdata)
      2.13, 2.14
      Classifiers
      HoeffdingAdaptiveTree
      2.8, 2.9
      HoeffdingOptionTree
      2.5
      HoeffdingTree (Weka)
      2.2, 2.3, 2.5
      HoeffdingTree (MOA)
      2.5, 2.6, 2.7, 2.9, 2.10
      IBk
      2.3
      Incremental
      2.2, 2.3
      J48
      2.3, 2.13, 2.14
      MajorityClass
      2.9, 2.12
      NaiveBayes
      2.7, 2.9
      NaiveBayesUpdateable
      2.3
      NaiveBayesMultinomial
      2.10, 2.12
      SGD (Stochastic gradient descent)
      2.10
      SGDText
      2.12
      Updateable classifiers
      2.2
      Metalearners
      LeveragingBag
      2.8, 2.9
      OzaBag
      2.4, 2.5, 2.7, 2.9
      OzaBagAdwin
      2.8, 2.9
      Filters
      Resample
      2.5
      StringToWordVector
      2.12
      Packages
      massiveOnlineAnalysis (MOA)
      2.4, 2.5
      Generators
      HyperplaneGenerator
      2.5, 2.6
      LED24 (Weka)
      2.3
      LEDGenerator (MOA)
      2.4
      RandomRBFGenerator
      2.8, 2.9
      RandomRBFGeneratorDrift
      2.9
      WaveformGenerator
      2.7
      Evaluation
      Kappa statistic
      2.7, 2.10, 2.12
      Kappa-temporal statistic
      2.12
      Periodic holdout
      2.6, 2.7
      Prequential
      2.6, 2.7, 2.9, 2.10, 2.12
      Plus …
      Adaptive sliding window (ADWIN)
      2.8
      Bioinformatics
      2.13, 2.14
      Bootstrap sampling
      2.8
      Data stream mining
      2.1–2.12
      MOA system
      2.6–2.12
      Sequence analysis
      2.13, 2.14
      Sentiment analysis
      2.10, 2.11, 2.12
      Signal peptide prediction
      2.13, 2.14
      Twitter
      2.10, 2.11, 2.12

<-- 3.0 Todo -->
Reaching out to other data mining packages 
What’s in Weka's LibSVM and LibLINEAR packages?
This week's first Big Question!
3.1
What’s in Weka's LibSVM and LibLINEAR packages?
article
LibSVM and LibLINEAR
LibLINEAR and LibSVM are Weka's most popular packages. The first contains many algorithms for linear classification: it's very fast. The second implements non-linear SVMs, and requires parameter optimization (using gridSearch).
3.2
LibSVM and LibLINEAR
video (08:45)
3.3
One-class classification
discussion
3.4
One-class classification 
quiz
3.5
One-class classification results
article
How do you access R from Weka?
This week's second Big Question
3.6
How do you access R from Weka?
article
Setting up R with Weka
R is a programming language for statistical computing, whose tools for classification, regression, and plotting data can be accessed through WEKA via the RPlugin.
3.7
Setting up R with Weka
video (08:13)
3.8
Getting started with R
discussion
3.9
Using the Explorer’s R console
quiz
Using R to plot data
A well-known library for R, ggplot2,
enables construction of complex plots of data using layers. The RConsole for WEKA’s Explorer can be used to execute plotting commands.
3.10
Using R to plot data
video (13:14)
3.11
Plotting data with ggplot2 
quiz
Using R to run a classifier 
The MLR library interfaces to a vast collection of classification algorithms. These can be accessed from Weka just like regular classifiers, using the MLRClassifier. Algorithm implementations can be compared using the Experimenter
3.12
Using R to run a classifier
video (09:31)
3.13
Learning algorithms in the MLR package 
quiz
Using R to preprocess data
R contains many data preprocessing functions. These can be applied as part of a workflow in the KnowledgeFlow interface, using the RscriptExecutor. 
3.14
Using R to preprocess data
video (10:25)
3.15
Using R to preprocess data 
quiz
Analyzing functional MRI Neuroimaging data 
Analyzing functional MRI data is a machine learning problem, and one application is to diagnose Attention-deficit/hyperactivity disorder (ADHD). Functional MRI neuroimaging data is enormous, and high-dimensional!
3.16
Analyzing functional MRI Neuroimaging data
video (04:51)
3.17
Playing with fMRI data 
quiz
3.18
Playing with fMRI data 
article
3.19
Reflect on this week’s Big Questions
discussion
3.20
Index
article

<-- 3.1 Article -->
What’s in Weka's LibSVM and LibLINEAR packages?
This week is about reaching out from Weka to other open source data mining software systems.
We have two big questions: the first about the LibSVM and LibLINEAR systems, and the second about the R statistical programming language.
What’s in Weka’s LibSVM and LibLINEAR packages?
You already know how easy it is to install Weka packages and access the new data mining schemes they offer. These two are Weka’s most popular packages, and they simply contain two eponymous machine learning schemes, LibSVM and LibLINEAR – which were created by the same people and are widely used outside Weka.
So the question is: what do they do? Both methods implement schemes that are already available in the Weka core: SMO and LinearRegression. So why would you use them? And how do you use them?
By the end of the week (after the upcoming lesson, in fact), you will know.

<-- 3.2 Video -->
LibSVM and LibLINEAR
Ian Witten demonstrates LibLINEAR, which contains fast algorithms for linear classification; and LibSVM, which produces non-linear SVMs. Both implement support vector machines – which are already available in Weka as the SMO method. The difference is that LibLINEAR is generally far faster than SMO (and can, optionally, minimize the sum of absolute values of errors instead of the sum of squared errors), while LibSVM is far more flexible. Support vector machines can be made to implement different kinds of non-linear decision boundaries using different kernels, and the effect can be explored using Weka’s boundary visualizer. They benefit greatly from a parameter optimization process, which can be done using Weka’s gridSearch meta-classifier.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello! Nice to see you. Nice to be back. It’s me again. This is Class 3, interfacing to other data mining packages. We’re going to concentrate on the R package for most of this class, but just to begin with, we’re just going to look at the LibSVM and LibLINEAR packages. These are written by the same people. They are widely used outside of Weka, and they are also Weka’s most popular packages. You should install them. I’ve got them installed, and also you should install the gridSearch package, as well. Both of these packages are to do with support vector machines. Weka already has the SMO implementation for support vector machines, but LibSVM is more flexible and LibLINEAR can be much faster.
It’s important to know that SVMs can be either linear or nonlinear through a kernel function. Also, they can do classification or regression, which we haven’t mentioned. Weka contains SMOreg for regression, the same algorithm. We’re going to use the gridSearch method to optimize parameters for SVMs, which is quite important. Let’s just look at LibSVM and LibLINEAR, these two packages, and also the standard SMO and SMOreg. All three implement linear SVMs. All but LibLINEAR are capable of accommodating nonlinear kernels. LibSVM does one-class classification. LibLINEAR does logistic regression. It’s linear. LibLINEAR is very fast, and LibLINEAR can operate with the L1 norm, which I’m not going to explain in this lesson. Just a quick look at LibLINEAR. I did a speed test.
I used the data generator on Weka’s Preprocess panel to generate 10,000 instances of this data, LED24. LibLINEAR took two seconds to build the model. LibSVM took 18 seconds to build the model, but that’s a slightly unfair comparison because it’s using a nonlinear kernel. So when I changed it to use a linear kernel, it took 10 seconds. And SMO with default parameters, which is a linear kernel, took 21 seconds. So you can see LibLINEAR is quite a lot faster. Now, let’s just talk about linear boundaries and support vector machines in general. Support vector machines try to drive a channel between the two classes.
Here we’ve got the blue class and the green class, and they try and drive a channel halfway between the classes to leave as large a margin as possible. In this case, we’ve got zero errors on the training data, and a pretty small margin, the distance between the dashed lines. However, when we look at the test data – now this is an artificial dataset – but in this case you can see that some points in the test data are being classified incorrectly. Four points, in fact.
If, instead of using this line, we turned it a bit and used a line with a much larger margin, although it makes one error on the training data, in this particular situation it gets all of the test data correct – no errors on the test data. It’s an advantage sometimes to have a large margin, even at the expense of errors on the training data. SVMs try to give you large margin classifiers. Here we are with a nonlinear dataset. I’ve drawn a linear boundary here, the boundary that’s produced by LibLINEAR or LibSVM with a linear kernel, or indeed the SMO package and the SMO classifier in Weka. This gives 21 errors on the dataset, or training set.
Here’s a nonlinear boundary for the same dataset, implemented by LibSVM with an RBF kernel. I’ve got this dataset open in Weka’s BoundaryVisualizer over here, and I’m going to just choose LibSVM. Luckily, I’ve installed the package already and I just start. OK, let’s speed this up.
There we are. That’s the result, and you can see it’s making some errors down here and up here on the dataset, on the training set. Let’s just go to the Explorer. I’ve got the same data file open, and I’m going to go again to LibSVM and take a look. We’re plotting the training set here, so if I look at that I get a total of 9 errors, 4 and 5 respectively on the different training set parts. That’s with the default parameters. If I change the LibSVM parameters, then I can get this boundary.
Now this is quite a good boundary, because it gives 0 errors on the training set, but it gives poor generalization, because it doesn’t drive a channel right between those two classes. With different parameters, I can continue to get 0 errors on the training set but a much more satisfactory boundary, which will probably generalize better. Whenever you use nonlinear support vector machines you need to optimize the parameters. The parameters we’re talking about are called “cost” and “gamma”. When we optimize parameters in Weka, we use the gridSearch method, which is in the meta category. These are the parameters for gridSearch. The default configuration for gridSearch, well let’s look at it.
Down at the bottom, it says use SMOreg, that’s the default, and evaluate using the correlation coefficient. We’re going to need to change those. Then the first 6 boxes are talking about X of the grid and the next 6 boxes about Y. The X property being optimized is called C, and that’s going from 10^3 down to 10^–3 in multiplicative steps of 10. That’s what those first 6 parameters signify. The second 6 parameters give the same range with the Y property of kernel.gamma. That’s for SMOreg. If we want to use LibSVM, we need to change some things. We’re going to optimize the properties cost and gamma. We’re going to choose the classifier LibSVM and we’re going to evaluate using Accuracy.
Let me set that up in Weka. I’m going to choose gridSearch from the meta category. In gridSearch, I’m going to first of all choose the classifier. I’m going to choose LibSVM. I’m going to optimize – let’s move this up so you can see – optimize the Accuracy. And the two properties involved are cost and gamma. If I run that ... it’s finished here, and the result is – the parameters are 1000 for the X coordinate, that’s cost, and 10 for the Y coordinate, that’s gamma. We’ve got 100% accuracy with that dataset. We could see we were going to get 100% accuracy when we looked at the boundary visualization. That’s for LibSVM.
If we were to choose a different method, like SMO, it’s got different parameters. Let me just look at SMO here. I’m going to choose SMO. I need to find the appropriate parameters. Here’s the SMO parameters. I want C here for the cost, and if I look at the kernel, I want an RBF kernel, and in the RBF kernel the key parameter here is gamma. So it’s kernel.gamma. Kernel here dot gamma here. I’m going to use C and kernel.gamma. C and kernel.gamma. That will allow me to optimize SMO.OK, so gridSearch is fairly complicated to use, but it’s necessary to optimize the parameters when using nonlinear support vector machines. Here’s a summary.
We’ve looked at LibLINEAR, which does all things linear, linear SVMs, logistic regression, and it can use the L1 norm, which minimizes the sum of absolute values, not the sum of squares, which has big advantages under certain conditions and is very fast.LibSVM is all things SVM, linear and nonlinear SVMs. The practical advice when you want to use SVMs is first use a linear SVM – do it quickly with libLINEAR, perhaps, and see how you get on. Then for a nonlinear SVM, select the RBF kernel. But when you select a nonlinear kernel like RBF, it’s really important to optimize cost and gamma, and you can do this using the gridSearch method. Here’s a reference to support vector machines, to these packages.
<End Transcript>

<-- 3.3 Discussion -->
One-class classification
Though we didn’t see it in the video, LibSVM is capable of tackling a new type of problem that we have’t encountered before: one-class classification.
In two-class classification, the problem is to distinguish instances in one class (say A) from instances in the other (B). Positive instances for B are negative instances for A, and vice versa. Or maybe not every non-A is a B, in which case we have three classes: A, B, and Neither. That’s three-class classification. And multi-class classification is an obvious further extension (A, B, C, etc.).
But one-class classification is different. There’s only one class, and no negative instances. Though we haven’t encountered this before, it’s common in real life.
One-class classification is weird! Two-class classification might separate the sheep from the goats, whereas one-class classification tries to separate the sheep from the … what? … non-sheep?
But there are many real-life examples. Consider outlier detection, anomaly detection, and novelty detection. How about the classification of the operational status of a nuclear power plant as “normal” (fortunately, there are very few negative examples!).
Can you think of other situations that call for one-class classification?

<-- 3.4 Quiz -->
One-class classification 
Question 1
The iris dataset contains 50 Iris-setosa instances, 50 Iris-versicolors, and 50 Iris-virginicas—in that order. We will train a one-class classifier on the first 26 instances in the dataset—that is, about half of the Setosas—and test it on the remaining Setosas, all the Versicolors, and all the Virginicas. Ideally it will classify the remaining Setosas as “Setosa” and all the other instances as “not Setosa”.
Open the iris.arff dataset, select the LibSVM classifier, and set SVMType to one-class SVM.
Use a Percentage split of 17%, which will put 26 instances in the training set. Prepare yourself for disappointment, and click Start.
What went wrong?
Nothing
LibSVM cannot handle numeric attributes
LibSVM cannot handle a multivalued class (in One-class classifier mode)
---
Correct answer(s):
LibSVM cannot handle a multivalued class (in One-class classifier mode)
---
Feedback incorrect:
You may find
1.20 Reflect on this week’s Big Question
useful.

<-- 3.4 Quiz -->
One-class classification 
Question 2
LibSVM with 1-class classification only takes a dataset with one class value. This is kinda dumb, but there it is.
In the Preprocess panel: delete the class from the dataset; use a filter to add a new nominal class attribute (the Add filter adds an attribute whose values are all “missing”); and use another to replace the missing values with “setosa” (note: the ReplaceMissingWithUserConstant filter does not operate on the class value).
Go to the Classify panel. Before you click Start, answer this question: How many instances are in the test set?
0
50
100
124
150
---
Correct answer(s):
124
---
Feedback correct:
You set the Percentage split to train on 17% of instances, so the “Total number of instances” in the test set is 124 (83% of the 150 in the dataset).

<-- 3.4 Quiz -->
One-class classification 
Question 3
Now click Start and check the output.
26 instances have been used for the training set, so the test set is supposed to represent 24 Iris-setosas, 50 Iris-versicolors, and 50 Iris-virginicas. The classifier is attempting to classify the Iris-setosas as “setosa”; the remainder are deemed to be “not setosa”.
How many instances in the test set are classified as “setosa”?
24
26
50
58
62
124
---
Correct answer(s):
58
---
Feedback incorrect:
This would be expected if the Iris-setosas are being identified correctly
---
Feedback incorrect:
This would be expected from a random split into “setosa” and “not setosa” (124 instances divided by 2)

<-- 3.4 Quiz -->
One-class classification 
Question 4
How many of these does Weka consider to be correct?
0
24
26
50
all of them
---
Correct answer(s):
all of them

<-- 3.4 Quiz -->
One-class classification 
Question 5
Is this good?
yes
no
---
Correct answer(s):
no
---
Feedback correct:
All the instances in the dataset are marked with class “setosa”, so of course these 58 are all judged correct. But our intention was to include 26 of the original 50 setosas in the training set, leaving only 24 actual setosas in the test set.
---
Feedback incorrect:
This is a rhetorical question!

<-- 3.4 Quiz -->
One-class classification 
Question 6
The problem is that with the Percentage split option, Weka randomizes the order of the dataset before doing the split. Find out how to prevent this (look under More options).
How many test instances are classified as “setosa” now?
0
15
100
124
150
---
Correct answer(s):
15

<-- 3.4 Quiz -->
One-class classification 
Question 7
Use the “More options” menu to output the predictions as plain text.
How many errors does the LibSVM one-class classifier make on the Iris-setosas, Iris-versicolors, and Iris-virginicas respectively?
0, 0, 0
9, 0, 0
15, 0, 0
50, 50, 50
---
Correct answer(s):
9, 0, 0
---
Feedback correct:
There are 9 question marks (i.e. classified as “not setosa”) amongst the first 24 instances output, which really are Iris-Setosas, and all the remaining instances — Iris-versicolors and Iris-virginicas — are correctly classified as “not setosa”.

<-- 3.4 Quiz -->
One-class classification 
Question 8
Install the oneClassClassifier package in Weka, and choose the OneClassClassifier from the “meta” classifiers.
It’s set up more sensibly than the LibSVM one-class classifier. Load the original iris dataset and configure the OneClassClassifier to use Iris-setosa as the targetClassLabel. Press Start.
With the same training data as before (17% percentage split; preserve order), how many errors does this one-class classifier make on the Iris-setosas, Iris-versicolors, and Iris-virginicas respectively?
0, 0, 0
2, 0, 0
9, 0, 0
15, 0, 0
---
Correct answer(s):
2, 0, 0
---
Feedback correct:
There are 2 question marks (i.e. “not setosas”) amongst the first 24 instances output, which really are Iris-Setosas, and all the remaining instances are correctly classified as “not setosa”.

<-- 3.4 Quiz -->
One-class classification 
Question 9
Weka’s OneClassClassifier is quite sophisticated and has a host of parameters, most of which are obscure. However, the first is the base classifier, which by default is “Bagging.” This is a fairly advanced classifier.
Choose NaiveBayes as the base classifier. How many errors does the one-class classifier make on the Iris-setosas, Iris-versicolors, and Iris-virginicas respectively?
2, 0, 0
3, 0, 0
9, 0, 0
15, 0, 0
---
Correct answer(s):

<-- 3.4 Quiz -->
One-class classification 
Question 9
Weka’s OneClassClassifier is quite sophisticated and has a host of parameters, most of which are obscure. However, the first is the base classifier, which by default is “Bagging.” This is a fairly advanced classifier.
Choose NaiveBayes as the base classifier. How many errors does the one-class classifier make on the Iris-setosas, Iris-versicolors, and Iris-virginicas respectively?
2, 0, 0
3, 0, 0
9, 0, 0
15, 0, 0
---
Correct answer(s):
3, 0, 0
---
Feedback correct:
There are now 3 question marks (i.e. “not setosas”) amongst the first 24 instances output, and question marks on all other instances.

<-- 3.5 Article -->
One-class classification results
Frankly, I found the results obtained in the preceding quiz amazing.
Trained on only 26 instances of Iris-Setosa, Weka’s OneClassClassifier is 92% accurate (2 errors out of 24) on the remaining Iris-setosas and 100% accurate on the Iris-versicolors and Iris-virginicas. This is pretty impressive. Even using Naive Bayes as the base classifier, it is 88% accurate (3 errors). The LibSVM one-class classifier is worse: 62% accurate (9 errors) on Iris-setosas but still 100% accurate on the rest.
However, we were lucky. In the Iris dataset, the Iris-setosa class stands pretty well by itself—you can see this in Weka’s Visualize panel. If you were to train on some data from one of the other classes instead, the results would not be nearly as good: all the Iris-setosas would probably be identified as outliers but more errors would occur on the Iris-versicolors and Iris-virginicas.
And even with Iris-setosas, if you train OneClassClassifier (default parameters) on 15% or 20% of the dataset (23 and 30 instances respectively) instead of 17% (26 instances), the number of errors increases (to 3 in both cases). And if you train on 10% (15 instances) the classifier doesn’t work at all—it declares that the entire test set is made up of Iris-setosas!
One-class classification is difficult. But under the right circumstances it can be successful.

<-- 3.6 Article -->
How do you access R from Weka?
How do you access R from Weka?
Well, what is R anyway? And why is it important? And what can you do with it?
Questions abound. Now for the answers.

<-- 3.7 Video -->
Setting up R with Weka
R is a powerful statistical programming system that contains data mining tools for classification, regression, and plotting data, some of them very advanced. Eibe Frank shows how to access these from Weka. Setting this up is a little tricky: it involves installing R, installing a package (an R package) into R, setting some environment variables, and installing Weka’s RPlugin package. This video demonstrates the process. Detailed instructions are given in the accompanying download (these slides do not appear in the video itself). Be sure to install the latest version of R (3.4.0 or later).
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello! My name is Eibe Frank. I’m with the Department of Computer Science at the University of Waikato, the home of Weka, and it is my job to tell you a bit about how to use the statistical computing environment R from Weka. So let’s get started. Because R is implemented in a different programming language than Weka, which is implemented in Java, getting things set up so that Weka can use R is a little bit tricky, but we will go through the steps in this first video. The following assumes that you’re using 64-bit Windows, 64-bit R, and 64-bit Java. You can also do the same if you use 32-bit versions of everything.
Furthermore, we’ll assume that you have administrator access on your computer, and we assume that you have a direct connection to the Internet. All right. The first thing we need to do is download R and install it. Download the current version, to save some time, I’ve already downloaded R, and we can just install it from here. OK. We accept this. We run the installer. We want English as the language, and we just accept the license, which is the same as the one used for Weka. Accept the default install location. Now, because I want to use 64-bit R, I unselect 32-bit files here, and then I just go with the standard setup.
I also want to create a Start menu folder, and we accept the defaults here, as well. OK, finished! Now we have installed R. The first thing we should do is install a particular package in R that is necessary for R to be able to communicate with Java, the programming environment that Weka is implemented in. We start R from the shortcut, and we get the R console, where we can enter text commands. This is the standard way to interact with R, because R is really a programming language. We type in install.packages(“rJava”).We want to install this in the personal library, and we want to create this library.
Because I’m in New Zealand, I want to download from a New Zealand computer, a New Zealand server, so I click on New Zealand here. OK, rJava has been installed successfully. We just close this, and now what we need to do next is set up some environment variables. We search for “variables” using the Windows search functionality, and then we click the item “Edit environment variables for your account”. There are already some environment variables there, we need to add some new ones. We click on New to enter a new variable. This new variable is called R_HOME, and the variable value is the location of the R distribution.
To find this, we right-click on the R shortcut and we go in Properties, and now we have the location of the R distribution here. It’s the path to the directory containing the R binaries. We select everything up to the “bin” folder. Then we paste it here. That’s the R_HOME variable. The next variable we need to insert is the R_LIBS_USER variable, which determines the location of the user libraries that R installs. Now, we’ve already installed one user library, namely the rJava library, so we just need to find it and put the location of this library here. Let’s just use the Windows search functionality again to search for rJava. It’s a file folder. Now we just go up one level.
This is the folder containing all the user libraries for R, so we right-click on this text field and we select “Copy address as text” to copy this path. Then we go back to our form to enter the variable value for our user variable. We right-click and we paste it in. We’re almost done now. The last thing we need to do is modify the PATH environment variable to include the directory containing the R executable. We select this PATH environment variable and click on the Edit button, and at the end we add a semicolon, and then we use the location of the R executable. In this case, we actually use this bit of the path for the R executable.
This should be it. We just go OK here. Now what we need to do is install the R plugin package for Weka, which is Weka’s interface for R. We start Weka, and we go to the Package Manager. It just refreshes the package cache at the start. Once it’s done that and popped up the window, we can select the R plugin. RPlugin is here. We choose the install button. OK. Right. There’s quite a bit of information here in this window, install information for the R plugin. This is about setting the environment variables that we just set before, so we just click OK here.
Now it takes a little while for R to be downloaded and installed, but it doesn’t take too long. It actually also installs an additional R library, the Java JD library for R, which makes it possible to output R plots in Java. OK, now it’s finished. You just need to restart Weka. We close this, close this. Start Weka again. Now when we start the Explorer, we can load in some data. In this case, just go to the Program Files folder, and then the Weka folder, and there’s a data folder. We load in the iris data.
Now we can go to the R console, which is a new tab here that comes as part of the R plugin package, which provides us with a console for R implemented in Java. This console allows us to address the data that we’ve loaded in the Preprocess panel using the name “rdata”. We can go plot(rdata), and this will give us a plot of the iris data generated by R.
<End Transcript>

<-- 3.8 Discussion -->
Getting started with R
Now is the time to download and install R, as explained in the video.
Detailed instructions are given in the slides that accompany the video (though they do not appear in the video itself). Be sure to install the latest version of R (3.4.0 or later).
How did it go? It should be easy, but you never know … If you have problems, post them. (Don’t forget to include details such as the computer, maybe system version too.) If you’ve figured out the solution, post that. And if you can help someone else, please do so! You’re a community.

<-- 3.9 Quiz -->
Using the Explorer’s R console
Question 1
One useful R function in R is summary(), which when applied to a data frame outputs summary statistics for all the variables (i.e., attributes) in the data. In particular, it outputs the median and the 1st and 3rd quartile, which are not output by the Preprocess panel.
Apply the summary() function to the diabetes.arff dataset. (Remember that the data loaded into the Preprocess panel is available as a data frame in the RConsole under the name rdata.)
What is the median value of the plas attribute?
3
72
117
121
---
Correct answer(s):
117
---
Feedback incorrect:
That’s the median for the preg attribute
---
Feedback incorrect:
That’s the median for the pres attribute
---
Feedback incorrect:
That’s the mean, not the median

<-- 3.9 Quiz -->
Using the Explorer’s R console
Question 2
For which attribute does approximately one-quarter of the data have a value greater than 80?
preg
plas
pres
skin
insu
mass
pedi
age
---
Correct answer(s):
pres
---
Feedback correct:
The third quartile for this attribute is 80

<-- 3.9 Quiz -->
Using the Explorer’s R console
Question 3
For which attribute does approximately three-quarters of the data have a value greater than 24?
preg
plas
pres
skin
insu
mass
pedi
age
---
Correct answer(s):
age
---
Feedback correct:
The first quartile for this attribute is 24

<-- 3.9 Quiz -->
Using the Explorer’s R console
Question 4
The difference between the two quartiles is called the “interquartile range”, and is a statistically robust measure of dispersion – in contrast to standard deviation, which can be heavily influenced by outliers.
The IQR() function in R computes the interquartile range for an attribute. For example, to apply this function to the pres attribute in the diabetes data (assuming it has been loaded into the Preprocess panel), use IQR(rdata$pres). (The $ operator is used to extract the values of a particular column in a data frame.)
Apply the IQR() function to all numeric attributes in the data. Which one has the largest interquartile range?
preg
plas
pres
skin
insu
mass
pedi
age
---
Correct answer(s):
insu
---
Feedback correct:
This has IQR = 127.25, which is greater than for any other attribute

<-- 3.9 Quiz -->
Using the Explorer’s R console
Question 5
For data that follows a normal distribution, the interquartile range is about 1.35 times the sample standard deviation.
Which attribute has the smallest value when the interquartile range given by the IQR() function is divided by the sample standard deviation given by the sd() function?
preg
plas
pres
skin
insu
mass
pedi
age
---
Correct answer(s):
pres
---
Feedback correct:
You can apply a function automatically to all columns of a data frame, e.g., try sapply(rdata, function(x) IQR(x)/sd(x))
For this attribute IQR(rdata$pres)/sd(rdata$pres) = 0.93, which is less than for any other attribute
---
Feedback incorrect:
You can apply a function automatically to all columns of a data frame, e.g., try sapply(rdata, function(x) IQR(x)/sd(x))

<-- 3.9 Quiz -->
Using the Explorer’s R console
Question 6
We can generate useful plots using R’s basic plotting functionality.
Applying the hist() function to an attribute produces a histogram.
Plot a histogram for the pres attribute. If you rank the bins of the histogram by the number of instances they contain, with the largest bin receiving rank 1, what is the rank of the leftmost bin?
1
2
3
4
5
6
7
8
9
10
---
Correct answer(s):
5

<-- 3.9 Quiz -->
Using the Explorer’s R console
Question 7
Applying the boxplot() function to an attribute produces a boxplot.
A boxplot shows a box bounded by the two quartiles of the attribute, and it also shows the median. By default, the whiskers shown in the plot extend from the box to the most extreme attribute value that is no more than 1.5 times the interquartile range from the box. Attribute values that are beyond the extent of the whiskers are shown as outlier points in the plot.
Generate a boxplot for the pres attribute.
How many visually distinct points are shown as locations of outliers in the plot?
1
2
3
4
5
6
7
8
9
10
---
Correct answer(s):
7

<-- 3.9 Quiz -->
Using the Explorer’s R console
Question 8
The table() function in R, when applied to an attribute, gives a list of attribute values that occur in the data; beneath each one is the number of times that value occurs.
Apply it to the pres attribute. How many times does the value 0 occur?
0
1
2
24
30
35
---
Correct answer(s):
35
---
Feedback correct:
This is the number that appears beneath “0”

<-- 3.9 Quiz -->
Using the Explorer’s R console
Question 9
We may be interested in a particular percentile of a numeric attribute, rather than just the 1st quartile, 3rd quartile, and median.
The quantile() function is used for this. For example, quantile(rdata$plas, 0.01) gives the first percentile of the data.
What is the 99th percentile of the plas attribute?
181
184
188
192
196
199
---
Correct answer(s):
196

<-- 3.9 Quiz -->
Using the Explorer’s R console
Question 10
What is the 5th percentile of the plas attribute?
73
76
79
80
82
83
---
Correct answer(s):
79

<-- 3.9 Quiz -->
Using the Explorer’s R console
Question 11
The covariance and correlation coefficients between two numeric attributes in the data are given by the cov() and cor() functions respectively.
For example, cov(rdata$plas, rdata$age) gives the covariance between plas and age.
What is the covariance between age and preg?
0.54
3.14
21.57
22.56
33.10
36.67
---
Correct answer(s):
21.57

<-- 3.9 Quiz -->
Using the Explorer’s R console
Question 12
What is the correlation coefficient between age and preg?
0.54
3.14
21.57
22.56
33.10
36.67
---
Correct answer(s):
0.54

<-- 3.9 Quiz -->
Using the Explorer’s R console
Question 13
For numeric attributes, R can compute skewness and kurtosis.
Skewness is given by the skewness() function. Positive values indicate right-skew; negative values left-skew. Kurtosis, given by the kurtosis() function, is a measure of peakedness of a distribution: negative values indicate a flat distribution; positive ones a peaked distribution.
To access these functions you need to install and load R’s e1071 package. Use the R command
install.packages("e1071")
to download and install the package and
library(e1071) 
to load it into the current R session. When applying these functions, it is instructive to consider them in conjunction with the histogram for the attribute concerned.
Compute skewness for all the numeric attributes in the diabetes data.
Which attribute has the smallest skewness value?
preg
plas
pres
skin
insu
mass
pedi
age
---
Correct answer(s):
pres
---
Feedback correct:
This has skewness –1.84, which is less than for any other attribute

<-- 3.9 Quiz -->
Using the Explorer’s R console
Question 14
Which attribute has the largest kurtosis value?
preg
plas
pres
skin
insu
mass
pedi
age
---
Correct answer(s):
insu
---
Feedback correct:
This has kurtosis 7.13, which is greater than for any other attribute

<-- 3.9 Quiz -->
Using the Explorer’s R console
Question 14
Which attribute has the largest kurtosis value?
preg
plas
pres
skin
insu
mass
pedi
age
---
Correct answer(s):

<-- 3.10 Video -->
Using R to plot data
This video demonstrates an R package called ggplot2 that provides extensive plotting capabilities, which can be accessed from Weka. Detailed instructions are given in the accompanying download (these slides do not appear in the video itself).
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Today we’re going to look a bit more at the extensive plotting functionalities in R. More specifically, we’ll look at the ggplot2 package for R and what kinds of plots we can generate with this package. OK! Let’s get started. To save some time, I’ve loaded the iris data into the Explorer already. Now we go to the R console to issue our commands in the R language. The first thing we need to do is install the ggplot2 package, which is the plotting package that
we want to use: install.packages(“ggplot2”).OK, it’s finished. Now that we’ve downloaded and installed the package, we can load it into the R environment by using the library function. We use library(ggplot2).Now the library is loaded, we can use it to plot some data. In ggplot2 we construct a plot in layers. We can add several different layers of plots to construct very complex plots, but there’s one layer that is always present in every plot. That is the data layer, which specifies the data that needs to be plotted. The data layer is specified using the ggplot function. With the ggplot function, we specify the data we want to use. In this case, the data is
referred to using the “rdata” variable: rdata is the name of the variable that refers to the data that we’ve loaded into the Preprocess panel. Then we need to also say which attributes we want to use. This is done using the aesthetics function, the aes function. For the second argument, use the result returned by the aesthetics function. Say x = petallength to specify the petallength attribute as the attribute we want to plot. In this case, you’re just generating a plot based on a single attribute in the data. This is now the data layer for our plot. We also need to add a geometry later, which actually specifies what type of plot we want to generate.
Let’s say we want to generate a kernel density estimate based on this attribute that we have selected. Then we add another layer to our plot using the + operator. We call the geometry function for density estimates, geom_density(). OK, let’s try this. Right. Now we have a kernel density estimate for the petallength attribute. On the x-axis we have the value of the petallength attribute, and on the y-axis we have the have the density estimate. You can see that there are two peaks in this density estimate, but you can also see that the plot is not wide enough to cover the entire area that is relevant.
We should increase the limits of the plot, and we can do that by adding a call to the xlim function, where we specify the lower limit and the upper limit. Let’s say we use 0 as the lower limit and 8 as the upper limit. That looks better, but perhaps this kernel density estimate is still a little bit too smooth. It doesn’t show enough detail in the data, because the kernels that are used are too wide. Let’s reduce the width of each kernel. We can do that by specifying the adjust argument for the geom_density function. This multiplies the width of each kernel by the given parameter. Let’s say we halve the width of each kernel estimator.
Now we get a plot showing a little bit more detail. In Weka, we primarily deal with classification problems. So, really, we should try to take the class information into account in our plot. We can do that by generating three different plots, one for each class value, and combine them into one graph. How do we do that? It’s very simple. We just add another argument to the call of the aesthetics function. Just say the color is given by the “class” attribute in rdata. “Class” is the name of the class attribute in the iris data. We just say that the color is based on the class attribute. Now we get a separate kernel density estimate for each of the three classes.
You can see that the distributions for iris_versicolor and iris_virginica overlap a little bit, but iris_setosa is nicely separated. We may want to enhance this plot by filling the area under each estimate. This is also easy. It’s again done by providing an additional argument to the aesthetics function. You just say the fill color should also be based on the class attribute. You can see that there is a little bit of a problem here. We can’t really differentiate the iris_versicolor and the iris_virginica cases. We should introduce some transparency in our plot. We can do that by providing an “alpha” value for our kernel density estimators.
This is a values between 0 and 1 that determines the amount of transparency: 1 means no transparency; 0 means totally transparent. Let’s set this to 0.5.Now we have a nice plot of the three kernel density estimates. Let’s say we want to plot the same kind of plot, but for all four attributes in the iris data, not just the petallength attribute. We can also do that, but we need to massage our data a little bit to achieve that. We need to load a library
called “reshape2”: library(reshape2). Then, we can call the so called “melt”
function to transform our data into an appropriate format: melt(rdata). The new data, the new format, will be stored in ndata. Let’s just have a look at what this data looks like. We can just type in “ndata”, and it will show us the data. You can see that we have 600 instances in the transformed dataset. There are three attributes in the dataset. The class value is given as the value of the first attribute. The name of the attribute is given as the second attribute, and the attribute value is given as the third attribute.
Scrolling all the way up to the first instance, we can see the first attribute now is called “class”, the second attribute is called “variable”, and the last attribute is called “value”. We have 600 instances because there are 4 attributes and 150 instances in the original dataset. We now have a separate dataset for each of the attributes. First we have all of the attribute values for the 150 iris flowers for sepallength. Then we have all the 150 iris flowers for sepalwidth. Then we have petallength, and finally we have petalwidth. Now that we have the data in this format we can use the “variable” attribute as a way to generate different plots for each attribute. How do we do that?
It’s quite simple. Our X value is now based on the “value” attribute in this transformed data. That is the actual numeric value for each of the attributes. The color is still based on the class, and, at the end, we now use the facet_grid function to generate a grid of facets, where facets are subplots. Here, as arguments for the facet_grid function, we need to specify which attribute should be used for the X dimension of the grid, and which attribute should be used for the Y dimension of the grid. In this case, we only have one meaningful dimension. Let’s say we want to use “variable” as the variable determining the X dimension.
Then we use the tilde character to separate the X dimension and the Y dimension. In this case, we don’t have a variable for the Y dimension of the grid, so we just use a full stop. This means there will be just one column in the grid. I forgot to change the name of the data. We want to plot ndata, not rdata. Now you can see that we have a different plot for each attribute. In the first facet, the first row in this case, we have the sepallength. The second row we have the sepalwidth. The third row we have the petallength, and in the fourth row we have the petalwidth.
We can also use columns instead of rows simply by swapping the order of the arguments here. We can use a dot on the left-hand side of the tilde and “variable” on the right-hand side. Now we have the kernel density estimates arranged vertically. Now that we have generated a nice-looking plot, we may want to save it as a PDF file. We can do that quite easily, as well. We just need to redirect the output of the plot. We do that by using the PDF function, and we specify the file name, let’s say /Users/Eibe/Documents/test.pdf. Then we simply call the plotting function again.
Now it’s actually printing the plot into the PDF file, and to redirect our plot to the window again we just call the dev.off() function. There are many other types of plots that we can generate with ggplot2. We can generate scatter plots, two-dimensional kernel density estimate plots, and many other plots. One very useful type of plot that we cannot generate with Weka’s own graphical user interfaces is a box plot. So let’s generate a box plot for the iris data for each attribute individually, using facet grids. First, we need to specify the data layer again using the ggplot function “ggplot” – let’s, say, use this ndata that I’ve already prepared.
And then, we use the aesthetics function to specify what exactly we want to plot. We want to plot the value on the y-axis in a box plot, and we want to use the class to distinguish different box plots on the x-axis. We want the color to be also based on the class. Now, we use the geom_boxplot function to generate box plots, and we use the facet_grid function again to generate the grid of plots. In this case, let’s say, use “variable” to determine the column. As you can see here, we have a really nice set of box plots. First, we have the box plot for sepallength, then for sepalwidth, then for petallength and for petalwidth.
So we have generated a fairly complex plot here. You can generate many more types of plots using ggplot2. Hopefully, this has given you a taster.
<End Transcript>

<-- 3.11 Quiz -->
Plotting data with ggplot2 
Question 1
Histograms can be generated with the R’s hist() function, but ggplot2 provides more powerful options for plotting.
In the video, Eibe showed how to plot a kernel density estimate for an attribute using the command ggplot(rdata, aes(x = petallength)) + geom_density(). A histogram can be plotted instead by replacing geom_density() with geom_histogram().
How many peaks are there in the histogram for the petallength attribute when geom_histogram is applied with default settings, where a “peak” is defined as a point that is higher than both its neighbors?
1
2
3
4
5
6
---
Correct answer(s):
6

<-- 3.11 Quiz -->
Plotting data with ggplot2 
Question 2
The granularity of the histogram can be adjusted by passing a value for the binwidth parameter to geom_histogram.
How many peaks are there when geom_histogram(binwidth = 0.4) is applied?
1
2
3
4
5
6
---
Correct answer(s):
3

<-- 3.11 Quiz -->
Plotting data with ggplot2 
Question 3
In the video, Eibe showed how to indicate the three different classes in the plot by changing aes(x = petallength) to aes(x = petallength, color = class).
Exactly the same can be done with histograms. How many peaks does the Iris.setosa histogram have when the default bin width is used?
1
2
3
4
5
6
---
Correct answer(s):
1

<-- 3.11 Quiz -->
Plotting data with ggplot2 
Question 4
Frequency polygons, an alternative to histograms, can be obtained using the geom_freqpoly function.
Replace the geom_histogram() call you have been using in the previous questions by geom_freqpoly().
How may peaks are there in the frequency polygon for Iris.virginica?
1
2
3
4
5
6
---
Correct answer(s):
4

<-- 3.11 Quiz -->
Plotting data with ggplot2 
Question 5
Histograms and frequency polygons can be plotted together!
Just append geom_histogram() to the last command line using the + operator. Give geom_histogram() the argument alpha = 0.5 to ensure that the histograms do not obscure the other plots.
How many peaks in the histogram plot exceed the corresponding frequency polygon plots? (Leave all other parameters of geom_histogram and geom_freqpoly at their default values, and ignore slight imprecisions in the plot.)
1
2
3
4
5
6
---
Correct answer(s):
2

<-- 3.11 Quiz -->
Plotting data with ggplot2 
Question 6
ggplot2 can make plots based on two attributes.
For example, two-dimensional kernel-density estimates can be plotted using the geom_density2d() function.
Generate such a plot using ggplot(rdata, aes(x = petallength, y = petalwidth)) + geom_density2d() + xlim(-0.5, 7.5) + ylim(-0.5, 3). This gives a contour plot.
How many peaks are there?
1
2
3
4
5
6
---
Correct answer(s):
2

<-- 3.11 Quiz -->
Plotting data with ggplot2 
Question 7
Make the density estimates class dependent by providing the color = class argument to the aes() function.
The plots for some of the classes overlap. How many overlapping areas are there?
1
2
3
4
5
6
---
Correct answer(s):
1
---
Feedback correct:
Iris-versicolor and Iris-virginica overlap

<-- 3.11 Quiz -->
Plotting data with ggplot2 
Question 8
Remove the color argument from the aes() function and replace geom_density2d() by stat_density2d(aes(fill=..level..), geom=”polygon”).
This gives a contour plot in which the fill color is based on the value of the density function. It also provides a nice legend. How many numeric values for the density levels are shown in the legend?
1
2
3
4
5
6
---
Correct answer(s):
4
---
Feedback correct:
The values shown are 0.2, 0.15, 0.1, and 0.05

<-- 3.11 Quiz -->
Plotting data with ggplot2 
Question 9
Append facet_grid(class ~ .) to the command line using the + operator: this gives a separate plot for each class value.
But only one of these plots is useful – because in the others, different density levels cannot be distinguished visually. Which one?
Iris-setosa
Iris-versicolor
Iris-virginica
---
Correct answer(s):
Iris-setosa

<-- 3.11 Quiz -->
Plotting data with ggplot2 
Question 10
The problem can be rectified by applying a logarithmic transformation.
Append scale_fill_gradient(trans=”log”) to the last command line using the + operator.
Which of these two classes exhibits greater density on average (brighter pixels correspond to greater density)?
Iris.versicolor
Iris.virginica
---
Correct answer(s):
Iris.versicolor

<-- 3.11 Quiz -->
Plotting data with ggplot2 
Question 11
Plots can be overlayed.
For example, add scatter plots to the density estimates by appending geom_point() to the command line using the + operator.
Some data points for one of the classes lie outside the contours of the corresponding density estimate. Which class?
Iris-setosa
Iris-versicolor
Iris-virginica
---
Correct answer(s):
Iris-setosa

<-- 3.11 Quiz -->
Plotting data with ggplot2 
Question 12
Histograms can be generalized to two dimensions using the geom_bin2d() function.
Try ggplot(rdata, aes(x = petallength, y = petalwidth)) + geom_bin2d().
How many bright blue bins are there?
0
1
2
3
4
5
---
Correct answer(s):
1

<-- 3.11 Quiz -->
Plotting data with ggplot2 
Question 13
Add a scatter plot, make the bins transparent, and change the bin width by providing a 2-element vector using the c() function: ggplot(rdata, aes(x = petallength, y = petalwidth)) + geom_bin2d(alpha = 0.7, binwidth = c(0.8,0.8)) + geom_point().
How many of the resulting bins appear to contain only two data points?
0
1
2
3
4
5
---
Correct answer(s):
0
1
---
Feedback correct:
For one rectangle there are two data points exactly on the boundary; it’s not really clear whether they are in or out.
For one rectangle there are two data points exactly on the boundary; it’s not really clear whether they are in or out.
---
Feedback correct:
For one rectangle there are two data points exactly on the boundary; it’s not really clear whether they are in or out.

<-- 3.11 Quiz -->
Plotting data with ggplot2 
Question 14
A smoothed version of the scatterplot can be used to visually investigate the relationship between two numeric attributes.
Try ggplot(rdata, aes(x = petallength, y = petalwidth)) + stat_smooth() + geom_point().
The gray area indicates 95% confidence limits. What is the smoothed value of petalwidth when petallength = 4?
1.1
1.25
1.3
1.5
1.6
---
Correct answer(s):
1.25

<-- 3.11 Quiz -->
Plotting data with ggplot2 
Question 15
Use 99% confidence limits instead by applying stat_smooth(level=0.99), and generate plots on a per-class basis by passing color=class as an argument to the aes() function.
How many areas are there where the confidence bands for more than one class overlap?
0
1
2
3
4
5
---
Correct answer(s):
2
---
Feedback correct:
One between petallength = 4.5 and 4.75, and the other, very small, around petallength = 5

<-- 3.11 Quiz -->
Plotting data with ggplot2 
Question 15
Use 99% confidence limits instead by applying stat_smooth(level=0.99), and generate plots on a per-class basis by passing color=class as an argument to the aes() function.
How many areas are there where the confidence bands for more than one class overlap?
0
1
2
3
4
5
---
Correct answer(s):

<-- 3.12 Video -->
Using R to run a classifier
Weka’s MLR classifier includes many of the learning algorithms that are available in the R environment. Choosing the MLRClassifier in the Explorer’s Classify panel gives access to 75 classification methods and 60 regression methods. Behind the scenes, Weka transfers the data into the R environment, builds the classifier or regressor there, feeds in the test data, and extracts the predictions from the R environment and returns them to Weka. In the video Eibe runs R’s rpart method, which builds a classification tree; and rferns, which generates an ensemble of ferns – a restricted form of decision tree. Detailed instructions are given in the accompanying download (these slides do not appear in the video itself).
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Today, we’re going to look a bit more at how to use R from Weka. More specifically, we’ll look at how to use the MLR library from Weka. MLR stands for Machine Learning in R. This library includes many of the learning algorithms that are available in the R environment all nicely bundled up in one package. As we’ll see, it’s quite easy to use MLR from Weka. There is a particular classifier that can be used to do this. OK, let’s have a look at how this classifier works. I have loaded the diabetes data into the Explorer so that we can process it using MLR learning algorithms. One way to use MLR is to just use the R console.
We’ve seen last time that we can, for example, plot the data in the R console, by referring to the data using rdata. This will plot the data that we have loaded into the Preprocess panel. We can also use the MLR learning algorithms from this console by typing in commands. However, that is a little bit inconvenient. Instead, we can use the MLR classifier by selecting it under the Classify panel. We select the Choose button to choose the MLR classifier. As you have seen, this has taken a while, because Weka actually needs to download and install the MLR package in R the first time we want to use it.
However, once this has happened, we don’t need to install the package again, so this will be much faster in the future. OK, now you can see here that we have an MLR package in the classifiers package. There’s an MLR classifier there, so let’s select it. The MLR classifier wraps the MLR R library for building and making predictions using various R classifiers and regression methods. Right. Just like with any other Weka classifier, we have the text box up here which contains the configuration information for the MLR classifier. Let’s just run it with default settings. You press the Start button, and, by default, the MLR classifier runs the rpart learning algorithm in R.
This builds a classification tree from the data using the CART decision tree learning method. You can see that it gets 75% accuracy in the cross-validation on the diabetes data. We get all the other performance statistics that we are used to, as well. Really, we treat the learning algorithm in R just like any other Weka learning algorithm. For this to happen, behind the scenes the MLR classifier actually has to transfer the data into the R environment, build the classifier in the R environment, and then also feed the test data to the classifier in the R environment and get the predictions back. But it all happens transparently.
Further up, we can see the tree that has been generated from this data in textual form. We also get some information on the learning algorithm that was used and the package it originally comes from. We used rpart, which is a classification algorithm, so in MLR it’s called classif.rpart. This learning algorithm comes from the “rpart” package, which is a separate package for R. The MLR package for R just bundles algorithms from a lot of other packages that are available in R in one convenient interface, which we can easily make use of using the MLR classifier. The name is given here and also some properties of this algorithm. It can deal with two classes. It can deal with multiple classes.
It can deal with missing values, numeric variables – numeric attributes, in other words – factors, which are nominal attributes. It could also, potentially, deal with ordinal attributes. It can produce probability estimates, and it can deal with instance weights. This is the rpart learning algorithm from R, but there are many other learning algorithms that are available in the MLR package, and most of them are available through the MLR classifier. We can choose the algorithm we want to use by using the RLearner property. By default, we can see here that rpart is chosen, but there are many other algorithms that we can choose from. There are many classification algorithms, and there are also many regression algorithms.
Let’s run one other classification algorithm in MLR. Let’s run random ferns. This is available as classif.rferns. Living in New Zealand, I am quite fond of ferns, and it’s intriguing to see that there is also a learning algorithm that generates random ferns. Now, you can see that when I’ve clicked this, nothing happens for a while because Weka actually has to download and install the rferns package. That has happened now, and we can use this classifier. The fern is a variant of a decision tree where all the tests at one level of the decision tree are exactly the same, so they all test the same attribute and they perform the same split of the data.
A fern is a restricted form of a decision tree. Just like the random forest classifier does for regular decision trees, it generates an ensemble of ferns. OK, let’s try this. Right. OK, so this classifier is slightly less accurate than the rpart classifier, but there may be other datasets where it outperforms rpart, because it is an ensemble classifier. You’ve seen that it runs quite quickly. It has actually generated an ensemble of 1000 ferns, and the depth was restricted to 5.So maybe we should try to decrease the depth to reduce the chance of overfitting. We can also specify parameters for the learning algorithm here in the learnerParams field of the MLR classifier.
To find out some information about the parameters that we can use, we actually need to go on the web. It’s best to go to the list of learning algorithms that are available in the MLR library first. To do that, we just search for “MLR integrated learners” and we search for the release version of MLR. There is also a development version. The first link here is the link we want. You have the integrated learners here. This has a list of all the learning algorithms that are in the MLR package, and most of them are available through the MLR classifier in Weka. We want to look for rFerns, so I search for “rFerns” on this page. There’s a link here.
This will take us to the appropriate documentation page. It has a list of all the topics that are in the manual for the rFerns package for R. rFerns is the actual learning method, so let’s click on this. Here we have some information on the usage of the method. We have arguments that can be used in R. X and Y is just the data. We can ignore that; that’s filled in by Weka by the MLR classifier. Formula, you can also ignore that, and data, yes, we can ignore that, as well. But here we can see some relevant parameters that we might want to change.
We can change the depth for example of the ferns, and we can change the number of ferns. Let’s change the depth. Let’s try to reduce it in our experiment. What we do is we type depth = 2, if we want to reduce the depth to 2. Let’s re-run the experiment. We start it again. Right. We can also specify multiple parameters. We can change the number of trees that we want to generate. By using the ferns argument, we can say how many ferns we want to include in our ensemble. To specify multiple arguments, we just separate them by a comma. So ferns = 100 will generate 100 ferns instead of 1000.This runs even more quickly now.
The accuracy has actually slightly gone up. This is most likely due to chance. We’ve seen now how we can use MLR classifier from Weka, and you can also run MLR classifier from the other user interfaces in Weka. You can run it from the Weka Experimenter. You can run it from the command line, and you can also run it from the Knowledge Flow.
<End Transcript>

<-- 3.13 Quiz -->
Learning algorithms in the MLR package 
Question 1
Multivariate adaptive regression splines (MARS) are a famous learning method for regression problems, and are implemented by MLR’s regr.earth function.
The MARS model is a linear combinations of hinge functions based on individual attributes, but may also include products of hinge functions. The “degree” parameter of regr.earth determines how many attributes may be included in each product, which controls the degree of interaction between attributes.
Run a 10-fold cross-validation of MLRClassifier with regr.earth as the learning algorithm on Weka’s cpu.arff dataset. What is the resulting correlation coefficient?
0.90
0.92
0.93
0.94
0.96
---
Correct answer(s):
0.94
---
Feedback correct:
(More precisely, 0.9364)

<-- 3.13 Quiz -->
Learning algorithms in the MLR package 
Question 2
By default, regr.earth runs with a parameter degree of 1, which means that no interactions between attributes are modeled.
Try values 2, 3, and 4. What is the largest correlation coefficient obtained?
0.90
0.92
0.93
0.94
0.96
---
Correct answer(s):
0.96
---
Feedback correct:
On my machine the result is 0.9598 for both degrees 3 and 4. However, the exact number may differ slightly depending on the operating system and Java version.

<-- 3.13 Quiz -->
Learning algorithms in the MLR package 
Question 3
MLR also provides access to a model tree learner, called regr.mob.
Compare it to the M5P model tree learner in WEKA. Run both methods, with default parameters, on the cpu dataset, using 10-fold cross-validation in the Explorer.
Which gives the greater correlation coefficient?
M5P
regr.mob
---
Correct answer(s):
M5P
---
Feedback correct:
Though not by much. M5P gives a correlation coefficient of 0.93, compared with regr.mob’s 0.92.

<-- 3.13 Quiz -->
Learning algorithms in the MLR package 
Question 4
Now try the two methods on some artificial data.
Go to WEKA’s Preprocess panel and generate data using the MexicanHat data generator for regression (with default settings).
Run a 10-fold cross-validation on this data with each of the two model tree learners. Which method achieves the best correlation coefficient?
M5P
regr.mob
---
Correct answer(s):
M5P
---
Feedback correct:
M5P gives a correlation coefficient of 0.91, compared with regr.mob’s 0.89.

<-- 3.13 Quiz -->
Learning algorithms in the MLR package 
Question 5
It is interesting to compare implementations of learning algorithms in R to those available natively in WEKA.
Use WEKA’s Experimenter to compare two implementations of multinominal logistic regression, WEKA’s Logistic and MLR’s classif.multinom, using 10-times 10-fold cross-validation, on the diabetes, glass, ionosphere and iris datasets.
One of these datasets give exactly the same estimated accuracy for the two methods. Which one?
diabetes
glass
ionosphere
iris
---
Correct answer(s):
ionosphere
---
Feedback correct:
Both methods give a Percent_correct of 87.72%.

<-- 3.13 Quiz -->
Learning algorithms in the MLR package 
Question 6
Logistic standardizes numeric attributes internally, whereas classif.multinom does not.
To eliminate this potentially confounding factor, run both implementations by wrapping them into the FilteredClassifier in conjunction with WEKA’s Standardize filter.
This makes a difference for just one of the datasets. Which one?
diabetes
glass
ionosphere
iris
---
Correct answer(s):
glass
---
Feedback correct:
The results are the same for Logistic, but the standardization operation lowers the performance of classif.multinomial slightly, from 63.71% to 62.99%.

<-- 3.13 Quiz -->
Learning algorithms in the MLR package 
Question 7
Repeat the previous experiment (without the Standardize filter), this time comparing WEKA’s RandomForest with MLR’s classif.randomForest, again using the Experimenter with default settings.
RandomForest outperforms classif.randomForest on just one of the datasets. Which one?
diabetes
glass
ionosphere
iris
---
Correct answer(s):
ionosphere
---
Feedback correct:
RandomForest gives 93.48%, and classif.randomForest gives 93.42%.

<-- 3.13 Quiz -->
Learning algorithms in the MLR package 
Question 8
To make the comparison fair, we should fix the number of randomly selected attributes used at each node of each decision tree in the forest. We need to do this because the two implementations automatically select an appropriate number using different heuristics. We should also use the same number of trees in each forest.
To this end, change the K (i.e., numFeatures) parameter in RandomForest to 2, and use mtry = 2 and ntree = 100 as parameters for classif.randomForest.
Now the situation is reversed: RandomForest is outperformed by classif.randomForest on just one of the datasets. Which one?
(There is one possible reason for remaining differences: RandomForest uses information gain to select splits in each tree whereas classif.randomForest uses the Gini index.)
diabetes
glass
ionosphere
iris
---
Correct answer(s):
iris
---
Feedback correct:
RandomForest gives 94.8%, and classif.randomForest gives 95.2%.

<-- 3.13 Quiz -->
Learning algorithms in the MLR package 
Question 9
WEKA’s metalearners can be used in conjunction with R learners (indeed, we have already used FilteredClassifier in conjunction with classif.multinom).
For example, MARS can be applied to classification problems by using regr.earth as the base learner for the ClassificationViaRegression metalearner.
In the Explorer, evaluate ClassificationViaRegression with regr.earth as the base learner (via MLRClassifier) on the ionosphere dataset, using 10-fold cross-validation.
How many correct classifications are there?
37
89
308
311
314
317
---
Correct answer(s):
314
---
Feedback incorrect:
That’s the number of incorrectly classified instances
---
Feedback incorrect:
That’s the percentage of correctly classified instances, not the number

<-- 3.13 Quiz -->
Learning algorithms in the MLR package 
Question 10
Try values 2, 3, and 4 for the degree of interactions in regr.earth (as you did in Q.2).
Which degree gives the greatest accuracy?
1
2
3
4
---
Correct answer(s):

<-- 3.13 Quiz -->
Learning algorithms in the MLR package 
Question 10
Try values 2, 3, and 4 for the degree of interactions in regr.earth (as you did in Q.2).
Which degree gives the greatest accuracy?
1
2
3
4
---
Correct answer(s):
2
---
Feedback correct:
On my machine a degree of 2 gives 317 correctly classified instances. However, the exact number may differ slightly depending on the operating system and Java version.
---
Feedback incorrect:
A degree of 1 gives 314 correctly classified instances
---
Feedback incorrect:
A degree of 3 gives 308 correctly classified instances
---
Feedback incorrect:
A degree of 4 gives 311 correctly classified instances

<-- 3.14 Video -->
Using R to preprocess data
Tools implemented in R can preprocess data before passing it on to Weka learning algorithms. The Knowledge Flow’s RScriptExecutor component executes a user-supplied R script. Data can be loaded using an ArffLoader and passed to the RScriptExecutor, which is supplied with a script. Eibe demonstrates scripts that delete an attribute, produce a scatter plot matrix, and decompose the input into statistically independent components – after which the Naive Bayes classifier is run, and evaluated using cross-validation. R includes many other useful transformation methods.
Detailed instructions are given in the accompanying download (these slides do not appear in the video itself).
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! In the last video we saw how we can use the MLR classifier in Weka to run learning algorithms implemented in R from within Weka. Before that, we saw how we can use the R console in Weka to run R commands, for example, to plot data. In today’s lesson, we’ll see how we can use the preprocessing tools implemented in R to preprocess data before we pass that data on to Weka learning algorithms. OK, let’s get started. We will use the Knowledge Flow environment in Weka to process data in R and then pass it on to a Weka learning algorithm. The Knowledge Flow environment, once the RPlugin has been installed, provides an RScriptExecutor component that executes a user-supplied R script.
So we can put this on the canvas, and then right-click on it or double-click on it to configure it. What we can see here is that we can enter an R script, or we can load a script from a file. Before we start with our script we should load some data. So we need a data source. Let’s use an ArffLoader data source. OK, maybe we just use the iris data to start off with, and then we make a dataset connection to our RScriptExecutor.
Now, the data will be loaded as an ARFF file in Java, then it will be passed to the R environment, and the data that is processed by this R script will be passed back into Weka so that, for example, we can visualize it. We can put a scatterplot matrix here, and then, once we’ve configured our R script, we can connect it up to the RScriptExecutor. Something very simple that we can do in our R script is to delete one of the attributes from the incoming data. The incoming data is referred to by the name rdata, just as is done in the R console in the Explorer, as well.
Then in brackets, we can specify which columns of this data we want to keep. Let’s say we just want to keep the first 4 attributes of the data and discard the class attribute. Now, this command will be executed and then the result will be passed into Weka as an ARFF file. We can now connect our RScriptExecutor component to the ScatterPlotMatrix component using a dataset connection. Right, let’s try this. So we run the flow, and now let’s check the plot by right-clicking on the component. As we can see, we’ve got a ScatterPlotMatrix here, which has 4 of the attributes but not the class attribute. So it all worked as intended. Let’s try something more sophisticated.
We want to preprocess the data using one of R’s many preprocessing tools. More specifically, we want to use independent component analysis to decompose the input data into statistically independent components. We want to do that using the fastICA library in R.First, we need to install this library in R. We can do that from the Knowledge Flow if we enable the R perspective. Perspectives allow us to implement additional functionality in the Knowledge Flow. There’s an R console perspective here. Let’s tick this. Now you can see up here we have additional R console, which is just the same as the R console in the Explorer. To install the fastICA package, we can just go install.packages(“fastICA”).
Now that this is installed, we can use the library in our R script. We go back to the Data mining processes perspective, and now we can change our script to make use of this fastICA library. First of all, we need to load the library into R, so we have library(fastICA) as the first statement in our script. Now, for convenience, let’s just define a variable that allows us to specify the number of components we want to extract from our data. Let’s say we want to extract as many components as there are predictor attributes in our data. So we say num = ncol, which is the function that gives us the number of columns in the data – ncol(rdata) – 1.
So this will be 4 in the case of the iris data. Now, we can call the fastICA function.
fastICA: we specify the data we want to use. We go from 1 to num – these are the columns that we want to perform our independent component analysis on. Then we say how many components we want to extract, also num. Right. Now, fastICA actually returns a list of results in R. If we check the fastICA documentation on the web, we see that there are actually several things that are returned by the fastICA function. Let’s search for “fastICA documentation R”. This page here, the rdocumentation.org site, looks helpful.
Right. If you look at the documentation for the function, you can see that it returns several values. It returns the preprocess data matrix and then several other things. What we want here is the estimated source matrix. The source consists of the estimated components, the independent components. We want to use the S value from the result. We can get that value by adding “$S” at the end of the invocation of the function. This will extract the independent components from the result. Right. We are almost done now. This will actually return the independent components as a matrix; however, to be able to pass the data back into Weka, we need to make this matrix into a data frame.
To make this into a data frame, we can just call the data frame function, data.frame, and we put the whole thing into brackets. Let’s try this out. Now we should see the independent components that have been extracted from the data. They are called X1 to X4. You can see here that the independent component analysis has produced the desired results. For example, if you look at the relationship between X1 and X4, those two components look pretty much statistically independent. We’ve run independent component analysis on the data and passed the data back into the Weka environment. Let’s run a learning algorithm on this data.
As you know, the Naive Bayes learning algorithm assumes that the attributes are conditionally independent given the class attribute. It’s a plausible hypothesis to assume that data preprocessed using ICA is easier to classify using Naive Bayes. In order to run a supervised learning algorithm on the data, we need to attach the original class labels to the data again. We can do that quite easily using the cbind function, the column bind function in R. We go “cbind” and then the two sets of columns that we want to bind together. Here we need to assign this data to a variable. Let’s call this variable d, so that we can refer to it in the cbind function.
We want to bind the columns in d and the “class” column in the rdata data frame. The address of the column is given using square brackets again, and the index of this column is num + 1. That’s the last column in our rdata, so that’s the class column in the iris data. Now we will have labeled data. Let’s try this. Now we can see that the class attribute has been added. We have the data decomposed into independent components using ICA and then labeled again with the original class labels. Now we can run a learning algorithm on this data, for example Naive Bayes.
What we need to do is go into the evaluation package and choose the class assigner to assign the class attribute to our data and make a dataset connection to this ClassAssigner. By default, it just uses the last attribute, so this is fine. We pass the data to the CrossValidationFoldMaker, and from there we pass the data to Naive Bayes – both the training set and the test set. After we have added the classifier, we need to evaluate the classifier. So we take a ClassifierPerformanceEvaluator, and we use a batch classifier connection to connect Naive Bayes to it.Finally, we want to see the output of the evaluation process so we use a TextViewer, and we use a text connection from the ClassifierPerformanceEvaluator.
So let’s run this flow. OK, it’s finished, and we can get the cross-validated accuracy
now in this text viewer: 98% accuracy. This is quite a high accuracy on the iris data, so it looks like independent component analysis has helped to improve performance. Note that, strictly speaking, we have performed semi-supervised learning in this experiment, because we used an unsupervised feature extraction method on the whole dataset before we applied cross-validation on the dataset. Right. This was to show you a bit of how to use R from the Knowledge Flow. We’ve covered all the aspects of how to use R in Weka now, so that’s it for this topic. See you later!
<End Transcript>

<-- 3.15 Quiz -->
Using R to preprocess data 
Question 1
Replace ICA by other preprocessing tools as follows, and evaluate using Naive Bayes with 10-fold cross-validation. Remember to install the relevant package in each case.
The cmdscale tool does classic multidimensional scaling. Use it to replace fastICA with two target dimensions (k = 2):
data.frame(cmdscale(dist(rdata[1:num]), k = 2))
How many correct classifications does Naive Bayes achieve?
[Note: cmdscale is built into R version 3.4.3, but you will need to install it as a separate package in prior versions of R.]
132
135
138
142
463
498
---
Correct answer(s):
135

<-- 3.15 Quiz -->
Using R to preprocess data 
Question 2
For kernel PCA, use kpca from the kernlab package, with an RBF kernel (rbfdot) and sigma = 0.01:
data.frame(rotated(kpca(~ ., data = rdata[1:num], kernel = "rbfdot", 
        kpar = list(sigma=0.01), features = 2)))
(Don’t forget to install the kernlab package, and include it in the library statement of the RScriptExecutor script.)
How many correct classifications does Naive Bayes achieve?
132
135
138
142
463
498
---
Correct answer(s):
132

<-- 3.15 Quiz -->
Using R to preprocess data 
Question 3
For principal curves, use prcurve from the analogue package:
data.frame(prcurve(rdata[1:num])$s)
How many correct classifications does Naive Bayes achieve?
132
135
138
142
463
498
---
Correct answer(s):
138

<-- 3.15 Quiz -->
Using R to preprocess data 
Question 4
LLE is a non-linear algorithm for mapping high-dimensional data into a lower dimensional space.
Use lle from the lle package, with the k parameter set to 3:
data.frame(lle(rdata[1:num], m = 2, k = 3)$Y)
How many correct classifications does Naive Bayes achieve?
132
135
138
142
463
498
---
Correct answer(s):
142

<-- 3.15 Quiz -->
Using R to preprocess data 
Question 5
R can generate data and pass it to a WEKA learning algorithm.
Data sources for regression and classification problems are implemented in R’s mlbench package, which you should install. Remove the ArffLoader from your knowledge flow, so that RScriptExecutor is the first component.
The following R script generates a 2-class dataset with 1 Gaussian per class; sets the random number generator seed to 1; and converts the last attribute to nominal:
library(mlbench)
set.seed(1)
d = data.frame(mlbench.2dnormals(500,2))
d
How many correct classifications does Naive Bayes achieve on this data (using 10-fold cross-validation)?
132
135
138
142
463
498
---
Correct answer(s):
463

<-- 3.15 Quiz -->
Using R to preprocess data 
Question 6
Use mlbench.smiley(n = 500, sd1 = 0.1, sd2 = 0.05) instead of mlbench.2dnormals(500,2).
How many correct classifications does Naive Bayes achieve?
132
135
138
142
463
498
---
Correct answer(s):

<-- 3.15 Quiz -->
Using R to preprocess data 
Question 6
Use mlbench.smiley(n = 500, sd1 = 0.1, sd2 = 0.05) instead of mlbench.2dnormals(500,2).
How many correct classifications does Naive Bayes achieve?
132
135
138
142
463
498
---
Correct answer(s):
498

<-- 3.16 Video -->
Analyzing functional MRI Neuroimaging data
Pamela Douglas from UCLA introduces the problem of classifying functional MRI data. An FMRI scan records signals over time from 100,000 voxels covering the brain region, which creates a huge 4-dimensional dataset. The ADHD2000 machine learning competition is to predict a subject as either “Typically developing (TD)” or “Attention deficit hyperactivity disorder (ADHD)” using data from 1000 subjects that includes both demographic and structural neuroimaging features. Pamela’s team calculated 100,000 functional neuroimaging attributes from the raw data, and was placed 3rd using a voted perceptron learning algorithm. Ironically, the winning team ignored the neuroimaging features and used demographic data only! The video also introduces Haxby’s classic FMRI dataset, collected while subjects viewed images from 8 object categories. You will use these in the Quiz that follows.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi, I’m Pamela Douglas from the UCLA Semel Institute. We’re going to talk about a very interesting application of using Weka for classifying functional MRI data. Classifying these data can be very challenging for a number of reasons. First of all, these data are very high dimensional. A structural MRI scan can consist of approximately 100,000 voxels, and an FMRI scan records signals from these voxels over time, resulting in 4-dimensional data. The number of possible features and attributes that can be derived from these data are very large, and one of the recent events that highlights why this can be problematic was the ADHD200 Global Machine Learning Competition.
The goal of this competition was to predict a subject’s diagnosis as either “Typically developing (TD)” or ADHD using a combination of demographic and structural and functional neuroimaging features. A number of sites from around the world collaborated to provide data for this competition. This resulted in approximately 800 subjects’ worth of data in the training set, as well as 200 subjects’ worth of data in the test set, where the diagnosis was unknown to the participants in the competition. My team participated in this competition, and our first goal was to figure out how to derive meaningful information from structural MRI data. The first thing we did was calculate Freesurfer metrics for automated brain parcellation.
This resulted in 9 different attributes, like brain volume, from 68 different cortical regions, as well as 3 different measures from each of 45 subcortical and non-cortical brain regions. Collectively, this resulted in more than 700 structural brain attributes using just the SMRI data. Our next step was to determine how to extract features from the resting state functional MRI data. The first thing we did was calculate resting state functional connectivity matrices, or pairwise time series correlations between different brain regions. We then calculated the total number of independent components used that were required to describe 99% of the data variance. We also calculated power spectra, regional homogeneity, and a number of different graph-theoretic metrics, like functional modular organization.
Overall, this resulted in more than 100,000 functional neuroimaging attributes. My team, as it turns out, placed third in this competition using a voted perceptron, as implemented in Weka, but the overall results of this competition were very unsatisfying. As it turned out, the winning team used only demographic features and a very small number of attributes overall. Classification using these features alone outperformed all the other teams that used demographic features in combination with MRI data. This result really highlights the importance of using feature selection, either as a separate step or as part of a regularization scheme, since the inclusion of irrelevant and redundant features can vastly degrade the performance of a classifier.
Another reason Weka can be useful for classifying MRI data is that there are a number of algorithms that are readily available for testing that have already been vetted by the machine learning community. In a previous lesson, we learned about the “No Free Lunch Theorem”. As a brief reminder, each classifier has its own inductive bias, and there is no way to know a priori which classifier will perform best on a given dataset. Therefore, it’s often a good idea to test out a few different classifiers and use model selection to determine your best option. In the exercise that follows, you’ll be able to test out a few different classifiers using the classic Haxby et al. dataset.
In this 2001 study, functional MRI data was collected while subjects viewed images from eight different object categories. You’ll also get to test out a few different methods for feature selection, as well as parameter tuning using nested cross-validation. In summary, functional MRI data is high dimensional, so feature selection and regularization are highly recommended. Weka can be very useful for classifying these data, since Weka has the capability of handling large datasets and combining across multiple feature categories, like nominal and numeric data, as well as handling missing data. Testing out a variety of models and classifiers can be very helpful.
Lastly, the Weka group has kindly now added a “brain button” to their software, so you can now load in MRI data files in NIFTI format directly into Weka for classification, without needing to convert it to the attribute relation file format. I hope that you’ll enjoy testing out Weka for classification on brain imaging data as much as I have. Thanks so much!
<End Transcript>

<-- 3.17 Quiz -->
Playing with fMRI data 
Question 1
Download NIFTI_Files.zip, uncompress it, and examine it in your file browser.
This is the data for one of Haxby’s 12 subjects, in a format developed by the Neuroimaging Informatics Technology Initiative (NIFTI). It contains one folder for each object type.
How many object types are there?
2
4
6
8
10
---
Correct answer(s):
8

<-- 3.17 Quiz -->
Playing with fMRI data 
Question 2
Which of these is not an object type?
Select all the answers you think are correct.
bottle
cup
dog
house
scissors
scrambledpix
---
Correct answer(s):
cup
dog

<-- 3.17 Quiz -->
Playing with fMRI data 
Question 3
In Haxby’s experiment, subjects were shown a single example of each object type for half a second, one after another, with a brief rest period between each; and the whole procedure was repeated 12 times.
The data that you have is for a single subject. It contains 11 (not 12) examples of most object types – e.g., scissors. However, it contains only 10 examples of three of them. Which are they? You can tell by looking at the number of files in each object-type folder.
Select all the answers you think are correct.
bottle
cat
chair
face
house
scissors
scrambledpix
shoe
---
Correct answer(s):
chair
face
scrambledpix

<-- 3.17 Quiz -->
Playing with fMRI data 
Question 4
Load the brain images in NIFTI_files into the Weka explorer.
First install the NIFTI loader using Weka’s package manager.
Then, in the Preprocess panel, under Open file, navigate to your NIFTI_Files folder. Click Open. You will see the message “Cannot determine file loader automatically, please choose one.” Click OK; select NIfTIDirectoryLoader after clicking on the Choose button, and click OK (note: you do not need to enter anything into the directory field).
Everything should work OK on Windows, but if you are using Linux or Mac and have installed the RPlugin package, your Java system may crash when loading the NIFTI file due to an issue with the stack space. To solve the problem, either temporarily disable the RPlugin package using Weka’s package manager, or increase the stack space, say to 10 Mb. You can do this by setting the environment variable _JAVA_OPTIONS that most Java virtual machines support (don’t forget the underscore at the start) to –Xss10m and then re-start Weka.
Loading the data may take some time (1 min on my computer).
How many instances are there in this dataset?
8
12
85
96
224
226
---
Correct answer(s):
85
---
Feedback correct:
The original experiment, in which each subject was shown 12 instances of 8 object types, would have generated 96 instances for one subject. The NIFTI_Files dataset contains 85 instances, or 90% of this data. The remaining 10% was set aside for validation.

<-- 3.17 Quiz -->
Playing with fMRI data 
Question 5
The class attribute gives the object type, and the remaining attributes represent fMRI activity in different voxels in the ventral-temporal region of the subject’s cortex.
How many of these attributes are there?
88546
105620
127900
142264
163840
163841
---
Correct answer(s):
163840
---
Feedback correct:
Although this is an enormous number, it represents just a small fraction of the original recorded data, because all the voxels outside the ventral-temporal region have been removed
---
Feedback incorrect:
That’s the number of attributes including the class attribute

<-- 3.17 Quiz -->
Playing with fMRI data 
Question 6
In fact, most of the attribute values are uninformative because they have zero values for all instances.
Delete these using Weka’s removeUseless filter, which removes attributes that have constant values. How many voxel attributes are there now?
8
577
578
586
---
Correct answer(s):
577

<-- 3.17 Quiz -->
Playing with fMRI data 
Question 7
Now open the dataset Haxby_Subj1_Training.arff.
This is essentially the same as the dataset you just created, but we recommend using it instead because datasets may differ from one operating system to another due to different ways of ordering files in folders.
The dataset has 8 classes. Assuming they are approximately equally populated, what accuracy would you expect from ZeroR?
5%
12.5%
25%
50%
---
Correct answer(s):
12.5%
---
Feedback correct:
With 8 classes, approximately equally populated, ZeroR should be correct about 1/8 of the time.

<-- 3.17 Quiz -->
Playing with fMRI data 
Question 8
Evaluated with 10-fold cross-validation, what accuracy does ZeroR actually give?
3.4%
11.8%
12.9%
28.2%
---
Correct answer(s):
11.8%
---
Feedback incorrect:
That’s the accuracy using Percentage split
---
Feedback incorrect:
That’s the accuracy when evaluated on the training set

<-- 3.17 Quiz -->
Playing with fMRI data 
Question 9
What accuracy does OneR give?
10.3%
11.8%
28.2%
41.2%
---
Correct answer(s):
28.2%
---
Feedback incorrect:
That’s the accuracy using Percentage split
---
Feedback incorrect:
That’s the accuracy when evaluated on the training set

<-- 3.17 Quiz -->
Playing with fMRI data 
Question 10
With 577 attributes, there is great potential for overfitting because the number of attributes vastly overwhelms the number of instances per class (10 or 11).
Evaluated on the training set, what accuracy does Naive Bayes give?
9.4%
31.0%
65.9%
92.9%
---
Correct answer(s):
65.9%
---
Feedback incorrect:
That’s the accuracy when evaluated using 10-fold cross-validation
---
Feedback incorrect:
That’s the accuracy using Percentage split

<-- 3.17 Quiz -->
Playing with fMRI data 
Question 11
Sounds too good to be true! And it is. Evaluated with 10-fold cross-validation, what accuracy does Naive Bayes give?
9.4%
31.0%
65.9%
92.9%
---
Correct answer(s):
9.4%
---
Feedback incorrect:
That’s the accuracy using Percentage split

<-- 3.17 Quiz -->
Playing with fMRI data 
Question 12
Repeat with J48. What are its training set and cross-validation accuracies?
Select all the answers you think are correct.
Training set accuracy: 9.4%
Training set accuracy: 30.6%
Training set accuracy: 65.9%
Training set accuracy: 92.9%
Cross-validation accuracy: 9.4%
Cross-validation accuracy: 30.6%
Cross-validation accuracy: 65.9%
Cross-validation accuracy: 92.9%
---
Correct answer(s):
Training set accuracy: 92.9%
Cross-validation accuracy: 30.6%

<-- 3.17 Quiz -->
Playing with fMRI data 
Question 13
This performance is extremely disappointing. Naive Bayes is even worse than ZeroR! – and J48 is not much better than OneR.
Support vector machines often excel in situations with excessive numbers of numeric attributes. What accuracy does SMO give, evaluated with 10-fold cross-validation?
9.4%
30.6%
65.9%
92.9%
---
Correct answer(s):
65.9%

<-- 3.17 Quiz -->
Playing with fMRI data 
Question 14
It’s hard to improve on this by optimizing parameters or choosing some filtering operation. But it’s easy to deceive yourself.
In the Preprocess panel, filter the dataset using PartitionMembership, a supervised attribute filter. (If you want to know what it does, consult the More button.)
How many attributes are there in the filtered dataset (excluding the class)?
8
35
577
578
---
Correct answer(s):
35

<-- 3.17 Quiz -->
Playing with fMRI data 
Question 15
What is SMO’s cross-validated accuracy on the filtered dataset?
9.4%
29.4%
65.9%
92.9%
---
Correct answer(s):
92.9%
---
Feedback correct:
Yay! (???)

<-- 3.17 Quiz -->
Playing with fMRI data 
Question 16
Sounds too good to be true! And it is.
PartitionMembership is a supervised filter and therefore has access to the class information, which risks tainting the new attributes with information derived from the class. The correct way to evaluate any supervised filter is to use the FilteredClassifier instead of pre-filtering the dataset in the Preprocess panel.
Restore the original dataset by undoing the filtering you just performed, and configure the FilteredClassifier to use the PartitionMembership filter and the SMO classifier.
What is the cross-validated accuracy now?
9.4%
29.4%
65.9%
92.9%
---
Correct answer(s):

<-- 3.17 Quiz -->
Playing with fMRI data 
Question 16
Sounds too good to be true! And it is.
PartitionMembership is a supervised filter and therefore has access to the class information, which risks tainting the new attributes with information derived from the class. The correct way to evaluate any supervised filter is to use the FilteredClassifier instead of pre-filtering the dataset in the Preprocess panel.
Restore the original dataset by undoing the filtering you just performed, and configure the FilteredClassifier to use the PartitionMembership filter and the SMO classifier.
What is the cross-validated accuracy now?
9.4%
29.4%
65.9%
92.9%
---
Correct answer(s):
29.4%
---
Feedback correct:
Bummer!

<-- 3.18 Article -->
Playing with fMRI data 
Correctly used, the PartitionMembership filter doesn’t help at all. The best we have been able to do with this dataset is 65.9% accuracy, with SMO.
There’s probably not much more you can do with Weka alone. To do better, you might have to go back to the data, and consult a specialist in MRI for brain studies. For example, we noted above that Haxby repeated the procedure 12 times for each subject. The NIFTI_Files dataset amalgamates these 12 runs: perhaps it would be better to treat each one separately? Also, Haxby had 12 subjects and we have only used the data for one of them: perhaps they should be combined – but how? Or maybe we should be looking at a different region of the brain – different voxels.
One important message of the Data Mining with Weka courses is that running experiments with Weka is usually only a small part of any actual data mining application.

<-- 3.19 Discussion -->
Reflect on this week’s Big Questions
This week’s first Big Question is, “What’s in Weka’s LibSVM and LibLINEAR packages?”
We promised that by the end you’d be able to explain what they do, why you would use them, and how to use them.
And the second is, “How do you access R from Weka?”
We promised that by the end you’d be able to explain what R is, why it’s important, and what you can do with it. And, of course, how to access R from Weka.
The first question is easy. LibSVM and LibLINEAR are powerful implementations of support vector machines. LibSVM is very flexible; LibLINEAR is very fast. And now you can use them. But more importantly, you also learned some useful things about machine learning generally: the power of non-linear kernels, 1-class classification, and the L1 norm.
As for the second question, now you know what R is, and how to create and use Weka’s R Console. It’s a powerful statistical programming language, and we only looked at a tiny subset of what it can do: impressive facilities for plotting data through the ggplot2 package for R; how to use R’s numerous regression and classification schemes just as you would any other Weka classifier through the MLR classifier; and how to access R’s comprehensive data preprocessing facilities from Weka’s Knowledge Flow interface through the RScriptExecutor component.

<-- 3.20 Article -->
Index
(A full index to the course appears at the end of Week 1.)
      Topic
      Step
      Datasets
      cpu
      3.13
      diabetes
      3.9, 3.12, 3.13
      Functional MRI data
      3.16, 3.17
      glass
      3.13
      Haxby_Subj1_Training
      3.17
      ionosphere
      3.13
      iris
      3.4, 3.5, 3.10, 3.11, 3.13, 3.14, 3.15
      NIFTI_Files
      3.17
      Structural MRI data
      3.16
      Classifiers
      J48
      3.17
      LibLINEAR
      3.2
      LibSVM
      3.2, 3.4, 3.5
      Logistic
      3.13
      M5P
      3.13
      MLRClassifier
      3.12, 3.13
      NaiveBayes
      3.4, 3.14, 3.15, 3.17
      OneR
      3.17
      RandomForest
      3.13
      SMO
      3.2, 3.17
      SMOreg
      3.2
      ZeroR
      3.17
      R classifiers
      multinom
      3.13
      randomForest
      3.13
      regr.earth
      3.13
      regr.mob
      3.13
      rFerns
      3.12
      rpart
      3.12
      Metalearners
      ClassificationViaRegression
      3.13
      FilteredClassifier
      3.13, 3.17
      GridSearch
      3.2
      OneClassClassifier
      3.4, 3.5
      Filters
      Add
      3.4
      PartitionMembership
      3.17
      RemoveUseless
      3.17
      ReplaceMissingWithUserConstant
      3.4
      Standardize
      3.13
      Packages
      gridSearch
      3.2
      LibLINEAR
      3.1, 3.2
      LibSVM
      3.1, 3.2
      niftiLoader
      3.17
      oneClassClassifier
      3.4
      RPlugin
      3.7
      R libraries
      analogue
      3.14
      fastICA
      3.14
      ggplot2
      3.10, 3.11
      kernlab
      3.15
      lle
      3.14
      mlbench
      3.12, 3.13
      MLR
      3.12, 3.13
      Generators
      LED24 (Weka)
      3.2
      MexicanHat
      3.13
      KnowledgeFlow components
      ArffLoader
      3.14, 3.15
      ClassAssigner
      3.14, 3.15
      ClassifierPerformanceEvaluator
      3.14, 3.15
      CrossValidationFoldMaker
      3.14, 3.15
      RScriptExecutor
      3.14, 3.15
      ScatterPlotMatrix
      3.14
      TextViewer
      3.14, 3.15
      R commands
      install.packages
      3.7
      boxplot, cor, cov, hist, install_packages, IQR, kurtosis, library, quantile, sapply, sd, skewness, summary, table
      3.9
      aes, dev.off, facet_grid, geom_density, ggplot, install_packages, library, melt, PDF, xlim
      3.10
      aes. C, facet_grid, geom_bin2d, geom_density, geom_density2d, geom_freqpoly, geom_histogram, geom_point, ggplot, install_packages, library, scale_fill_gradient, stat_density2d, stat_smooth, xlim, ylim
      3.11
      cbind, data_frame, fastICA, library, ncol
      3.14
      chind, cmdscale, install_packages, data.frame, fastICA, library, lle, mlbench.2dnormals, ncol, prcurve
      3.15
      Plus
      Attention deficit hyperactivity disorder (ADHD)
      3.16
      Environment variable
      3.7
      One-class classification
      3.3, 3.4, 3.5
      R (statistical language)
      3.6–3.15
      Radial basis function (RBF) kernel
      3.2

<-- 4.0 Todo -->
Distributed processing 
Can you distribute Weka jobs over several machines?
This week's Big Question!
4.1
Can you distribute Weka jobs over several machines?
article
What is Distributed Weka?
Distributed Weka is a plugin that runs Weka on a cluster of machines. It uses the “map-reduce” framework, and operates with both Spark (used here) and Hadoop.
4.2
What is distributed Weka?
video (06:04)
4.3
The "map-reduce" framework
discussion
4.4
Learning about Spark and Hadoop 
quiz
Distributing Knowledge flows
The main way to interact with Distributed Weka is through the Knowledge Flow environment. Processing components are chained together so that one will not execute until the previous one has completed. 
4.5
Installing with Apache Spark
video (12:14)
4.6
ARFF headers for Spark processing 
quiz
Using Naive Bayes and JRip 
We show an example knowledge flow to train and save two classifiers. Naive Bayes yields a single Naive Bayes model. JRip yields multiple JRip models that are combined using a Vote ensemble learner.
4.7
Using Naive Bayes and JRip
video (12:05)
4.8
Training classifiers with Spark 
quiz
Map tasks and Reduce tasks
Map tasks produce models and a Reduce task aggregates them. For Naïve Bayes this involves adding together the statistics. Other model types (trees, rules) use a voted ensemble. Cross-validation uses a Reduce task for each fold.
4.9
Map tasks and Reduce tasks
video (11:36)
4.10
Cross-validating classifiers with Spark 
quiz
4.11
Random Forest performance
article
Miscellaneous Distributed Weka capabilities 
We show how to compute a correlation matrix in Distributed Weka and use it as input to Principal Component Analysis. We also show a parallel implementation of k-means clustering.
4.12
Miscellaneous Distributed Weka capabilities
video (08:38)
4.13
Playing with template flows
quiz
Image classification
With appropriate features, machine learning can be used to classify images. Image features are statistics that describe an image, and can be extracted using a Weka package.
4.14
Image classification
video (07:32)
4.15
Processing images with different feature sets 
quiz
4.16
Reflect on this week’s Big Question
discussion
4.17
Index
article

<-- 4.1 Article -->
Can you distribute Weka jobs over several machines?
Of course, the answer is “yes”!
The question is, how? There are several ways. For example, the Experimenter contains its own mechanism for distributing an experiment over many machines. (To do it, you need to change the Experiment Configuration Mode at the top left of the Setup panel from Simple to Advanced.)
But this week we look at a more general framework for distributing Weka jobs over several machines. You will learn about the design goals for distributed Weka, and the “map-reduce” idea for breaking computations down for parallel implementation.
By the end of the week you’ll be able to install Distributed Weka and use it from the Knowledge Flow interface. You’ll be running it on a single desktop machine, of course, and each of the individual cores in the CPU is treated as a separate processing node.

<-- 4.2 Video -->
What is distributed Weka?
Mark Hall from Pentaho introduces a plugin that runs Weka on a cluster of machines. It uses the “map-reduce” framework, and operates with both Spark and Hadoop. It comprises two Weka packages, distributedWekaBase, which provides general map-reduce tasks for machine learning that are not tied to any particular map-reduce implementation, and distributedWekaSpark, a wrapper for the base tasks that operates on the Spark platform. (There are also packages for Hadoop.) The aim is to support all Weka’s classification and regression algorithms without reimplementing them, generating output just like that produced by standard Weka. Clustering, however, had to be rewritten specifically for the distributed framework.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello! My name is Mark Hall. I’m a software architect and data mining consultant with the Pentaho Corporation. I live here in New Zealand, not very far away from where the Weka software was originally developed. This lesson is about using Weka in a distributed processing framework, such as Spark or Hadoop. Distributed Weka is a plugin for Weka 3.7 that allows Weka algorithms to run on a cluster of machines. You would use this when your dataset is too large to load into main RAM on your desktop, or you’re perhaps applying an algorithm that would just take too long to run on a single machine, you covered data stream mining.
You saw sequential online algorithms that can be used to handle large datasets in the Moa framework, and also inside of Weka using Moa. Distributed Weka works with distributed processing frameworks that use something called map-reduce. So this is a little bit different. It’s more suited to large, offline, batch-based processing scenarios. Essentially, your data is divided up over the nodes in the processing cluster – the machines in a processing cluster – and is conquered. Each piece is conquered independently of the other pieces. More on map-reduce shortly. The Distributed Weka plugin is actually made up of two packages. First, there is something called distributedWekaBase.
This is a package that provides general map-reduce-style tasks for machine learning that are not tied to any particular map-reduce framework implementation. We’ll discuss map-reduce in just a second. It includes tasks for training classifiers and clusterers, and computing summary statistics and correlations from the data. A second package is needed in order to apply the base package – or the algorithms in the base package – within a particular implementation of the map-reduce programming model. So in this lesson, we’re going to be looking at an implementation for the Spark distributed processing environment. So we will need to install something called distributedWekaSpark, as well. This is a wrapper for the base tasks that works on the Spark platform.
There is also a package – or several, actually – that work with Hadoop, depending on which version or flavor of Hadoop you have installed. Now, let’s return to map-reduce. Map-reduce is the main processing model used by distributed frameworks such as Spark and Hadoop.
Map-reduce programs involve two phases: a map phase followed by a reduce phase. To start with, we have a dataset, probably a large dataset. This dataset is divided up into disjoint subsets. The framework takes care of doing this for us. It then feeds a split of the data, a subset of the data, into a map task. Now, map tasks do their processing independently of all other map tasks; they are not aware of any of the other data splits or what the other tasks that may be running in parallel are doing. So the kind of operations that map tasks do include sorting the data, perhaps, filtering it in some way, or computing some kind of partial result.
The output of map tasks are these partial results associated with a distinct key value. Now, the key values allow the framework to group together related intermediate results and pass them on to reduce tasks. The reduce tasks’ job is to take all of the values associated with one distinct key and aggregate them in some fashion. So they may count or add or do some averaging, or some kind of aggregation which produces a final result. Now, the job of the map-reduce framework itself is to provide all of this orchestration. So as I said, they handle splitting up the data for us; they handle invoking and initializing the map and reduce tasks; they provide redundancy and fault-tolerance, as well.
So if there is some failure out on the cluster which causes map tasks to abort processing before they finish, or were a reduce task to fail, the framework will take care of ensuring that there are additional map and reduce tasks that can be started up to take care of and complete the processing. OK. The design goals of Distributed Weka were to provide a similar experience to that of using standalone desktop Weka. It enables you to use any classification or regression learner in Weka and also has some support for clustering, as well. It also generates output, including evaluation output that looks just like that produced by standard desktop Weka. The models that are output from Distributed Weka are normal Weka models.
That means they can be saved to your file system, loaded into desktop Weka at a later stage, used for making predictions – just like any other Weka model. One thing that wasn’t a goal of the package – initially, at least – was to provide distributed implementations of every algorithm in Weka. One exception to this is k-means clustering, which was written specifically to work within a framework such as Spark and Hadoop. So we’ll see exactly how Weka handles distributing different types of models in a later lesson. That’s pretty much the end of our first lesson on Distributed Weka. We learned what Distributed Weka is.
We learned when you would want to use it, under what conditions you would want to use it. We’ve learned what map-reduce it, and we’ve taken a look at the basic design goals of Distributed Weka.
<End Transcript>

<-- 4.3 Discussion -->
The "map-reduce" framework
Map-reduce is a general framework for processing big data sets using a parallel, distributed algorithm on a cluster of computers that Mark introduced in the lesson video.
The map-reduce framework has been widely applied to all sorts of tasks, not just distributed machine learning. In order to broaden your understanding of the concept, I invite you to dream up (or look up) applications of the framework to other computational problems, and briefly explain to your colleagues what the map tasks and the reduce tasks have to do.

<-- 4.4 Quiz -->
Learning about Spark and Hadoop 
Question 1
Where did Spark and Hadoop originate?
Select all the answers you think are correct.
Spark: Cambridge, UK
Spark: Google
Spark: MIT
Spark: University of California, Berkeley
Spark: Yahoo
Hadoop: Cambridge, UK
Hadoop: Google
Hadoop: MIT
Hadoop: University of California, Berkeley
Hadoop: Yahoo
---
Correct answer(s):
Spark: University of California, Berkeley
Hadoop: Yahoo

<-- 4.4 Quiz -->
Learning about Spark and Hadoop 
Question 2
The development of Hadoop was inspired by …
Yahoo MPPF and Datashard
The lack of open source distributed processing frameworks
Google MapReduce and Bigtable
---
Correct answer(s):
The lack of open source distributed processing frameworks

<-- 4.4 Quiz -->
Learning about Spark and Hadoop 
Question 3
What is one major advantage of the Spark framework over Hadoop?
(Hint: search the internet for comparisons between Spark and Hadoop.)
Spark was designed specifically for machine learning, whereas Hadoop was not.
Spark has a far higher coefficient of coolness than Hadoop.
Spark has the advantage of being able to run on embedded low power devices.
Spark keeps data in memory whereas Hadoop does not.
---
Correct answer(s):
Spark keeps data in memory whereas Hadoop does not.
---
Feedback correct:
Spark’s Resilient Distributed Dataset (RDD) concept keeps data in memory and allows for faster computation, especially for iterative tasks.

<-- 4.4 Quiz -->
Learning about Spark and Hadoop 
Question 3
What is one major advantage of the Spark framework over Hadoop?
(Hint: search the internet for comparisons between Spark and Hadoop.)
Spark was designed specifically for machine learning, whereas Hadoop was not.
Spark has a far higher coefficient of coolness than Hadoop.
Spark has the advantage of being able to run on embedded low power devices.
Spark keeps data in memory whereas Hadoop does not.
---
Correct answer(s):

<-- 4.5 Video -->
Installing with Apache Spark
Having installed Distributed Weka, you can interact with it in the KnowledgeFlow environment. New components such as ArffHeaderSparkJob,  WekaClassifierSparkJob, and WekaClassifierEvaluationSparkJob become available. In addition, example knowledge flows are provided as templates that operate “out of the box” using all the CPU’s cores as processing nodes – without having to install and configure a Spark cluster. Distributed Weka operates on header-less CSV files, because it splits data into blocks to enable distributed storage of large datasets and allow data-local processing, and it would be inconvenient to replicate the ARFF header in each block. Instead, the ArffHeaderSparkJob creates a separate header that contains a great deal of information that would otherwise have to be recomputed by each processing node.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
In this lesson we’re going to install Distributed Weka and start to use some of the components that come with it. OK, here we are in Weka’s Package. What we’re going to do here is scroll down a little bit in the package list, and we’re going to install Distributed Weka for Spark. And here it is, just down here. OK. So if I click install with this one selected, it tells me that I’m going to install the
following package: distributedWekaSpark version 1.0.2. We click “Yes”. We click “OK”. And then it tells me that, in order to install this, we need also to install distributedWekaBase 1.0.12. At this stage, I’ll click “No”, because I already have this installed, and we won’t show it installing at the moment, because the download is fairly large for distributeWekaSpark, and it’ll take a little while. And I already have it installed. OK. Once you’ve installed Distributed Weka, you need to make sure that you restart Weka, so that the packages – or the newly installed packages – are loaded correctly. The main way to interact with Distributed Weka is through the Knowledge Flow environment.
This allows us to chain together processing components in such a fashion that a given component will not execute until the previous one has completed executing. It’s also possible to use Distributed Weka from the command line, but the graphical user interface provided by the Knowledge Flow is a really convenient and easy way to edit the sometimes many parameters that are involved in setting up a Distributed Weka job. Let’s verify that our installation of Distributed Weka has proceeded correctly. All right, so in the Weka Knowledge Flow environment, you can see that there is on the left-hand side in the Design palette, a new folder called “Spark”.
If we open this up, we should find that there are a bunch of new components available to us. In particular, we have something called an ArffHeaderSparkJob, we have a WekaClassifierSparkJob, a WekaClassifierEvaluationSparkJob, and several others as well, that we’ll discuss shortly.The distributedWekaSpark package also comes with a bunch of example template flows. If we look in the templates folder, which is accessible from the templates button up here in the tool bar, we can see a bunch of entries that are prefixed with the word “Spark”. These are all example flows that we can execute right out of the box. They don’t require a Spark cluster to be installed and configured.
Spark has a very convenient local mode of operation, which allows you to use all of the cores in your CPU as processing nodes, if you like. So we can execute these particular example flows straight away without any further configuration. They are ready to go. Before we start running Distributed Weka examples, I need to introduce the dataset that we’re going to be looking at. We’re going to take a look at the hypothyroid data. This is a benchmark dataset from the UCI Machine Learning Repository. The goal in this data is to predict the type of thyroid disease a patient has using input variables such as demographic information about the patient, and various medical information as well.
In this dataset, there are 3,772 instances described by 30 attributes. A version of this data, in CSV format without a header row, can be found in the distributedWekaSpark package that you installed just before. If you browse to your home directory and look in wekafiles/packages/distributedWekaSpark/sample_data directory, you’ll find it there. The data in ARFF format is also included with the Weka 3.7.13 distribution in the data folder. So you can also load it up into the Explorer and take a look in there. Why don’t we do that now? Here we are in the Weka Explorer. Let’s open the hypothyroid data.
If you browse to the Weka installation directory, Program Files here, Weka 3.7, and in the data directory, we can see the hypothyroid data. Let’s open that up. As I mentioned before, there are 3,772 instances in this dataset, and we can see the attributes here. We have the age and sex of the patient, and we have a bunch of attributes related to various medical information. Down at the bottom is the class attribute. You can see there are four different class values here. By far the largest class in the data is that of negatives. So these are patients that don’t have hypothyroid disease. Then we have 194 cases of compensated_hypothyroid, 95 cases of primary_hypothyroid, and only 2 cases of secondary_hypothyroid.All right.
That’s the characteristics of the data. We can now return to the Knowledge Flow and start executing some Distributed Weka processes on this dataset. Before we do so, it’s worth spending a minute or two to explain why we’re going to be operating on comma-separated values, CSV files without a header, rather than ARFF. Systems like Hadoop and Spark split data files up into blocks. This is to facilitate distributed storage of large files out on the cluster and also to allow data-local processing. This is where the processing is taken to where the data resides.
So rather than move the data around, we take the processing to where the data is.Within such frameworks like Hadoop and Spark, there are “readers”, as they’re called, for various text files and for various structured binary files. These readers maintain the integrity of the individual records within the files. They know where the boundaries between records are, and they don’t ever split a record in half. If we were to use ARFF within such a framework, we would need to write a special reader, due to the fact that ARFF files, as you know, have header information that occurs at the start of the file.
That header information provides details on what attributes are in the data, their types, and legal values, and so forth. Now because the data file gets split up, only one of the blocks, or chunks, of data out on the cluster would have that header information. That is why we’d have to write a special reader to handle it. Distributed Weka for Spark, as it stands at the moment, operates just on CSV data, simply because there are readers already available within Spark and Hadoop for dealing with such data. All right. Here we are back in the Knowledge Flow environment. Let’s execute the first
Distributed Weka job in the list here: the “Create an ARFF header job”. Make it a little larger here. We’ll use this one to verify that everything is installed correctly and running properly. Now, the goal of this job on the hypothyroid data is to analyze that CSV file and produce some summary statistics, and do this in a distributed way. At the same time, it collects all the information that’s necessary to create an ARFF header, and it stores this. And then any future jobs that we run can make use of this ARFF header information straight away, and not be required to analyze the CSV data a second or third time before they can run.
What we can do is go ahead and execute this and see how it runs. First of all, make your log area – switch to the log from the status area down at the bottom here – and make it a little bit larger, so that we can see what’s happening in the log, because Spark generates a lot of log output; there is information about what it’s doing, and you’ll see any problems that occur in that log as well. We have just one job that’s going to be executed here – the job to create the ARFF header – and we’ll just run this right now and make sure everything is working correctly.
Later on, we’ll take a look at the parameters for the job, and I’ll explain a little bit about how it’s configured. Up here in the upper left-hand corner of the Knowledge Flow, we can press this Play button and start the flow running. As I said, we can see a lot of information being dumped into the log here. Most of this is coming from Spark. Our job has completed. You can see here it says “Successfully stopped” something called a “SparkContext”.All right, so what has this job produced? We can see here in the flow that we have a dataset connection coming out of the ArffHeaderSparkJob to a TextViewer.
So if we open up the TextViewer and show the results – I’m going to make this just a little bit larger here, so that it fills the screen – we can see that, as the name suggests, it has created an ARFF header for the hypothyroid data. In fact, it’s an ARFF header on steroids, because there is some extra information in here. What we can see at the top is standard ARFF header information. Here’s all our attributes – just like we saw in the Explorer before – all the way down to Class here, where, in this row here we can see all of the values of the class attribute listed.
Now, below this is a bunch of additional information that we’ve added into this header. The way that other jobs are programmed when they make use of this, is that they can either access this additional information or remove it and use a standard ARFF header. So what we have in this additional information is a bunch of summary statistics that have been computed on the hypothyroid data running in parallel in the Spark environment. You can see that for the “age” attribute here there is summary statistics that have been computed on that. We have a count, we have a sum, we have a sum of squares, we have minimum and maximum values, and we have a mean. This is a numeric attribute.
And a standard deviation, as well. And similar for other attributes. For nominal attributes, it computes a frequency distribution. So down here in the class, the summary attribute for the class, we can see the class label for each of the values of the class followed by an underscore and a number, and that number is the count for that particular class label. The ARFF header job has computed a header for us and a bunch of summary statistics. Next time, we’ll take a look at how that’s configured, and we’ll also look at running some other Distributed ARFF jobs, as well.
In this lesson, we’ve covered getting Distributed Weka installed; our test dataset, the hypothyroid data; the data format processed by Distributed Weka; and we’ve taken a look at a Distributed Weka job running on Spark to generate some summary statistics and an ARFF header for the hypothyroid data.
<End Transcript>

<-- 4.6 Quiz -->
ARFF headers for Spark processing 
Question 1
Why does Distributed Weka operate on CSV data without a header row?
Because it is the most common format for big datasets.
Because it is simpler to parse than ARFF.
Because Spark and Hadoop may split datasets into blocks.
---
Correct answer(s):
Because Spark and Hadoop may split datasets into blocks.
---
Feedback correct:
Both Spark and Hadoop maintain record integrity when the data is distributed across nodes in a cluster.

<-- 4.6 Quiz -->
ARFF headers for Spark processing 
Question 2
Create a CSV version of the iris data. You can do this in the KnowledgeFlow interface.
First, make a Knowledge Flow that contains an ArffLoader connected by a dataSet connection to a CSVSaver. Configure the ArffLoader to open the iris data. Configure the CSVSaver by choosing a suitable directory to save to; also set noHeaderRow to True. Run the flow and verify that iris.csv has been created.
From the KnowledgeFlow’s templates menu, load the “Spark: create an ARFF header” flow.
Double-click the ArffHeaderSparkJob and enter the pathname for the iris CSV file into the inputFile property of the Spark Configuration tab. For example, if you saved iris.csv to your desktop, set inputFile to ${user.home}/Desktop/iris.csv. Because the iris data has only a few attributes, we will forego the “names” file and enter the names of the attributes directly. In the ArffHeaderSparkJob tab, set the attributeNames property to sepallength, sepalwidth, petallength, petalwidth, class; and clear the attributeNamesFile property. Set the outputHeaderFileName to iris.arff.
Execute the flow and look at the resulting ARFF header in the TextViewer.
What is the mean value of sepallength?
0.83
3.05
5.84
150
876
---
Correct answer(s):
5.84
---
Feedback incorrect:
That’s the standard deviation, not the mean
---
Feedback incorrect:
That’s the mean of sepalwidth, not sepallength
---
Feedback incorrect:
That’s the count, not the mean
---
Feedback incorrect:
That’s the sum, not the mean

<-- 4.6 Quiz -->
ARFF headers for Spark processing 
Question 3
What is the standard deviation of sepallength?
0.43
0.83
5.84
150
876
---
Correct answer(s):
0.83
---
Feedback incorrect:
That’s the standard deviation of sepalwidth, not sepallength
---
Feedback incorrect:
That’s the mean, not the standard deviation
---
Feedback incorrect:
That’s the count, not the standard deviation
---
Feedback incorrect:
That’s the sum, not the standard deviation

<-- 4.6 Quiz -->
ARFF headers for Spark processing 
Question 4
Do these values correspond to what is displayed when the data is loaded into the Explorer’s Preprocess panel?
Yes
No
---
Correct answer(s):
Yes

<-- 4.6 Quiz -->
ARFF headers for Spark processing 
Question 4
Do these values correspond to what is displayed when the data is loaded into the Explorer’s Preprocess panel?
Yes
No
---
Correct answer(s):

<-- 4.7 Video -->
Using Naive Bayes and JRip
There are many options when configuring a Distributed Weka job. The ArffHeaderSparkJob’s configuration panel has two tabs, Spark configuration, whose options relate to how the cluster is configured, including how many partitions to make from the data and the desired level of parallelism; and ArffHeaderSparkJob, which determines how Weka parses the CSV file containing the input data, including the names of attributes and the name of the header file that is created. Another Distributed Weka template is “Spark: train and save two classifiers”, which trains Naive Bayes and JRip classifiers from the same dataset.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello! In the last lesson, we installed Distributed Weka and ran our first Distributed Weka for Spark job that analyzed and computed a header for the hypothyroid dataset. In this lesson, we’ll take a closer look at how these jobs are configured, and we’ll run a few more jobs that use Weka classification algorithms, and learn classification models on the hypothyroid data. OK, here is the job that we ran last time. I’ve loaded it back up into the Knowledge Flow here. Let’s take a look at how it’s configured. So if I double-click on the ArffHeaderSparkJob component here on the canvas, it will bring up the configuration dialog, which is made up of two tabs here.
The first tab is entitled Spark configuration, and, as the name suggests, there are a bunch of options here related to how the cluster is configured. So up at the top we have a couple of options that are related to how Spark handles or manages memory out on the cluster. We won’t go into detail about exactly how those work, but suffice it to say that the defaults that are set here work reasonably well for most situations. Under that, there is something called the InputFile parameter, and that’s most important, because it’s the dataset that we’re operating on.
You can see here that it’s preconfigured to point to the hypothyroid data, which is in this sample data directory, which is, in turn, in the package installation directory for Distributed Weka for Spark. Then we have the masterHost parameter. This is where you can specify the address of the machine that the master Spark process is running on. In our case, we don’t have a Spark cluster, we’re running locally on our desktop machine, and Spark is treating each of the processing cores in our CPU as a processing node. So that’s why we have the local host specified here, and in parentheses we have an asterisk, which tells Spark that we want to make use of all of our available processing cores.
If we wanted to limit the number of cores that Spark uses on our desktop machine, then we could place a number inside those parentheses there to limit that. Similarly, the masterPort would be used if we were running against a cluster and we needed to provide the port that the Spark master process is listening on. Further down in the list here we can see something called the output directory. This is where Weka will be saving any results generated by the job. OK. The last parameter we’ll take a look at here is called minInputSlices. With this parameter, we’re telling Spark how many logical chunks to split the dataset up into.
So Spark will create partitions or slices of the dataset and process those, and it uses one worker task running on a core of the CPU of a processing node in order to process a given partition. Here we can really have some control over the level of parallelism applied to our dataset. If we had a processing cluster of 25 machines where each machine had a CPU with four cores, then Spark would be working at maximum efficiency if we chose 100 input slices or fewer for our dataset. That way, we would have the entire dataset processed in one wave of tasks. Let’s take a quick look at the second configuration panel in the dialog here entitled ArffHeaderSparkJob. So we click on that.
This relates to how Weka will parse the CSV file of hypothyroid data. There are a lot of options here related to CSV parsing, so what the field separator is, what the date format might be if there are date attributes, and so forth. We can also tell Weka, since this is a headerless CSV file, what the names of the attributes
are in the data, and we can do that in one of two ways: either by typing a comma-separated list of attribute names in this first text box at the top here; or we can point Weka to a file on the file system that contains the names of the attributes. We’re using that option in this case by saying there is a file called hypothyroid.names on the file system. The format of that file is a simple one. It just contains one attribute name per line in the file. We also have this option called pathToExistingHeader here.
If we have already run this job and created an ARFF header file and computed all the summary statistics, then there is no need for the job to run again, but we may have it as a component in an overall larger job. In that case, we can provide the path to the header file that was created in a previous execution, and Weka will then realize that it does not need to regenerate that file. And the last dialog box here is one where we tell it what we want to call that ARFF header file when it gets created. In this case, we’re calling it hypo.arff. All right.
Now let’s try running another one of the example flows that are included with the Distributed Weka for Spark package. Up here in the templates menu, let’s choose the “Spark train and save two classifiers” flow. Let’s load that one in. All right, here it is. OK, so what do we have in this flow? Well, we have – as we can see on the left-hand side here – the ArffHeaderSparkJob again. This time, however, it is configured to make use of an existing header file, if we happen to have already run this particular job on the hypothyroid dataset.
As we can see here, this path is now filled in, so we can take advantage of that existing header file that we may have already generated. If that is the case that this exists on disk, then it will load and use this header file, and then the only point of this job entry in this particular example is to load the CSV data and parse it into an internal Spark format that can then be used in the downstream job entries in the rest of this flow. So that’s ready to go pretty much. What we have next in the flow is something called the WekaClassifierSparkJob, and in fact we have two entries in this flow. These components are executed in sequence.
The ArffHeaderSparkJob will run first. When it succeeds, it triggers execution of the next component downstream in the flow, so the WekaClassifierSparkJob will then execute. We can see from this Spark job that there is a “text” connection, so it will produce a textual description of the classifier that it learns, which will then be displayed in the TextViewer here. That also gets saved out, along with the model itself, the actual Weka model, to the file system in our output directory, and we’ll take a look in there once we’ve finished looking at this flow and executing it. There’s also a second Weka Spark job here on the right-hand side, and this learns a different classifier.
The first one learns a Naive Bayes classifier, and the second one learns a JRip rule set. In between the two, we have another job that gets executed. This is called the RandomlyShuffleDataSparkJob. We’ll discuss exactly what that’s doing a little later on.Let’s take a quick look at the configuration dialog for the first of these WekaClassifierSparkJobs, the one that trains Naive Bayes. So we open that up – make it a little bit larger here. What we can see is a whole bunch of settings we can change.
We have some stuff at the top here related to telling the system what the class attribute is and what we want to call the serialized model file that gets written out as the output of this job to our output directory. Down at the bottom here, we can see that we have an option that allows us to choose a classifier we want to run and also configure its options, just like in regular Weka. In this case, we’ve chosen the standard Naive Bayes algorithm. You can see that we also have an option here that allows us to combine some filtering with this classifier, as well.
So we can opt to specify one of Weka preprocessing filters here and have that applied to the data before it reaches the classifier. We can combine multiple filters if we so desire by using Weka’s multifilter filter, which allows us to specify multiple filters to apply. All right, there are a number of other options here. I won’t describe them at this point in time. Let’s go ahead and run this flow now. We have two classifiers that are going to be trained, Naive Bayes and the JRip rule set. We can start this running. Before I do so, I’ll get the log opened so we can see the activity in the log, and we launch it now. All right, it’s processing away.
A lot of output in the log, and now it’s completed. Let’s take a look in the TextViewer, which has picked up the textual output of these two steps, and see what we have. Let’s look at the results. I’ll make that a little bit larger. The first entry here is, as expected, a Naive Bayes classifier model learned on our hypothyroid data. This is very similar to what you would see if you just ran standard desktop Weka. The second entry in our result list here is for the JRip classifier. It doesn’t actually say “JRip” here; instead it has something called weka.classifiers.meta.BatchPredictorVote. That’s a little bit interesting. We’ll take a look at this output.
And what we have is, instead of a single rule set, we have 4 separate rule sets, and they’ve been combined into an ensemble learner, a “vote” ensemble learner, which has 4 separate sets of JRip rules as its base learners. We can see the individual rules here in this list. So what’s happened in this case? Why has it done this? Why has it learned one Naive Bayes model when we apply Naive Bayes, but it’s learned 4 sets of JRip rules when we applied the JRip classifier? To answer that question, we’ll have to learn a little bit about how Distributed Weka learns a classifier when it runs on Spark, but we’ll leave that to the next lesson.
<End Transcript>

<-- 4.8 Quiz -->
Training classifiers with Spark 
Question 1
Open the “Spark: train and save two classifiers” template in the Knowledge Flow.
Delete the RandomlyShuffleDataSparkJob step and connect the WekaClassifierSparkJob step directly to the WekaClassifierSparkJob2 step with a success connection. This effectively passes the dataset on from the first classifier to the second.
Leave the first WekaClassifierSparkJob as Naive Bayes, and change the classifier in the second WekaClassifierSparkJob from JRip to Random Forest. Configure the Random Forest by changing numIterations from 100 to 10 and setting printClassifiers to True.
Execute the flow.
How many trees did Random Forest learn?
4
8
10
100
---
Correct answer(s):
8

<-- 4.8 Quiz -->
Training classifiers with Spark 
Question 2
For an ensemble learning algorithm like RandomForest, distributed Weka configures it to learn the base models from the data in each partition of the dataset (that is, each Spark RDD).
In the last question we asked for 10 trees but ended up with fewer. Why?
Some trees were dropped due to insufficient data in a particular partition
Maximum accuracy was achieved with fewer than 10 trees
The number of partitions does not divide evenly into 10
---
Correct answer(s):
The number of partitions does not divide evenly into 10
---
Feedback correct:
The Spark configuration defined in the ArffHeaderSparkJob sets the minInputSlices parameter to 4; that is, 4 partitions.

<-- 4.8 Quiz -->
Training classifiers with Spark 
Question 3
Change the minInputSlices property in the ArffHeaderSparkJob to create a dataset with 5 partitions. Run the flow again.
How many trees did RandomForest learn this time?
4
8
10
100
---
Correct answer(s):
10

<-- 4.8 Quiz -->
Training classifiers with Spark 
Question 4
How many nodes do the biggest and smallest trees in the ensemble contain?
Select all the answers you think are correct.
19: smallest tree
19: biggest tree
56: smallest tree
56: biggest tree
104: smallest tree
104: biggest tree
159: smallest tree
159: biggest tree
---
Correct answer(s):
56: smallest tree
159: biggest tree

<-- 4.8 Quiz -->
Training classifiers with Spark 
Question 5
Now change the parameter to create a dataset with 10 partitions and run the flow again.
How many nodes do the biggest and smallest trees contain now?
Select all the answers you think are correct.
19: smallest tree
19: biggest tree
56: smallest tree
56: biggest tree
104: smallest tree
104: biggest tree
159: smallest tree
159: biggest tree
---
Correct answer(s):
19: smallest tree
104: biggest tree

<-- 4.8 Quiz -->
Training classifiers with Spark 
Question 6
The size of the trees in the ensemble tends to decrease as the number of partitions increases. Why?
More partitions improves the quality of the data in each, resulting in compact, more accurate trees.
The number of partitions is now in sync with the requested number of trees, resulting in compact, more accurate trees.
With more partitions, there is less data in each from which to build a tree.
---
Correct answer(s):
With more partitions, there is less data in each from which to build a tree.

<-- 4.8 Quiz -->
Training classifiers with Spark 
Question 7
Return to the results for Naive Bayes in the first two runs, that is, with 4 and 5 partitions. Are the two Naive Bayes classifiers the same or different?
Same
Different
---
Correct answer(s):
Different
---
Feedback correct:
Yes, they’re slightly different: for example, the first number in the model is 52.3531 for 4 partitions and 52.2629 for 5 partitions.
This is a great surprise! Naive Bayes models should be independent of the number of partitions, because the distributed Weka package ensures that all statistics are computed and aggregated correctly.
The observed difference is rather subtle, and stems from the way that Weka’s implementation of Naive Bayes handles numeric attributes. For each numeric attribute Weka estimates a precision parameter from the training data, and uses it to round numeric values and to set the minimum allowable standard deviation for that attribute.

<-- 4.8 Quiz -->
Training classifiers with Spark 
Question 8
Edit the filtersToUse property in the Naive Bayes WekaClassifierSparkJob to use a RemoveType unsupervised attribute filter, and configure the filter to delete numeric attributes.
After selecting RemoveType, click the Add button on the GenericArrayEditor to add it to the list of filters. Close the GenericArrayEditor (in this case by clicking on the “x” in the top right corner) and then click OK in the WekaClassifierSparkJob’s dialog box.
Run the flow again using 4 partitions and then 5 partitions. Are the Naive Bayes models for 4 and 5 partitions …
The same as before using the filter?
Different from each other, and from before the filter was used?
The same as each other?
---
Correct answer(s):

<-- 4.8 Quiz -->
Training classifiers with Spark 
Question 8
Edit the filtersToUse property in the Naive Bayes WekaClassifierSparkJob to use a RemoveType unsupervised attribute filter, and configure the filter to delete numeric attributes.
After selecting RemoveType, click the Add button on the GenericArrayEditor to add it to the list of filters. Close the GenericArrayEditor (in this case by clicking on the “x” in the top right corner) and then click OK in the WekaClassifierSparkJob’s dialog box.
Run the flow again using 4 partitions and then 5 partitions. Are the Naive Bayes models for 4 and 5 partitions …
The same as before using the filter?
Different from each other, and from before the filter was used?
The same as each other?
---
Correct answer(s):
The same as each other?
---
Feedback correct:
But they’re different from before, because the numeric attributes have been removed.

<-- 4.9 Video -->
Map tasks and Reduce tasks
Map tasks produce models and a Reduce task aggregates them. Reduce strategies differ for Naive Bayes and other model types. We saw in the last lesson that Naive Bayes and JRip are treated differently. The reason is that Naive Bayes is easily parallelized by adding up frequency counts from the individual partitions, producing a single model. For JRip (and other classifiers), separate classifiers are learned for each partition (4 in this case), and a “vote” ensemble learner is produced that combines them. Also, for some classifiers (like JRip) it is beneficial to randomize the dataset before splitting it into partitions. Finally, we look at the “Spark: cross-validate two classifiers” template and examine how DIstributed Weka performs cross-validation.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello again! You’ll recall last time we ran the example Knowledge Flow template that built two different classifiers in the Spark environment, a Naive Bayes classifier and a JRip classifier. But we found that it did something different with the JRip classifier than it did with Naive Bayes. It actually ended up building 4 separate JRip classifiers and wrapping them up in a voted ensemble. In this lesson, we’ll take a closer look at exactly what happened there and the reasons for why there is a difference between the Naive Bayes example and the JRip example. Here is that Knowledge Flow template we ran last time, and, if we open up the TextViewer we can refresh our memories as to what the results looked like.
At the top we had the Naive Bayes classifier on our hypothyroid dataset, and we ended up with one model, as we expected. The second classifier that ran in this example was JRip, and as we can see from the results we ended up with 4 sets of JRip rules and these were combined in a voted meta classifier. So the question was why did this happen? Let’s take a closer look at it, and I’ll attempt to explain. OK, so here is a slide that attempts to describe how the processing occurs in Spark for our classifier job. On the left-hand side, we have the ArffHeaderSparkJob, which initially loads the data into main memory for us.
It loads the CSV data and creates one of Spark’s resilient distributed datasets with a number of splits, or “partitions” as they’re called in Spark. Each partition is processed by a worker out on the cluster, or, in our case, by a CPU core on our desktop machine. The map tasks process these partitions and create models – or to be more precise, they create partial models. In the case of Naive Bayes, the algorithm is fairly simple, and the model is comprised of a number of probability estimators, all of which can be computed incrementally and additively.
So when it comes to combining these probability estimators, we can simply add together their statistics, and we end up with one final model, which is identical to what we would get if we ran Naive Bayes sequentially on the dataset on our desktop. In the case of other types of classifiers – tree learners and rule learners like JRip are an example – it’s somewhat more difficult to try and aggregate these partial models into one final model which would be the same as if you were to run sequentially.
In that case, Weka takes the easy route of taking the partial models or the smaller models, which are learned on the splits of the data, an d combining them by simply making a voted ensemble out of them. OK. We’re nearly finished with this example, but before we leave there are a couple more aspects to touch on. One is output. We’ve seen some output in the TextViewer here in the Knowledge Flow, but I mentioned earlier that the jobs in Distributed Weka also store output on the file system. This can be our local file system or, if we’re using Hadoop and Hadoop’s distributed file system, it could be stored in HDFS.
Anyway, if we take a look in the source of the data here, our ArffHeaderSparkJob, recall that we saw we had some setup for our input files and also our output directory down here. This is where the jobs will store their output. Let’s take a look at that on the file system. If I find my home directory, I can see that directory that was specified in the configuration there, sparkOutput. If we go into that directory, we can see a couple of subfolders. One is where the ARFF header was stored by the ARFF header job.
We can look in there, and we can see hypo.arff, and, if we go back out and look in this model directory, we can see the models that were created by Distributed Weka. So we have one for Naive Bayes and one for the voted JRip model. OK. The other thing we haven’t mentioned is this job in the middle here called the RandomlyShuffleDataSparkJob. So what does this do? Well, as the name suggests, it’s a job that, in a distributed fashion, randomly shuffles the order of the rows or instances in the dataset that was being processed. In some cases, it’s advantageous to do this random shuffle. There are certain classifiers which this is beneficial for. Naive Bayes isn’t one of them.
It’s not affected by the order of the instances in the dataset that it learns from. However, other classifiers, like trees and rules, can be. In a worst case, we might end up with a partition of our Spark RDD where certain class values aren’t represented at all, if the data has perhaps been collected in some systematic way. For that reason, it can be beneficial to randomly shuffle the order of the instances before learning a classifier like a rule set or a decision tree. Before we finish today’s lesson, let’s take a look at one more of the example templates that come with Distributed Weka. We’ll take a look at the one that cross-validates two classifiers.
So this runs an evaluation, a cross-validation, inside of Distributed Weka. If we load this one – and I make it a little larger here – we can see that, apart from the ArffHeaderSparkJob and the RandomlyShuffleDataSparkJob, we now can see two components called the WekaClassifierEvaluationSparkJob. These are job entries that will perform a cross-validation out in Spark for us. There are two entries here, two WekaClassifierEvaluationSparkJob entries, because we’re comparing two classifiers under cross-validation. We’re comparing the Naive Bayes classifier again, and, in this case, the Random Forest classifier. Both of these will be evaluated under cross-validation inside of Spark. Let’s run the flow and see what happens. Switch to the log; we can watch some activity.
This will take a little bit longer than the previous job, as we’re running ten-fold cross-validation. Let’s take a look at the results in the TextViewer here. OK, we’ve got two entries, one for Naive Bayes, which is this first one here, and the second one is for the Random Forest classifier. As we can see in the textual output, the results look exactly the same as if you were to run a cross-validation in desktop Weka, and, similarly for the Random Forest classifier. Let’s consider how Distributed Weka performs a cross-validation. It actually involves two separate phases, or passes over the data. Phase one involves model construction, and phase two involves model evaluation. If we consider a simple three-fold cross-validation.
We know that the dataset gets split up into three distinct chunks during this process, and that models are created by training on two out of the three folds. So we end up with actually three models created, each of them trained on two-thirds of the data, and then we test them on the fold that was held out during the training process. In this example, our dataset in Spark is made up of two logical partitions. We can think of each of these partitions as containing part of each cross-validation fold. In this case, they would hold exactly half of each cross-validation fold, because we have two partitions. Each partition, as we know, is processed by a worker, or a map task.
In the model-building phase, the workers will build partial models, and there will be a model inside the worker that is created for one of the training splits of this cross-validation. So it will be a partial model. For example, we’ll have the first model created on folds two and three, or parts of folds two and three. Similarly, model two will be created on fold one and three, and model three on fold one and two. In each of these workers, these models are partial models, because they’ve only seen part of the data from those particular folds. In our example here, the map tasks will output a total of six partial models, two for each training split of the cross-validation.
This allows us to get parallelism involved in the reduce phase. We can run as many reduce tasks as there are models to be aggregated. Each reducer will have the goal of aggregating one of the models. So in our example here, the six partial models are aggregated to the three final models that you would expect from a three-fold cross-validation. The second phase of cross-validation is somewhat simpler than the first. It takes the models learned from the first phase and applies them to the holdout folds of our cross-validation in each of the logical partitions of our dataset. It uses them to evaluate each of those holdout folds.
The reduce task then takes all of the partial evaluation results coming out of the map tasks and aggregates them to one final full evaluation result, which is then written out to the file system. Over the last couple of lessons, we’ve looked at some of the example Knowledge Flow templates that come with Distributed Weka. We’ve looked at one that creates ARFF metadata and summary statistics for a dataset. We’ve looked at how Distributed Weka builds models and how it performs cross-validation. The next lesson will wrap things up and leave you with some directions for what to look at next with respect to Distributed Weka.
<End Transcript>

<-- 4.10 Quiz -->
Cross-validating classifiers with Spark 
Question 1
Open the  “Spark: cross-validate two classifiers” template and run it.
What percentage of instances do Naive Bayes and Random Forest classify correctly?
Select all the answers you think are correct.
Naive Bayes: 0.1%
Random Forest: 0.1%
Naive Bayes: 4.5%
Random Forest: 4.5%
Naive Bayes: 95.5%
Random Forest: 95.5%
Naive Bayes: 99.2%
Random Forest: 99.2%
---
Correct answer(s):
Naive Bayes: 95.5%
Random Forest: 99.2%
---
Feedback incorrect:
That’s the percentage incorrectly classified by Naive Bayes

<-- 4.10 Quiz -->
Cross-validating classifiers with Spark 
Question 2
What is the total number of instances predicted by the classifiers (whether correctly or incorrectly classified)?
29
169
3605
3745
3774
---
Correct answer(s):
3774
---
Feedback incorrect:
That’s the number incorrectly classified by Random Forest
---
Feedback incorrect:
That’s the number incorrectly classified by Naive Bayes
---
Feedback incorrect:
That’s the number correctly classified by Naive Bayes
---
Feedback incorrect:
That’s the number correctly classified by Random Forest

<-- 4.10 Quiz -->
Cross-validating classifiers with Spark 
Question 3
Load the hypothyroid data into the Explorer.
How many instances are there, and how many belong to the secondary_hypothyroid class?
Select all the answers you think are correct.
2 total
2 secondary_hypothyroid
30 total
30 secondary_hypothyroid
95 total
95 secondary_hypothyroid
3772 total
3772 secondary_hypothyroid
---
Correct answer(s):
2 secondary_hypothyroid
3772 total
---
Feedback correct:
There’s a 2-instance discrepancy between the number reported by the classifiers and the number shown by the Explorer.
This occurs because the RandomlyShuffleDataSparkJob step stratifies the data as well as shuffling it. It ensures that all classes are represented in each partition of the Spark RDD dataset — even if a class has fewer instances than there are partitions.
Here it is configured to produce 4 partitions, so 2 additional secondary_hypothyroid instances (sampled from the 2 original ones) are generated to force each partition of the data to include a representative of that class.
---
Feedback incorrect:
There’s a 2-instance discrepancy between the number reported by the classifiers and the number shown by the Explorer.
This occurs because the RandomlyShuffleDataSparkJob step stratifies the data as well as shuffling it. It ensures that all classes are represented in each partition of the Spark RDD dataset — even if a class has fewer instances than there are partitions.
Here it is configured to produce 4 partitions, so 2 additional secondary_hypothyroid instances (sampled from the 2 original ones) are generated to force each partition of the data to include a representative of that class.

<-- 4.10 Quiz -->
Cross-validating classifiers with Spark 
Question 4
Configure the RandomlyShuffleDataSparkJob to use 2 splits  instead of 4 (numRandomlyShuffledSplits), and run the flow again.
What percentage of instances do Naive Bayes and Random Forest classify correctly?
Select all the answers you think are correct.
Naive Bayes: 95.4%
Random Forest: 95.4%
Naive Bayes: 95.5%
Random Forest: 95.5%
Naive Bayes: 99.2%
Random Forest: 99.2%
Naive Bayes: 99.7%
Random Forest: 99.7%
---
Correct answer(s):
Naive Bayes: 95.4%
Random Forest: 99.7%

<-- 4.10 Quiz -->
Cross-validating classifiers with Spark 
Question 5
What is the total number of instances predicted by the classifiers (whether correctly or incorrectly classified)?
3772
3774
---
Correct answer(s):
3772
---
Feedback correct:
With only 2 partitions there is no need to generate extra secondary_hypothyroid instances, because there are sufficient original instances to put at least one in each partition

<-- 4.10 Quiz -->
Cross-validating classifiers with Spark 
Question 6
The performance of the Random Forest increased slightly with 4 partitions rather than 2, even though 100 trees are learned in each case.
This could be due to a different shuffling of the data caused by a different number of partitions, but is more likely to be because, with fewer partitions, each one contains more data.
To investigate further, vary the randomSeed property in the RandomlyShuffleDataSparkJob (use values 10, 50, 100) and observe the percentage of instances correctly classified by Random Forest when using 2 and 4 partitions. How does the performance compare?
Always better with 2 partitions
Always better with 4 partitions
Sometimes better with 2, sometimes with 4
---
Correct answer(s):
Always better with 2 partitions
---
Feedback correct:
This suggests that fewer partitions give better performance.

<-- 4.10 Quiz -->
Cross-validating classifiers with Spark 
Question 7
Let’s investigate how performance of Random Forest varies with the number of trees.
Reset the RandomlyShuffleDataSparkJob step to use 4 partitions, and the randomSeed back to 1.
Increase the number of trees learned by Random Forest (numIterations property) in the WekaClassifierEvaluationSparkJob2 step from the default value of 100 to 200. Run the flow again.
What percentage of instances does Random Forest classify correctly?
99.23%
99.26%
99.31%
99.36%
---
Correct answer(s):
99.36%
---
Feedback incorrect:
That’s the result for 100 iterations
---
Feedback incorrect:
That’s the result for 500 iterations
---
Feedback incorrect:
That’s the result for 400 iterations

<-- 4.10 Quiz -->
Cross-validating classifiers with Spark 
Question 8
Increase the number of trees to 300, 400 and then 500. Does performance improve?
Yes
No
---
Correct answer(s):

<-- 4.10 Quiz -->
Cross-validating classifiers with Spark 
Question 8
Increase the number of trees to 300, 400 and then 500. Does performance improve?
Yes
No
---
Correct answer(s):
No

<-- 4.11 Article -->
Random Forest performance
The performance of Random Forest does tend to improve with more trees, but only up to a point.
We found in the preceding Quiz that performance increases from 100 to 200 trees, but stays the same for 300 and deteriorates for 400 and 500 trees. (However, the difference is probably not statistically significant in this small example.)
The amount of data in each partition tends to be the limiting factor, and, as we have seen, this can be improved by reducing the number of partitions. However, in a practical “big data” problem this is unlikely to be an issue.
For big data, set the number of partitions to match the available hardware – the number of nodes/cores in the cluster, along with the amount of memory available to each. Configure Weka so that each partition contains as much data as possible, consistent with it fitting into the available memory.

<-- 4.12 Video -->
Miscellaneous Distributed Weka capabilities
There are other useful KnowledgeFlow templates for Distributed Weka. One computes a correlation matrix for input to Principal Component Analysis; another runs a parallel version of the k-means clustering algorithm. To process large datasets you need to run Distributed Weka on a cluster. The Apache Spark website contains information on how to set up a cluster; this blog post explains how to run a Spark cluster on a single machine using separate Java processes that communicate as though they were running on different machines – which is different from the “local mode” we’ve been using, where the entirety of Spark runs in a single Java process.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
In this, the final lesson of this class, we’ll touch briefly on a couple of Knowledge Flow templates that we haven’t had time to look at so far, and we’ll leave you with some things to look at for Distributed Weka, if you wish to take it further. Here we are back in the Knowledge Flow. If we open up the Templates menu again and scroll down a little bit here, we can see a tem plate called “Compute a correlation matrix and run PCA”, where PCA stands for Principal Components Analysis. Let’s open this one. What we have is our trusty ArffHeaderSparkJob, which loads our hypothyroid data again, and we have a little step here called the CorrelationMatrixSparkJob.
And we have an ImageViewer and a TextViewer attached to that. This suggests that this job will produce some kind of an image that we can take a look at, and also some textual results. In the dialog for the CorrelationMatrixSparkJob here we have a few options, mainly related to exactly what sort of matrix is going to be computed, so we can compute either a correlation matrix or a covariance matrix. We have an option to run Principal Components Analysis. The algorithm for Principal Components Analysis can compute the principal components using either a correlation matrix as input or a covariance matrix. All right. Let’s run this now and see what it produces. It just takes a few seconds to run. And it’s finished.
OK, let’s open up the TextViewer. In the TextViewer, we have the result of the Principal Components Analysis and the correlation matrix that was computed. We can see that the correlation matrix and the Principal Components Analysis only involve the numeric attributes that are present in the hypothyroid data. Let’s take a quick look in the ImageViewer now. If we open up the ImageViewer, we can see that we have a graphical heat map representation of our correlation matrix, where the colors indicate the magnitude of the correlations between the attributes – the numeric attributes – in the hypothyroid data. Right, let’s take a look at one more example before we finish with Distributed Weka.
In the Templates menu here, we have a job called run K-means||. K-means parallel is, as the name suggests, a parallel version of the k-means algorithm. For clustering in Distributed Weka, unfortunately we can’t use the trick of creating a voted ensemble like we did in the classification case. It’s not possible to make a voted ensemble out of separate clustering models. This is why there is only k-means available in Distributed Weka so far, as it’s the only clustering algorithm that has been implemented in a distributed fashion, specifically for Distributed Weka. This job takes a little while to run, so through the magic of video editing, I’ve executed it in between cuts to save a little bit of time.
It actually takes longer to run than sequential Weka does if you were to run k-means in the Explorer on the hypothyroid dataset. This is simply due to the fact that there is a certain amount of overhead involved in Spark’s communication, the creation of its RDD data structures, and so forth; and that overhead actually outweighs the speed gained through parallel processing in this local case when we’re just using the cores that are available on our CPU. If our dataset was much larger and we were running on a real cluster, then we would have a true benefit from using a distributed approach.
In the TextViewer, we can see the clustering results for k-means, which look exactly the same – or are in the same format – as if you were to run k-means in standalone Weka on your desktop. So where to from here? Experimenting with Distributed Weka in local mode using small datasets is the best way to get familiar with the capabilities of it, and explore what it has to offer. However, if you want to process larger datasets, then you’ll need to run on a cluster. We’ll take a little look at what’s available on the web to help you get started in that area.
The first place to go for information is the main Apache Spark website, so let’s take a look at that. OK, under the documentation section here, we can find the documentation for the latest release of Spark. We go to that page, and there’s information on downloading, running some examples, and then down here a little ways we have information on launching on a cluster. The first thing to look at is the overview of cluster mode. This will describe exactly how a cluster is configured and set up to run. Then there are various different types of clusters that you can run Spark on. The simplest is called a stand-alone mode, and there is a documentation section here on that mode.
That would be the one to start with first. There are several other modes of clustered running for Spark, including something called Mesos and YARN. These are different ways of managing the machines in a cluster. The stand-alone mode is the simplest. There are a number of blogs on the web that step you through the process of setting up a stand-alone cluster on a single machine. So if we search for “Apache Spark standalone cluster install”, there are a number if hits in Google for information on setting up a cluster. One that’s particularly concise, or, at least, I thought it was concise and could be a good place to start, is this one here.
If we take a look at that, we can see a very short introduction to getting started with a Spark cluster running on a single machine. This is different from what we’ve been looking at so far, where we’ve been running in local mode. That’s where the entirety of Spark runs in a single JVM process. The stand-alone cluster running on a single machine involves multiple separate Java processes, and they communicate as if they were running on different machines. This tutorial is a reasonably short introduction to getting started with that. That’s it for this lesson.
Today, we took a look at how you can use Distributed Weka to compute a correlation matrix in Spark and then use that correlation matrix as input to a Principal Components Analysis. We also took a look at the k-means algorithm running in a distributed fashion inside of Spark, and we took a little look at information on setting up Spark clusters. Well, I hope you’ve enjoyed learning about how to use Weka in a distributed processing environment, and now I’ll leave you with some links to further information on Distributed Weka and on Apache Spark.
<End Transcript>

<-- 4.13 Quiz -->
Playing with template flows
Question 1
Open the “Spark: run k-means parallel” template and run it.
What is the within-cluster sum of squared errors of the k-means model learned on the hypothyroid data, and how many iterations did k-means perform to generate it?
Select all the answers you think are correct.
Sum of squared errors: 56.7
Sum of squared errors: 3867.2
Sum of squared errors: 3867.3
Sum of squared errors: 4329.4
Number of iterations: 6
Number of iterations: 8
Number of iterations: 10
Number of iterations: 12
---
Correct answer(s):
Sum of squared errors: 3867.3
Number of iterations: 10

<-- 4.13 Quiz -->
Playing with template flows
Question 2
k-means parallel uses a distributed version of the k-means++ initialization routine designed to give a better final solution than using random initialization.
Switch to random initialization in the KMeansClustererSparkJob by setting the initWithRandomCentroids property to True, and then run the flow again.
What is the within-cluster sum of squared errors now?
56.7
3867.2
3867.3
4329.4
---
Correct answer(s):
4329.4

<-- 4.13 Quiz -->
Playing with template flows
Question 3
Is the k-means++ initialization really better than random initialization?
Try different seeds, using values 10, 50, 100 for the randomSeed property in the KMeansClustererSparkJob for both k-means++ initialization (initWithRandomCentroids = False) and random initialization (initWithRandomCentroids = True).
The best result using seed values 1, 10, 50 and 100 is given by …
k-means++
k-means with random initialization
---
Correct answer(s):
k-means++
---
Feedback correct:
The 3867.3 obtained above is the best of all 8 results obtained using the 4 seeds and 2 initialization methods

<-- 4.13 Quiz -->
Playing with template flows
Question 4
How many of the top 4 of the 8 results in the previous question are obtained by k-means++ initialization?
0
1
2
3
4
---
Correct answer(s):
3

<-- 4.13 Quiz -->
Playing with template flows
Question 5
Open the “Spark: compute a correlation matrix and run PCA” template and run it.
Right-click on the ImageViewer and select Show images.
What is the largest off-diagonal correlation in the matrix?
0.25
0.29
0.31
0.52
0.65
---
Correct answer(s):
0.52

<-- 4.13 Quiz -->
Playing with template flows
Question 6
Open the TextViewer to see the PCA analysis.
Which attribute was dropped from the analysis?
age
sex
sick
TBG
TBG measured
TT4
---
Correct answer(s):
TBG

<-- 4.13 Quiz -->
Playing with template flows
Question 7
Examine the hypothyroid data in the Explorer’s Preprocess panel to determine why the TBG attribute was dropped.
It has a constant value across all instances
It is noisy
All its values are missing
---
Correct answer(s):

<-- 4.13 Quiz -->
Playing with template flows
Question 7
Examine the hypothyroid data in the Explorer’s Preprocess panel to determine why the TBG attribute was dropped.
It has a constant value across all instances
It is noisy
All its values are missing
---
Correct answer(s):
All its values are missing

<-- 4.14 Video -->
Image classification
Mike Mayo shows that with appropriate features, Weka can be used to classify images. The imageFilters package processes image files to extract features, and implements 10 different feature sets. You need to put all your images into a single folder and create an ARFF file with two attributes: the image filename (a string) and its class (nominal). The image filters appear as unsupervised instance filters, under imageFilter. One is the ColorLayoutFilter, another is the EdgeHistogramFilter. Each adds several attributes to the ARFF file. Furthermore, they can be used in sequence, each filter adding its own attributes.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! My name is Mike Mayo, and today I’m just going to demonstrate for you the imageFilters package, which is a package you can download for Weka using the Package Manager. What the imageFilters package does is let you convert images into features so that you can run image classification experiments, and then you can do exciting things like face recognition, scene recognition, and maybe even object detection. What I’ll do is just go over what the imageFilters package does in this lesson and give a quick demo. So what is an image feature? Basically, it’s a measurement concerning an image. In this
example, there are a couple of images: one is a sunflower; one is a tree. The measurements that we’re taking from the image are to do with things such as color and brightness and shape. Both of those images vary quite a bit in terms of those four different measurements. Once we calculate those measurements, we can put them together in a feature vector, and then we can use Weka’s standard machine learning algorithms to do some image experiments and see if we can classify different types of images. The first thing you need to do when you want to run an image classification experiment is get a whole lot of images.
For the image filters to work, they all need to be in one directory. Here I’ve got an example of a dataset. It’s basically a collection of monarch butterfly images and owl images, and we can see that they’re pretty easy to distinguish. Monarch butterflies are mostly orange and black; owls are mostly white. Once you have your directory of images, you need to create an ARFF file. The ARFF file is just like the normal ARFF files you’ve been using so far. The difference is that it only contains two attributes, and one of them is a string. The first attribute, which is the string, has to contain the filenames of the images, and the second attribute contains the class.
So here on the left I’ve got an example of such a dataset, and you can see that it’s pretty straightforward.
There are the two attributes there: the first one is a filename, the second one is the class. When we apply the filters, all the filters are going to do is add further attributes to the dataset. In all cases, they are numeric attributes, and so the example on the right shows a very simple filter that adds three numeric values to the dataset. Now I’ve got Weka open, and I’ve opened that ARFF file to have a look at. You can
see that there are two attributes here: the filename, which is a string, so there’s no useful information about it here; and the class, which is a nominal attribute, which is shown here. You can see that there are two classes and they’re both equal frequency. So there are 50 examples of each in this dataset. I now want to apply the filter, and if you’ve installed the imageFilters package, all the filters should be available under Unsupervised/Instance/imageFilter. OK. So if you’ve installed the package correctly, you should have this directory here, which you can then open to get all of these filters. All you do is select one. So I’m going to choose the ColorLayoutFilter, and there’s
a “–D” option here: that simply refers to the directory that contains all the images. I’m going to put the image directory in here. Once I’ve done that, I can now apply the filter and it will go away and process all the images. Weka is reading in all those images and extracting all the features, and after a few moments it’s done. We can see that a whole lot of additional features have been added to the dataset, and they’re all numeric. If we go down here, we can see that there are 33 in total, and the class label is still there.
In order to run a classification experiment after running the filter for the first time, what we have to do is remove the filename attribute because that is a string and that will cause problems for many different classifiers. So I’m going to remove it, and then switch over to the Classify tab. I can open that, and I can find a classifier. I’ll use J48 just for fun. Click Start, and then we can see that Weka correctly classified all of those images 90% of the time. That’s pretty good accuracy. It is possible to apply more than one filter in sequence to your images. If you want to do that, all you need to do is repeatedly apply different filters to the dataset.
I’ll give you a quick example of that. I’ll click undo, just to get the filename attribute back, because Weka needs to know what the filenames are. I’ll then choose a different filter. For this example, I’ll choose the EdgeHistogram as the second set of features. Again, I just set the directory. I click Apply, and we wait to see what happens. OK, the features are there now. We can see that they’ve got a different name. These are edge histogram features. If we scroll down, we can see there are a lot of them, 80 in total, and they’ve been inserted before the color layout features. So the old color layout features are still there.
The class is still at the end, but we have a lot more features this time. Again, I’ll remove the filename attribute, switch to Classify, run exactly the same experiment again, and this time, interestingly, Weka does slightly less. It gets 88% this time compared to 90% last time. That means that adding those additional edge histogram features has in fact decreased the accuracy a little bit. That probably makes sense if you think about it, because clearly the main differentiating feature between these two classes – monarch butterfly and owl – is color. So the size and direction of edges probably doesn’t provide that much information. Clearly, in this case only, the edge features haven’t been very useful.
That’s a quick run down of the imageFilters package. I’ve put a summary of the steps there, if you want to try that package out and do some experiments with your own images. It’s really straightforward to use, and I hope you enjoyed this lesson.
<End Transcript>

<-- 4.15 Quiz -->
Processing images with different feature sets 
Question 1
From the the unsupervised/instance/imagefilter menu choose the GaborFilter, and apply it to vehicle_images.arff.
Note: you will have to set the filter’s imageDirectory property so that Weka can find the images. In my case, I set it to /Users/ihw/wekafiles/packages/imageFilters/data/vehicle_images.
How many features are extracted?
59
60
61
62
---
Correct answer(s):
60
---
Feedback correct:
There are 62 attributes, but filename and class are not features extracted by the filter
---
Feedback incorrect:
This is the total number of attributes, but some of them are not features extracted by the filter

<-- 4.15 Quiz -->
Processing images with different feature sets 
Question 2
Remove the filename attribute and run 10-fold cross-validation experiments on the filtered dataset using J48, Naive Bayes and SMO.
Which classifier has the greatest accuracy?
J48
Naive Bayes
SMO
---
Correct answer(s):
Naive Bayes
---
Feedback correct:
Naive Bayes gets an accuracy of 60%, which is greater than J48 (41.7%) and SMO (58.3%)
---
Feedback incorrect:
J48 gets an accuracy of 41.7%
---
Feedback incorrect:
SMO gets an accuracy of 58.3%

<-- 4.15 Quiz -->
Processing images with different feature sets 
Question 3
Undo the changes to the dataset and repeat steps 1 and 2 using the ColorLayout filter.
Which classifier has the greatest accuracy now? (It’s a tie: there are 2 correct answers, so check 2 boxes.)
Select all the answers you think are correct.
J48
Naive Bayes
SMO
---
Correct answer(s):
Naive Bayes
SMO
---
Feedback correct:
Naive Bayes gets an accuracy of 72%
SMO gets an accuracy of 72%
---
Feedback incorrect:
J48 gets an accuracy of 58%
---
Feedback incorrect:
Naive Bayes gets an accuracy of 72%
---
Feedback incorrect:
SMO gets an accuracy of 72%

<-- 4.15 Quiz -->
Processing images with different feature sets 
Question 4
Undo the changes again, and this time apply the EdgeHistogram and ColorLayout filters in sequence, so that the dataset contains both sets of features.
Rerun your experiments. Which classifier has the greatest accuracy now?
J48
Naive Bayes
SMO
---
Correct answer(s):
SMO
---
Feedback correct:
SMO gets an accuracy of 92%
---
Feedback incorrect:
J48 gets an accuracy of 62%
---
Feedback incorrect:
Naive Bayes gets an accuracy of 87%

<-- 4.15 Quiz -->
Processing images with different feature sets 
Question 5
In the preceding question, which class has the fewest misclassifications when SMO is used?
PLANE
TRAIN
CAR
---
Correct answer(s):
TRAIN
---
Feedback correct:
The TRAIN class has 1 misclassification; both the others have 2
---
Feedback incorrect:
The PLANE class has 2 misclassifications
---
Feedback incorrect:
The CAR class has 2 misclassifications

<-- 4.15 Quiz -->
Processing images with different feature sets 
Question 6
With the EdgeHistogram/ColorLayout dataset still open, go to “Select Attributes” and use the InfoGainAttributeEval and Ranker combination to rank the attributes.
One type of feature is clearly more useful. Which?
color features
edge features
---
Correct answer(s):
edge features
---
Feedback correct:
Edge Histogram features clearly dominate the top of the ranked list of attributes

<-- 4.15 Quiz -->
Processing images with different feature sets 
Question 7
Look at the images in the dataset. Why are edge features clearly more useful than color features?
color characterizes the appearance of the cars/planes/trains but there are few distinctive shapes or edges within each class
edges characterize distinct shapes of the cars/planes/trains but there are few distinctive colors within each class
---
Correct answer(s):

<-- 4.15 Quiz -->
Processing images with different feature sets 
Question 7
Look at the images in the dataset. Why are edge features clearly more useful than color features?
color characterizes the appearance of the cars/planes/trains but there are few distinctive shapes or edges within each class
edges characterize distinct shapes of the cars/planes/trains but there are few distinctive colors within each class
---
Correct answer(s):
edges characterize distinct shapes of the cars/planes/trains but there are few distinctive colors within each class

<-- 4.16 Discussion -->
Reflect on this week’s Big Question
This week’s Big Question is, “Can you distribute Weka jobs over several machines?”
Of course, as we said when introducing it, the answer is “yes”!
We promised that by the end, you’d be acquainted with Distributed Weka, its design goals, its use of the map-reduce idea for breaking computations down for parallel implementation, and – most importantly – how to install it and use it from the Knowledge Flow interface. Even though you’re running it on a single desktop machine, each of the individual cores in the CPU is treated as a separate processing node.
Here is a good place to discuss what you have learned with fellow learners.

<-- 4.17 Article -->
Index
(A full index to the course appears at the end of Week 1.)
      Topic
      Step
      Datasets
      butterfly_vs_owl
      4.14
      hypothyroid
      4.5, 4.7, 4.8, 4.10, 4.12, 4.13
      iris
      4.6
      vehicle_images
      4.15
      Classifiers
      JRip
      4.7, 4.9
      J48
      4.14, 4.15
      NaiveBayes
      4.7, 4.8, 4.9, 4.10, 4.15
      RandomForest
      4.8, 4.9, 4.10
      SMO
      4.15
      Filters
      ColorLayout
      4.14, 4.15
      EdgeHistogram
      4.14, 4.15
      Gabor
      4.15
      RemoveType
      4.8
      Packages
      distributedWekaBase
      4.2, 4.5
      distributedWekaSpark
      4.2, 4.5, 4.6
      imageFilters
      4.14, 4.15
      KnowledgeFlow components
      ArffHeaderSparkJob
      4.5, 4.6, 4.7, 4.8, 4.9, 4.12
      ArffLoader
      4.6
      CorrelationMatrixSparkJob
      4.12
      CSVSaver
      4.6
      ImageViewer
      4.12, 4.13
      KMeansClustererSparkJob
      4.13
      RandomlyShuffleDataSparkJob
      4.7, 4.8, 4.9, 4.10
      WekaClassifierSparkJob
      4.5, 4.7, 4.8
      WekaClassifierEvaluationSparkJob
      4.5, 4.9, 4.10
      TextViewer
      4.5, 4.6, 4.9, 4.12, 4.13
      Plus
      Distributed Weka
      4.1–4.13
      Hadoop
      4.2, 4.4
      k-means clustering
      4.12, 4.13
      Map-reduce
      4.2, 4.3, 4.9
      Principal components analysis
      4.12, 4.13
      Spark
      4.2, 4.4, 4.5, 4.12

<-- 5.0 Todo -->
Scripting Weka 
How can you script Weka?
This week's Big Question!
5.1
How can you script Weka?
article
Invoking Python from Weka 
Scripting allows automation, and reproducible experiments. There's a Weka package that opens an editor in which you can write and execute Python; we show a script for loading and filtering data. 
5.2
Invoking Python from Weka
video (09:12)
5.3
Do it yourself!
article
5.4
Some Python commands 
quiz
Building models 
We write a Python script to build a J48 classifier for the "anneal" dataset, and add code to evaluate it using 10-fold cross-validation. We also build a J48 model and make predictions for an unlabeled test set.
5.5
Building models
video (09:21)
5.6
Datasets and scripts
article
5.7
Using Python scripts 
quiz
Visualization 
Using the open source library JfreeChart. We create four visualizations with Weka: classifier errors (with error sizes); multiple ROC curves; a J48 decision tree; and a BayesNet network graph.
5.8
Visualization
video (12:01)
5.9
Datasets and scripts
article
5.10
Showing bubble plots and ROC curves 
quiz
Invoking Weka from Python 
Now we bring Weka to the Python universe. Setting Python up on Linux is easy. But on Windows and OS X it’s not for the faint-hearted! We use the python-weka-wrapper library to replicate scripts from previous lessons.
5.11
Invoking Weka from Python
video (09:36)
5.12
Do it yourself!
article
5.13
Share your experience ...
discussion
5.14
Investigating the python-weka-wrapper library 
quiz
A data mining challenge, and some Groovy 
Put your knowledge to use by applying it to real-world data! Groovy is an alternative scripting language to Python; we use it to replicate some Python scripts.
5.15
A challenge, and some Groovy 
video (11:57)
5.16
Datasets and scripts
article
5.17
The data mining challenge
quiz
5.18
The data mining challenge: An expert speaks
article
5.19
The data mining challenge
discussion
5.20
Feelin' Groovy?
quiz
Course summary 
What have we done? You've learned a powerful technology: please use it ethically. One way to continue learning is through data mining competitions.
5.21
Course summary 
video (07:03)
5.22
Reflect on this week’s Big Question
discussion
Farewell
Once again, it's time to say goodbye.
5.23
Post-course assessment 
test
This is a test step, it helps you verify your understanding. If you want to earn a Certificate of Achievement on this course you need to complete this test and any others, scoring an average of 70% or above.
To take tests you need to upgrade this course.  It costs $94, which also gets you:
Unlimited access to the course for as long as it exists on FutureLearn, so you can learn at your own pace
A Certificate of Achievement when you’re eligible, to prove what you’ve learned
Upgrade
Find out more
5.24
Farewell
article
5.25
Index
article

<-- 5.1 Article -->
How can you script Weka?
Have you found that repeatedly executing the same or similar tasks through an interactive interface like the Weka Explorer can become tedious?
Scripts are programs that automate the execution of tasks that could alternatively be executed interactively by a human operator. Typically, scripts are typed into an interactive interface and executed immediately.
Wouldn’t it be cool to be able to “script” Weka, and avoid all this pointing, selecting, and clicking? Yes? Read on …

<-- 5.2 Video -->
Invoking Python from Weka
Peter Reutemann introduces scripting, and then demonstrates a Weka package that opens an editor in which you can write and execute Python scripts. Finally he writes a script for loading and filtering data.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi, everyone! I’m Peter. Welcome to my class on scripting. Why would you do scripting? Well, there’s pros and cons to scripting. On the positive side of things, when you write a script, it captures all the steps that you performed from preprocessing to modeling to evaluation. Also, when you write a script, you really only write it once, and you can run it multiple times with no extra cost. It’s also very easy to create a variant of the script in order to test some theories. For example, tweaking some parameters of a classifier or swapping out a classifier completely. The best thing about scripting is that you don’t need to compile anything like you would have to with Java code.
On the not-so-good side of things, you will have to do programming. You need to familiarize yourself with the APIs of the libraries that are involved, and writing code is usually slower than clicking in the GUI. Now, what scripting languages will we cover in this class? We will cover Jython, Python, and Groovy. Jython is basically a pure Java implementation of Python 2.7, which runs solely in the Java Virtual Machine. This means it gives you access to all the Java libraries that are on the CLASSPATH. If you’re using Python code, then it has to be pure Python, no native libraries like, for example NumPy. As for Python, we’ll be using Python 2.7, and we’ll be invoking Weka through Python 2.7.
It gives you then all the access that you need to the full Python library ecosystem. At the end, we’ll be touching briefly on Groovy, which has a Java-like syntax and also runs in the Java Virtual Machine. Once again, it gives you access to all the Java libraries on the CLASSPATH. In order to demonstrate why Python might be a good choice of programming language for doing the scripting, is simply by comparing what Java code would look like and Python code would look like for doing the same thing. What we’re trying to do is simply output ten times “Hello WekaMOOC!”.Looking at the Java code here, you have the outer class definition, then you have your main method.
Inside your main method, you have your for loop, where you finally output stuff. In Python, this whole thing collapses to a two-liner. You simply iterate from 0 to 9 and then print the whole thing out. Done. Now, in order to have Jython support in Weka, we need to install a package. I’m going to start up Weka. In the Package Manager, we need to install tigerJython. I’ve already done that, and, for plotting Jython, we also want to use jfreechart, and for that reason, you want to install the jfreechartoffscreenRenderer library.After we’ve done that, we have to restart Weka.
Then, under the Tools menu, we will have a Jython console menu item, which brings up a little user interface for writing and running Jython scripts. The first time round it takes a little bit longer because it analyzes all the libraries that are in your CLASSPATH.Here’s our little interface. What you can see here is basically where you write your script. Down here you would see errors and so on, and output that your script generates. You execute your script with the green triangle up here. You can also turn debug mode on and off, which allows you to basically step through the program that you’ve written.
You can also set breakpoints up here, which allow you to stop at certain points in the program and then analyze, for instance, what the values for variables are, and so on. When running things, I usually run multiple scripts in parallel, so under Preferences I usually have a smaller font, and I’d rather use tabs than just a single one. Let’s just revisit our really, really simple example that we had previously. We were just outputting our “Hello World”, more or less. When we run this – not in debug mode for the time being – I’m just going to run that, we’ll see there’s an output from 1 to 10, “Hello WekaMOOC!”
Now if we are in debug mode, once again toggling it, then we can define how fast it actually goes through, and we can simply go through and run it. You can see the instruction pointer sort of toggling between those two lines, and you can also see over here, when you open up variables and types, that the variable “i” gets incremented. This is a first quick introduction to tigerJython. When you’re writing code, you have to find information, and the best information on Java libraries, like Weka, is using the Javadoc.
Also, coming with your release or snapshot that you’ve installed, you’ll find a wekaexample s.zip file, which contains quite a lot of example code that should get you going in how to use APIs in Weka. Last, but not least, also check out the WekaManual.pdf document. In the appendix under the “Using the API” section, you will find most of the important APIs in Weka explained and how to use them. Of course, I promised that we’re going to write a little script. What we’re going to do is load data and filter it and print it out. However, since all the installations of Weka will be different around the world, in order to find datasets I’ll be using a little trick.
I’ll be using an environment variable to point to the directory where I’ve stored my datasets. I’m going to close Weka for the time being. You can see here on my desktop in the data directory, I have various datasets, and we want to point to that directory. I’m going to copy that path and I’m going to add an environment variable. I’m going into the Advanced settings, Environment variables, and I’m going to create one called MOOC_DATA and paste that in there. OK. Close that dialogue again, and we can close that, too. Then we can start up Weka again. We’re starting up our Jython console again. First of all, we’ll have to import some classes to actually do stuff.
First of all, we actually want to load data, and we’ll be using the DataSource class for that, abbreviated to DS; the Filter class for filtering a dataset; and the Remove filter to do the actual work. The os library is a Jython/Python library which gives us access to the operating system, like, for example, environment variables and so on.In order to utilize the MOOC_DATA environment variables that I’ve just configured, I’m using the os.environ.get method, the os.sep property for forward or backward slash, depending on what operating system you’re running, plus the name of the dataset, in this case iris, so I’m basically loading that. Then we’re going to configure our filter. So we want to have a Remove filter.
We want to remove the last attribute, which is done via the –R last option. Then we are telling the filter about what the data actually looks like, so it can configure itself internally. Then, we’re using the Filter class to actually push the data through our Remove filter and get a new dataset. Finally, we’re going to output that new dataset in the console. We run this now, and we get a lot of output here. If we scroll to the top of it, we can see that the relation name has changed with the filter set-up and there is no longer any class attribute. In this first lesson, we have installed tigerJython.
We’ve seen that Python is actually very easy to read and write, and is quite short as well compared to Java; learned about where we can find API documentation; and wrote our first Jython script.
<End Transcript>

<-- 5.3 Article -->
Do it yourself!
First install two packages using the Package Manager:
  the tigerjython package
  the jfreechartOffscreenRenderer package
Note: tigerjython is not yet compatible with Java 9. When you installed Weka, if you downloaded a self-extracting archive that included Java, it will have been Java 8; no problems. However, if you normally run Weka under Java 9 you will have to temporarily use Java 8.
Then create an environment variable called MOOC_DATA that points to your Weka data files folder:
  On Windows, Peter right-clicked on This PC and selected Properties to get the Control Panel, then Advanced system settings, then Environment Variables. Click New… and type in the variable name and value.
  On the Mac, you can install envPane in your Preferences, which makes it easy to set environment variables.
Here’s Peter’s script for loading and filtering data:
  load_and_filter_data.py

<-- 5.4 Quiz -->
Some Python commands 
Question 1
In Python, “lists” store a collection of different pieces of information under a single variable name. The list items are separated by commas, and the list is enclosed by square brackets.
In Weka’s Jython console, try this:
x = ['a', 'b', 'c', 1, 2, 3]
print x
print x[2]
… and click the little green arrow. (Numbering starts from 0; x[0] is ‘a’.)
Now for the question: What method is used to add an item to a list?
add
append
plus
---
Correct answer(s):
append
---
Feedback correct:
In Weka’s Jython console, try:
x = ['a','b','c',1,2,3]
x.append('d')
print x
… and click the little green arrow

<-- 5.4 Quiz -->
Some Python commands 
Question 2
How do you delete the third item from list x?
del x[2]
delete x[2]
remove(x, 2)
---
Correct answer(s):
del x[2]
---
Feedback correct:
x = ['a','b','c',1,2,3]
del x[2]
print x

<-- 5.4 Quiz -->
Some Python commands 
Question 3
How do you remove integer value 2 from list x = [‘a’, ‘b’, ‘c’, 1, 2, 3]?
del x[2]
del x, 2
x.remove(2)
remove(x, 2)
---
Correct answer(s):
x.remove(2)
---
Feedback correct:
x = ['a','b','c',1,2,3]
x.remove(2)
print x

<-- 5.4 Quiz -->
Some Python commands 
Question 4
A Python “tuple” is like a list but uses parentheses rather than square brackets; in addition, unlike lists, tuples cannot be changed.
Try this:
y = ('a', 'b', 'c', 1, 2, 3)
print y
Now for the question: How do you turn list x into a tuple?
x.as_tuple()
make_tuple(x)
tuple(x)
---
Correct answer(s):
tuple(x)
---
Feedback correct:
x = ['a','b','c',1,2,3]
y = tuple(x)
print y
print x

<-- 5.4 Quiz -->
Some Python commands 
Question 5
A Python “dictionary” is a set of key : value pairs that allow you to retrieve a value given the corresponding key; the set is enclosed in curly brackets.
Try this:
z = {1: 'hello', 2: 'world', '3': 'answer', 'a': 'is', 'this': 42}
print z[1], z[2]
Then experiment by printing z[3], z[4], z[‘3’], z[‘a’], etc.
Now for the question: How do you retrieve the value 42 from the dictionary z?
z[‘this’]
z.get(‘this’)
get(z, ‘this’)
---
Correct answer(s):
z[‘this’]
---
Feedback correct:
z = {1: 'hello', 2: 'world', '3': 'answer', 'a': 'is', 'this': 42}
print z['this']

<-- 5.4 Quiz -->
Some Python commands 
Question 6
How can you create a copy of list x?  (There are two ways, so check 2 answers.)
Select all the answers you think are correct.
x.copy()
x[:]
copy(x)
---
Correct answer(s):
x[:]
copy(x)
---
Feedback correct:
x = ['a','b','c',1,2,3]
x1 = x[:]
print x1
copy(x) is the recommended practice for copying objects, but it requires you to first import the copy function from the standard Python library. You do this by saying “from copy import copy”.
from copy import copy
x = ['a','b','c',1,2,3]
x1 = copy(x)
print x1
---
Feedback incorrect:
x = ['a','b','c',1,2,3]
x1 = x[:]
print x1
---
Feedback incorrect:
copy(x) is the recommended practice for copying objects, but it requires you to first import the copy function from the standard Python library. You do this by saying “from copy import copy”.
from copy import copy
x = ['a','b','c',1,2,3]
x1 = copy(x)
print x1

<-- 5.4 Quiz -->
Some Python commands 
Question 7
How can you create a copy of dictionary z?
z.copy()
z{:}
z.makecopy()
---
Correct answer(s):
z.copy()
---
Feedback correct:
from copy import copy
z = {1: 'hello', 2: 'world', '3': 'answer', 'a': 'is', 'this': 42}
z1 = z.copy()
print z1
(You don’t need the first line if  you’ve already imported copy)

<-- 5.4 Quiz -->
Some Python commands 
Question 8
Which of these is an if-statement that checks whether the numeric value 12 is present in a list x?
Maybe you’ll just have to guess!
if x contains 12:
if x.has(12):
if 12 in x:
---
Correct answer(s):
if 12 in x:
---
Feedback correct:
x = ['a','b','c',1,2,3]
if 3 in x:
    print("3 is there!")
if 12 in x:
    print("cannot happen")
(Why doesn’t it print “cannot happen”?)

<-- 5.4 Quiz -->
Some Python commands 
Question 9
What’s the value of the Python expression 2**4 + 2**3 + (87 % 8)?
11
21
31
---
Correct answer(s):
31
---
Feedback correct:
x = 2**4 + 2**3 + (87 % 8)
print x

<-- 5.4 Quiz -->
Some Python commands 
Question 10
What options are required to configure the unsupervised Discretize filter to use 15 bins on the third attribute?
To find out, go to Weka’s Javadoc (in your Weka download, or here), scroll down the main list to find weka.filters.unsupervised.attribute, and within that look for Discretize.
–b 15 –a 3
–b 15 –r 3
–B 15 –C 3
–B 15 –R 3
–bin 15 –attr 3
---
Correct answer(s):
–B 15 –R 3
---
Feedback correct:
The order of B and R is irrelevant

<-- 5.4 Quiz -->
Some Python commands 
Question 11
In Python, Weka options are specified as lists of strings, and if an option has a value it is given as a separate string.
What is the option list for configuring the unsupervised AddNoise filter to add 5% noise to the third attribute?
”[–P, 5, –C, 3]”
[–P, 5, –C, 3]
[“–P 5”, “–C 3”]
[“–P”, “5”, “–C”, “3”]
---
Correct answer(s):

<-- 5.4 Quiz -->
Some Python commands 
Question 11
In Python, Weka options are specified as lists of strings, and if an option has a value it is given as a separate string.
What is the option list for configuring the unsupervised AddNoise filter to add 5% noise to the third attribute?
”[–P, 5, –C, 3]”
[–P, 5, –C, 3]
[“–P 5”, “–C 3”]
[“–P”, “5”, “–C”, “3”]
---
Correct answer(s):
[“–P”, “5”, “–C”, “3”]
---
Feedback correct:
The order of P and C is irrelevant

<-- 5.5 Video -->
Building models
Peter demonstrates writing three Python scripts for Weka using the J48 classifier, using the anneal dataset. The first builds a classifier and outputs the model, the second evaluates a classifier and outputs summary statistics and a confusion matrix, and the third builds a classifier and makes predictions on an unlabeled dataset.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
What we’re going to cover in this lesson is building models and evaluating them. The classes that we’re going to touch upon in this lesson are weka.classifiers.Evaluation for evaluating classifiers; some classifiers, filters we’ve already seen in the last lesson; and some randomization stuff where we’re going to use some Java classes. The first thing that we want to do is build a J48 classifier. I’m going to start up our Jython console again. For this script, we’ll load some data, configure the J48 classifier, build it and output the model.
First of all, the imports: once again, our DataSource for loading data and our J48 classifier. Once again, we’re going to load our data using our environment variable, this time we’re loading the anneal UCI dataset; and, since it’s classification, we also have to set the class attribute. In this case, it’s the last one. So with numAttributes you can determine the number of attributes in a dataset, and with setClassIndex you can set which of these attributes is going to be the class attribute. However, since it’s an API, usually start counting at 0, not 1. That’s why we have numAttributes()–1.Next thing is I’m going to instantiate the J48 classifier, and we’re going to set some options.
In this case, we’re changing the confidence factor from the default value of 0.25 to 0.3. With the data available now and the classifier configured, we can build it, which simply happens with a buildClassifier call supplying the data. Then, as a final step, we’re outputting the model with a simple print statement. We run that. We can see the model that is being output after it's built from the data. As a next step, we want to evaluate a model that we’ve built. In this case, we’re going to use cross-validation, because there’s no point in building a model if you don’t actually know if it’s any good. I’m going to open a new tab and import some more stuff again.
In this case, we also need the Evaluation class and, since we’re cross-validating we also want to randomize the data. In that case, we’re importing the Random class. Just like before, we’re loading the anneal UCI dataset, setting the class attribute. Then we’re configuring the same classifier again; confidence factor once again 0.3. And then we’re setting up our evaluation. First of all, we’re initializing our Evaluation object with the current data in order to obtain the class priors.
Then we’re calling the crossValidateModel method of the Evaluation object with the classifier template not built; the data that we want to evaluate on; the number of folds (in our case, we’re doing 10-fold cross-validation); and a random number generator initialized with a seed value of 1.After that finishes, we’ll have basically all the statistics inside the Evaluation object, and we want to output some things. First thing, we want to output some summary statistics. There’s the so-called toSummaryString method. If you look at the Javadoc, you’ll realize there’s actually several methods, one with no attribute, one with a Boolean attribute, and one with String and Boolean attributes, like we’re using here.
Now the difference between Python and Java is that Python doesn’t have polymorphism; it has optional parameters, and named parameters. So in order for Jython to work, you basically have one method that has all the various parameters available. In this case, we’ll have to provide a title for basically our summary string and that we don’t want to output any complexity statistics, hence False. That is that. Since this is classification, we also want to output the confusion matrix, which you can do with the toMatrixString.
When we’re running this script, you’ll see in the output our usual summary statistics of accuracy, what’s missed, kappa statistic, all kinds of errors, coverage, and how many instances there were all together in the dataset – almost 900 in the anneal dataset case. The confusion matrix was also output. You can see there are hardly any instances that are not on the diagonal. According to our misclassified ones, it should be only 14. So we have 3 here, 2 there, 2 there, and 7 there, which is 14, so all is good. The final script that we want to do in this lesson is how we can actually use a built model to make predictions. I’m going to open up a new tab again.
In this case, like in the first script, we are importing our DataSource for loading data and our J48 classifier. We are once again loading a dataset. In this case, we’re not using the usual anneal dataset but one that’s been stripped down a bit, the anneal_train set. But still, the class attribute is in the same location, so it’s the last one. Setting that. We are once again configuring our J48 classifier, because we were happy with that configuration, based on our cross-validation results – it resulted in excellent results. Then we are building our classifier on the data, once again using the buildClassifier method, and since we want to make predictions on unlabeled data, we are now loading the unlabeled data in.
In this case, dataset anneal_unlbl, which basically has the same dataset structure, but just “missing values” for the class. We also set the class attribute for this one. It’s usually recommended that you compare your training and test/unlabeled data, whether they are actually compatible. You can use a method of the Instances class called equalHeadersMessage for telling whether two datasets are the same. If you look at this code here, the unlabeled data is checked against the training data, and this will return a message, but only in the case where they are different – for instance, different number of attributes, different types, or different order of labels. Then it will output a message.
Otherwise it will just output “None” or, in the Java case, “null”. In case we have a discrepancy between our datasets, then this will be output simply saying that they are not the same. And for making our predictions finally, since we now have our unlabeled data and our built model, we just iterate through our unlabeled data row by row, and then we obtain our class distribution by calling the distributionForInstance method. We want to know what the chosen class label is, so we’re using the classifyInstance method, which returns, in the case of a nominal class attribute, the label index (starting with 0).
In order to determine what the string label actually is, we use the dataset, retrieve the class attribute, and then determine the string value that is associated with that particular index. To output anything, we are then outputting with a simple print statement our class distribution, our label index, and the associated label.Running that, we get an output like this. First you get an array, which is the class distribution; then the index of the label; and the label itself; all separated by hyphens. At the bottom, you can see you have 1, 2, 3, 4, 5, 6 labels all together there, so index 5, and the label is U in this case.
So what we’ve learned in this lesson is how to build a classifier. We can output statistics from cross-validation that we’ve obtained from a classifier on a particular dataset, and we also used a built model to actually make predictions on new, unlabeled data.
<End Transcript>

<-- 5.6 Article -->
Datasets and scripts
Here are the datasets and scripts used in the preceding video.
The anneal dataset:
  anneal.arff (the full dataset)
  anneal_train.arff (used in the third script for training)
  anneal_unlbl.arff (used in the third script for making predictions)
Script for building a classifier and outputting the model:
  build_classifier.py
Script for evaluating a classifier and outputting summary statistics and a confusion matrix:
  crossvalidate_classifier.py
Script for building a classifier and making predictions on an unlabeled dataset:
  make-predictions_classifier.py

<-- 5.7 Quiz -->
Using Python scripts 
Question 1
With the build_classifier.py script in the Jython window, edit anneal.arff in the script to a nonexistent file and run again.
You will get an error. What line is it on?
Line 7
Line 8
Line 11
---
Correct answer(s):
Line 8
---
Feedback correct:
The error message is
Traceback (most recent call last):
  File "/Users/ihw/untitled", line 8, in <module>
AttributeError: 'NoneType' object has no attribute 'setClassIndex'

<-- 5.7 Quiz -->
Using Python scripts 
Question 2
It would be more elegant to print an appropriate message if the file is missing.
You can do this by replacing the line
data = DS.read(os.environ.get("MOOC_DATA") + os.sep + "anneal.arff")
with this:
dataset = os.environ.get("MOOC_DATA") + os.sep + "anneal.arff"
if ??:
  print("File does not exist: " + dataset)
else:
  data = DS.read(dataset)
and indenting the rest of the program. What should you put for the “??”?
dataset is None:
not os.path.exists(dataset)
os.path.missing(dataset)
---
Correct answer(s):
not os.path.exists(dataset)
---
Feedback correct:
You can read about the os.path module and its methods in the Python documentation

<-- 5.7 Quiz -->
Using Python scripts 
Question 3
Restore the original version of build_classifier.py and modify it to output a Naive Bayes model with default options for the iris.arff dataset.
How many code lines did you have to change?
2
3
4
5
---
Correct answer(s):
4
---
Feedback correct:
You should
  change an “import” line to import bayes.NaiveBayes rather than trees.J48
  change the filename anneal.arff to iris.arff
  change cls = J48() to use the NaiveBayes classifier
  delete the setOptions line

<-- 5.7 Quiz -->
Using Python scripts 
Question 4
Modify the program you have just created to print 5-fold cross-validation statistics (not the confusion matrix) for the same classifier (Naive Bayes) on the same dataset (iris.arff). (Hint: Examine crossvalidate_classifier.py to see how to make the change.)
How many import lines and how many code lines did you have to add, and how many code lines did you delete?
1 import line added, 3 code lines added, 3 code lines deleted
2 import lines added, 3 code lines added, 0 code lines deleted
2 import lines added, 3 code lines added, 2 code lines deleted
2 import lines added, 4 code lines added, 2 code lines deleted
---
Correct answer(s):
2 import lines added, 3 code lines added, 2 code lines deleted
---
Feedback correct:
  import weka.classifiers.Evaluation and java.util.Random (2 import lines added)
  initialize the evaluation, execute it, and print cross-validation statistics (3 code lines added)
  delete the buildClassifier line and the line that prints the model (2 lines deleted)
---
Feedback incorrect:
Close. But you should delete the buildClassifier line and the line that prints the model

<-- 5.7 Quiz -->
Using Python scripts 
Question 5
Modify the program you have just created to filter the data by discretizing the first attribute (only) using an unsupervised Discretize filter with 5 bins (and default parameters otherwise). (Hint: Examine the load_and_filter_data.py script supplied with the previous lesson.)
What is the percentage of correctly classified instances?
94.7%
95.3%
96.0%
96.7%
---
Correct answer(s):
96.7%
---
Feedback incorrect:
Are you sure you’re using 5-fold cross-validation rather than the default of 10-fold?
---
Feedback incorrect:
Are you sure you’re using 5 bins rather than the default of 10?
---
Feedback incorrect:
Are you sure you applied the filter?

<-- 5.7 Quiz -->
Using Python scripts 
Question 6
The remaining activities are for people who intend to actually use Python for scripting. You can learn from the scripts supplied with this lesson, and from the Weka Javadoc.
Examine the Python scripts to determine which of the following is a helper class used for loading data:
weka.core.converters.ConverterUtils.DataSource
weka.core.converters.LoadDataFromFile
weka.core.LoadData
---
Correct answer(s):
weka.core.converters.ConverterUtils.DataSource
---
Feedback correct:
It’s used in all the scripts

<-- 5.7 Quiz -->
Using Python scripts 
Question 7
Examine the Python scripts to determine how to retrieve the content of environment variable MY_DATA:
os.env[“MY_DATA”]
os.env.get(“MY_DATA”)
os.environ.get(“MY_DATA”)
---
Correct answer(s):
os.environ.get(“MY_DATA”)
---
Feedback correct:
It’s used in all the scripts.
You can read about the os module’s environ mapping object in the Python documentation

<-- 5.7 Quiz -->
Using Python scripts 
Question 8
Which of these methods is used for building a classifier?
build
buildClassifier
make
makeClassifier
---
Correct answer(s):
buildClassifier
---
Feedback correct:
It’s used in the build_classifier.py script

<-- 5.7 Quiz -->
Using Python scripts 
Question 9
How do you set the 3rd attribute to be the class attribute?
(Hint: examine the make-predictions_classifier.py script. Note that the index counts from 0.)
data.setClassIndex(2)
data.setClassIndex(3)
data.setClassIndex(data.numAttributes() - 3)
---
Correct answer(s):
data.setClassIndex(2)
---
Feedback incorrect:
(The index counts from 0.)

<-- 5.7 Quiz -->
Using Python scripts 
Question 10
Which of these methods returns the cluster an instance belongs to?
(Hint: examine the make-predictions_classifier.py script to determine what method is called to classify an instance, and work by analogy.)
buildClusterer
classifyInstance
clusterInstance
---
Correct answer(s):
clusterInstance
---
Feedback correct:
make-predictions_classifier.py contains the line
      labelIndex = cls.classifyInstance(inst)
and clusterInstance is the obvious analogy for clustering.
---
Feedback incorrect:
make-predictions_classifier.py contains the line
      labelIndex = cls.classifyInstance(inst)
and clusterInstance is the obvious analogy for clustering.
---
Feedback incorrect:
make-predictions_classifier.py contains the line
      labelIndex = cls.classifyInstance(inst)
and clusterInstance is the obvious analogy for clustering.
You are asked to return a cluster, not classify the instance

<-- 5.7 Quiz -->
Using Python scripts 
Question 11
Use Weka’s Javadoc to determine how to stratify a dataset data of type weka.core.Instances for 10-fold cross-validation.
data.stratify(10)
data.stratify(9)
data.stratify()
---
Correct answer(s):
data.stratify(10)
---
Feedback correct:
In the main (Overview) panel of the Javadoc: click weka.core; find Instances in the list of core classes; click it; and search for stratify to locate the method stratify(int numfolds).
Or just go here :-)

<-- 5.7 Quiz -->
Using Python scripts 
Question 12
How do you extract the directory from a file name fname?
(Hint: find an os.environ.get line in one of the scripts supplied, and experiment with the choices below.)
os.getdir(fname)
os.path.basename(fname)
os.path.dirname(fname)
---
Correct answer(s):
os.path.dirname(fname)
---
Feedback correct:
I extracted a couple of lines from load_and_filter_data.py and modified them to read
    import os
    x = os.environ.get("MOOC_DATA")
    print os.path.dirname(x)
You can read about the os.path module and its methods in the Python documentation.

<-- 5.7 Quiz -->
Using Python scripts 
Question 13
What does a classifier’s classifyInstance method returns in the case of a nominal class attribute?
(Hint: Examine the make-predictions_classifier.py script.)
Class label index
Class label string
---
Correct answer(s):
Class label index
---
Feedback correct:
The line
          labelIndex = cls.classifyInstance(inst)
in make-predictions_classifier.py is a bit of a giveaway!

<-- 5.7 Quiz -->
Using Python scripts 
Question 14
Look at Weka’s Javadoc to determine the equivalent of the FilteredClassifier meta-classifier for clusterers.
weka.clusterer.FilteredClusterer
weka.clusterer.MetaClusterer
weka.clusterers.FilteredClusterer
---
Correct answer(s):
weka.clusterers.FilteredClusterer
---
Feedback correct:
It’s here

<-- 5.7 Quiz -->
Using Python scripts 
Question 14
Look at Weka’s Javadoc to determine the equivalent of the FilteredClassifier meta-classifier for clusterers.
weka.clusterer.FilteredClusterer
weka.clusterer.MetaClusterer
weka.clusterers.FilteredClusterer
---
Correct answer(s):

<-- 5.8 Video -->
Visualization
Peter shows how to create visualizations from Weka’s Jython console using the open source library JfreeChart. First he plots the errors made by LinearRegression on a dataset, indicating the size of each error by the radius of a bubble. Next he displays multiple ROC curves, one for each class in the dataset. Then he draws a tree generated by J48, and finally a graph generated by the BayesNet classifier.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
We’ll be using JFreeChart for some of the plotting, because Weka’s plotting is a little bit complicated and it’s much, much nicer doing JFreeChart plots. If you haven’t done already, please install the jfreechartOffscreenRenderer package, which I already mentioned earlier, and, if you’re looking for more Javadoc on the JFreeChart library you can do that on the jfree.org website. The classes that we’ll be touching on for JFreeChart will be datasets that JFreeChart needs for plotting; ChartFactory for creating plots; and ChartPanel, which is actually used for embedding plots in the GUI. And, finally, some Weka classes for displaying trees and graphs. First of all I’m going to start up Weka in the Jython console.
For the first script, we’ll want to plot the classifier errors obtained from a linear regression regressor on a dataset and plot these. But not just actual versus predicted, but also take into account how bad the error is. So first thing, we’re going to import a whole bunch of classes
again: Evaluation for evaluating our classifier; we’re going to use LinearRegression as a simple classifier for doing the regression; DataSource for the usual loading of the dataset. DefaultXYZDataset is a JFreeChart dataset, which allows you to store 3 dimensions for each data point. We’re basically using the z as the error. ChartFactory class for generating the plot; ChartPanel for embedding it; and the BubbleRenderer basically plots a bubble at the x, y position using the z value as the radius. OK. So we’re loading our data. In this case, it’s a numeric class in the bodyfat UCI dataset. Then we are configuring our LinearRegression classifier, turning off some bits we don’t need. It also makes it a bit faster.
Once again we are cross-validating our classifier with 10-fold cross-validation. And after the cross-validation is done, we need to collect the predictions and need to compute the error. So what we’re going to do here is quite simple. We’re going to start with three empty lists, the actual, the predicted and the error. We’re going to look through all the predictions, which we can retrieve by the predictions method, and retrieve those predictions; store the actual and predicted; and calculate the error, which is basically actual minus predicted, and the absolute value of that. Having done that, we can then create our dataset, which is a DefaultXYZDataset.
We are adding a series to this dataset, which we simply give it a name, like LinearRegression on the name of the dataset, with the actual, predicted, and error. Then we’re using our ChartFactory to create a plot, in this case a scatter plot, with the title “Actual” and “Predicted” as the axis titles. As a renderer, since we not only want to plot a little dot at that location x and y, we use a specific renderer, XYBubbleRenderer. Then we are simply embedding the whole thing in the frame and displaying that. Let’s run that. Here we go.
As we can see, some of the outliers are quite large, and the ones that are closest to the diagonal – the optimal case – are the smallest ones. We can even zoom in if we want to, and it adjusts accordingly. The next script handles ROC curves for classification, because the area under the curve and how the curves for the various class labels are is actually telling you an important story about how well your classifier’s doing.
In this case, once again: new tab; and we’re going to import a whole lot of classes again. In this case we’re evaluating NaiveBayes, and we’re using the ThresholdCurve class from Weka, which allows us to calculate the ROC curve data, among other things. Since we’re only plotting x, y in this case, we don’t need an XYZDataset; just an XY one will do. Once again ChartFactory and so on, which we’ve already seen in the other one. Now, once again, we’ll load a dataset. In this case, we’re loading the balance-scale UCI dataset, which has a nominal class.
Setting the class attribute to the last one again; instantiating our NaiveBayes classifier (no options to be set); and cross-validating that once again with 10-fold cross-validation to obtain the statistics. We’re creating our dataset again, and since we want to plot the ROC curves for all the class labels we’re going to have to look through all the labels. So what we’re going to do here is we’re going to have a variable which is going to range from 0 to the number of values minus 1 that the class attribute has.
In each case, we’re going to create the threshold curve data, so we instantiate a ThresholdCurve object, and then use the predictions of the Evaluation class and the current index of the label that we’re interested in, and create curve data from that. We can simply extract those columns of data from the dataset curve that was generated, and put that into a list. We’re looking at the False Positive Rate versus the True Positive Rate that we want to plot. Then, since we already have a dataset, we’re adding a plot series to it and, to make it a bit more interesting, we’re also calculating the ROC curve for each of the class labels and using that as the label for the plot.
Now we’re creating an XY line plot, because we’re connecting the dots rather than just dotting them around like it was with the bubble plot earlier. Put the titles for the axes down, False Positive Rate and True Positive Rate. Then, once again, put that in a frame and display it. Let’s run that, and we have our three class labels L, B, and R. As you can see, the blue line is the worst one; and if you look it up, it also only has an ROC of 0.719, whereas the other ones have almost 1. As, you can see, they go straight up and really nestle quite nicely in the corner here, and then plateauing out at pretty much 1 there.
So that looks pretty sweet. This was using JFreeChart to plot some graphs. However, we can also plot some data using simple Weka classes. In this case, we want to plot a tree that got generated by J48. Once again we import stuff, and for visualization we’re going to use the TreeVisualizer. First of all, once again we have to load some data in, in this case the iris dataset. We’re going to build an unpruned J48 tree, build it on the dataset, and then we’re creating a TreeVisualizer using the graph that the built classifier returns.
Then we’re embedding the whole thing in a frame, visualizing that, and once the frame has been displayed we can also fit the tree then to the size that’s on the screen. Running that. We have our nice little tree of the iris dataset. Now, trees aren’t the only thing that Weka can plot. The BayesNet classifier allows you to plot network graphs and this is what we’re going to do now. In this case, we’re going to use the BayesNet classifier and the GraphVisualizer from Weka to plot a graph that this classifier generates. Once again, load the iris dataset, and we’re going to configure our BayesNet classifier.
To make the graph a little bit more interesting I’m using two parents rather than just one. I am building the classifier, and then I’m initializing the GraphVisualizer using the graph that the classifier returned. In this
case, it’s in the BIF or Bayesian Network Interchange Format (http://www.cs.cmu.edu/~fgcozman/Research/InterchangeFormat/). Once again, we embed the whole thing in a frame, display that, and just like with a TreeVisualizer, we also want to make sure that the layout is all right. Let’s run this, and we have our little network graph. If we click on the various nodes, we can then see the probability tables. We can inspect it further.
<End Transcript>

<-- 5.9 Article -->
Datasets and scripts
Here are the datasets and scripts used in the preceding video.
Datasets:
  balance-scale.arff
  bodyfat.arff
Plot the errors made by LinearRegression:
  crossvalidate_classifier-error-bubbles.py
Display multiple ROC curves, one for each class:
  display_roc-multiple.py
Draw a tree generated by J48:
  display_tree.py
Draw a graph generated by the BayesNet classifier:
  display_graph.py

<-- 5.10 Quiz -->
Showing bubble plots and ROC curves 
Question 1
Change the classifier from functions.LinearRegression to rules.ZeroR with default options (remove the setOptions line).
How do the bubbles compare with the ones generated previously for linear regression?
much the same, but all smaller
similar, but in different places
large bubbles are much the same, but small ones are all smaller
they don’t look like bubbles at all!
---
Correct answer(s):
they don’t look like bubbles at all!
---
Feedback correct:
Pretty well the whole screen is covered by the bubble color.

<-- 5.10 Quiz -->
Showing bubble plots and ROC curves 
Question 2
Change the classifier to functions.SMOreg (a version of SMO that copes with numeric class values), again with default options.
How do the bubbles compare with the ones for linear regression?
much the same, but all smaller
similar, but in different places
large bubbles are much the same, but small ones are all smaller
they don’t look like bubbles at all
---
Correct answer(s):
large bubbles are much the same, but small ones are all smaller

<-- 5.10 Quiz -->
Showing bubble plots and ROC curves 
Question 3
Load the display_roc-multiple.py script into the Jython console and execute it to get the ROC plot that you saw in the video (you will need to put the balance-scale.arff data file into your MOOC_DATA folder).
There are three graphs, red, blue, and green (the green line is largely obscured by the red one), and the legend at the bottom labels them as L, B, and R respectively.
What do these three different ROC curves correspond to?
B and R stands for blue and red, and L for Lime green
different class values in the data file
different learning algorithms
different parameters for the Naive Bayes algorithm
different thresholds used to calculate the ROC
---
Correct answer(s):
different class values in the data file
---
Feedback correct:
Loading balance-scale.arff into the Explorer shows that the class values are L, B, and R (corresponding to “left”, “balanced”, and “right”).

<-- 5.10 Quiz -->
Showing bubble plots and ROC curves 
Question 4
Change the Python script to use breast-cancer.arff and run it.
You will see two curves rather than three.
Now change the program to plot a curve just for the single value classIndex = 0. (When ROC curves are used for 2-class data, usually the curve for just one class is shown.)
Ignoring changes in indentation, how many lines of code do you have to add, delete, and change (there are 3 correct answers, so for a fully correct answer you must click 3 boxes)?
Select all the answers you think are correct.
add 0, delete 0, change 1
add 0, delete 1, change 2
add 0, delete 2, change 1
add 1, delete 1, change 0
add 1, delete 4, change 2
---
Correct answer(s):
add 0, delete 0, change 1
add 0, delete 1, change 2
add 1, delete 1, change 0
---
Feedback correct:
You could change the for classIndex … line to for classIndex in [0]:
You could delete the for classIndex … line and change classIndex to 0 in two places. (You must also remove the indentation from the lines that were previously within the for-loop.)
You could delete the for classIndex … line and add a line classIndex = 0.  (You must also remove the indentation from the lines that were previously within the for-loop.)
---
Feedback incorrect:
You could change the for classIndex … line to for classIndex in [0]:
---
Feedback incorrect:
You could delete the for classIndex … line and change classIndex to 0 in two places. (You must also remove the indentation from the lines that were previously within the for-loop.)
---
Feedback incorrect:
You could delete the for classIndex … line and add a line classIndex = 0.  (You must also remove the indentation from the lines that were previously within the for-loop.)

<-- 5.10 Quiz -->
Showing bubble plots and ROC curves 
Question 5
The value of True Positive Rate continually increases.
At approximately what value of False Positive Rate does it cross the 50% mark?
0.15
0.19
0.23
0.25
---
Correct answer(s):
0.23
---
Feedback incorrect:
That’s the value you would get for the ROC curve with classIndex = 1

<-- 5.10 Quiz -->
Showing bubble plots and ROC curves 
Question 6
Change your program to make it print “Naive Bayes” in the legend at the bottom.
Now add code to also plot a similar ROC curve for the J48 classifier, suitably labeled.
Which of these statements best describes the relationship between the two curves?
The J48 line is always above the Naive Bayes one because J48 performs better on this data
The J48 line is always below the Naive Bayes one because J48 performs worse on this data
The J48 line starts above the Naive Bayes one and crosses below it at a False Positive Rate of about 0.5, rising above it again at about 0.9
The J48 line starts below the Naive Bayes one and crosses it at a False Positive Rate of about 0.7, falling below it again at about 0.9
---
Correct answer(s):
The J48 line starts below the Naive Bayes one and crosses it at a False Positive Rate of about 0.7, falling below it again at about 0.9

<-- 5.10 Quiz -->
Showing bubble plots and ROC curves 
Question 7
Answer these questions by examining the Python scripts supplied with this lesson.
What class is used for cross-validating a classifier?
weka.evaluations.Evaluation
weka.classifiers.Evaluation
weka.core.Evaluation
---
Correct answer(s):
weka.classifiers.Evaluation
---
Feedback correct:
This class is imported by crossvalidate_classifier.py and display_roc-multiple.py

<-- 5.10 Quiz -->
Showing bubble plots and ROC curves 
Question 8
How do you make a scatter plot with JFreeChart?
ChartFactory.createScatterPlot(…)
ChartFactory.makeScatterPlot(…)
ChartFactory.scatterPlot(…)
new ScatterPlot()
---
Correct answer(s):
ChartFactory.createScatterPlot(…)
---
Feedback correct:
This is used in crossvalidate_classifier-error-bubbles.py

<-- 5.10 Quiz -->
Showing bubble plots and ROC curves 
Question 9
How do you calculate the error in a numeric prediction to be used as size for a bubble plot?
abs( pred.actual() – pred.predicted() )
pred.error()
pred.predicted() * pred.weight()
---
Correct answer(s):
abs( pred.actual() – pred.predicted() )
---
Feedback correct:
This is used in crossvalidate_classifier-error-bubbles.py
---
Feedback incorrect:
Note that pred.error() can be negative!

<-- 5.10 Quiz -->
Showing bubble plots and ROC curves 
Question 10
What class is used to calculate the data for receiver operating characteristic (ROC) curves?
weka.classifiers.evaluation.CostCurve
weka.classifiers.evaluation.MarginCurve
weka.classifiers.evaluation.ThresholdCurve
---
Correct answer(s):
weka.classifiers.evaluation.ThresholdCurve
---
Feedback correct:
This is used in display_roc-multiple.py

<-- 5.10 Quiz -->
Showing bubble plots and ROC curves 
Question 11
What is plotted for an ROC curve?
false negatives against true negatives
false negative rate against true negative rate
false positives against true positives
false positive rate against true positive rate
---
Correct answer(s):
false positive rate against true positive rate
---
Feedback correct:
You can see this in display_roc-multiple.py

<-- 5.10 Quiz -->
Showing bubble plots and ROC curves 
Question 12
How do you calculate the area under a ROC curve?
ThresholdCurve.getAUCArea
ThresholdCurve.getPRCArea
ThresholdCurve.getROCArea
---
Correct answer(s):
ThresholdCurve.getROCArea
---
Feedback correct:
This is used in display_roc-multiple.py. ROC curves are explained in the course More Data Mining with Weka.
---
Feedback incorrect:
This method is used for calculating the area under the precision-recall curve, not the ROC curve

<-- 5.10 Quiz -->
Showing bubble plots and ROC curves 
Question 13
In what format is the graph of a BayesNet network output?
dot
json
XMLBIF
---
Correct answer(s):

<-- 5.10 Quiz -->
Showing bubble plots and ROC curves 
Question 13
In what format is the graph of a BayesNet network output?
dot
json
XMLBIF
---
Correct answer(s):
XMLBIF
---
Feedback correct:
readBIF() is used in display_graph.py

<-- 5.11 Video -->
Invoking Weka from Python
So far, we’ve been using Python from within Weka. However, in this lesson we work the other way round and invoke Weka from within Python. This allows you to take advantage of the numerous program libraries that Python has to offer. You need to install Python, and then the python-weka-wrapper library for Python. Having set this up, we replicate some scripts from earlier lessons.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
So far, we’ve been using Python from within the Java Virtual Machine. However, in this lesson, we’re going to invoke Weka from within Python. But you might ask, “why the other way? Isn’t it enough using Jython?” Well, yes and no. Jython limits you to pure Python code and to Java libraries, and Weka provides only modeling and some limited visualization. However, Python has so much more to offer. For example, NumPy, a library of efficient arrays and matrices; SciPy, for linear algebra, optimization, and integration; matplotlib, a great plotting library. You can check all this out on the Python wiki under Numeric and Scientific libraries. So what do we need?
Well, first of all we need to install Python 2.7, which you can download from python.org. But make sure the Java that you’ve got installed on your machine and Python have the same bit-ness. So they’re either 32bit or 64bit. You cannot mix things. You have to set up an environment that you can actually compile some libraries. On Linux, that’s an absolute no-brainer. A few lines on the command line and you’re done within 5 minutes. However, OSX and Windows have quite a bit of work involved, so it’s not necessarily for the faint-hearted.
You can install the python-weka-wrapper library, which we’re going to use in today’s lesson, and you’ll find that and some instructions on how to install it on the various platforms on that page. Good luck with that. I’ve got it already installed, so I’m going to talk a bit more about what the python-weka-wrapper actually is. This library fires up a Java Virtual Machine in the background and communicates with the JVM via Java Native Interface. It uses the javabridge library for doing that, and the python-weka-wrapper library sits on top of that and provides a thin wrapper around Weka’s superclasses, like classifiers, filters, clusterers, and so on.
And, in difference to the Jython code that we’ve seen so far, it provides a more “pythonic” API. Here are some examples. Python properties are, for example, used instead of the Java get/set-method pairs. For example, options instead of getOptions/setOptions. It uses lowercase plus underscore instead of Java’s camel case, crossvalidate_model instead of crossValidateModel. It also has some convenience methods that Weka doesn’t have, for example data.class_is_last() instead of data.setClassIndex(data.numAttributes()–1). And plotting is done via matplotlib. Right. So I presume you were lucky installing everything, and you’ve sorted everything out. I’ve already done that on my machine here because it takes way too long, and I’m going to fire up the interactive Python interpreter.
For the first script, we want to revisit cross-validating a J48 classifier. As with all the other examples, we have to import some libraries. In this case, we’re communicating with the JVM, so we have to have some form of communicating with it and starting and stopping it, so we import the weka.core.jvm module. We want to load data, so we’re going to import the converters, and we’re importing Evaluation and Classifier. First of all, we’re going to start the JVM. In this case, using the packages as well is not strictly necessary, but we’ll just do it. You can see a lot of output here. It basically tells you what the libraries are in the classpath, which is all good.
Next thing is we’re going to load some data, in this case our anneal dataset, once again using the same approach that we’ve already done with Jython using the environment variable. That’s loaded. Then we’re going to set the class, which is the last one, and we’re going to configure our J48 classifier. Whereas in Jython we simply said “I want to have the J48 class”, we’re going to instantiate a Classifier object here and tell that class what Java class to use, which is our J48 classifier, and with what options. So the same confidence factor of 0.3.Once again, same thing for the Evaluation class.
We instantiate an Evaluation object with the training data to determine the priors, and then cross-validate the classifier on the data with 10-fold cross-validation. That’s done. And now we can also output our evaluation summary. This is simply with Evaluation.summary(...). The title, and we don’t want to have any complexity statistics being output, and since in our Jython example we also had the confusion matrix we’re going to output that as well. Here’s our confusion matrix. One thing you should never forget is, once you’re done, you also have to stop the JVM and shut it down properly. We can see once again like with the other one, we have 14 misclassified examples out of our almost 900 examples.
You can count those: 3, 2, 2, and 7, which is 14; here’s the confusion matrix as well. For the next script we’ll be plotting the classifier errors obtained from a LinearRegression classifier on a numeric dataset. Once again we’ll be using the errors between predicted and actual as the size of the bubbles. Once again I’m going to fire up the interactive Python interpreter. I’m going to import, as usual, a bunch of modules. In this case, new is the plotting module for classifiers I’m going to import here. We’ll start up our JVM. We’re loading our bodyfat dataset in, setting the class attribute. Then we’re going to configure our LinearRegression, once again turning off some bits that make it faster.
We’re going to evaluate it on our dataset with 10-fold cross-validation. Done. And now we can plot it with a single line. Of course, we’re cheating here a little bit, because the module does a lot of the heavy lifting, which we had to do with Jython manually. Here we go. Nice plot. Of course, you can also zoom in if you wanted to. Great. As a final step, stop the JVM again, and we can exit. The last script that we’re going to do in this lesson, we’ll be plotting multiple ROC curves, like we’ve done with Jython. Once again, the Python interpreter. It’s
a nice thing: we can just open it up and do stuff with it straight away. Import stuff. Once again we’re using a plotting module for classifiers. We are starting up the JVM; loading the balance-scale dataset like we did with Jython; and we also use the NaiveBayes classifier – as you can see, this time there are no options. Cross-validate the whole thing with 10-fold cross-validation. Then we use the plot_roc method to plot everything. We want to plot 0, 1, and 2 class label indices. Here we have those. Once again, we can see the AUC values for each of the labels, whether
it’s L, B, or R.Final step: stop the JVM again and exit.
<End Transcript>

<-- 5.12 Article -->
Do it yourself!
To set up Python with Weka you first need to install Python, and then install the python-weka-wrapper library for Python. You will probably need admin access to your computer for this.
Python-weka-wrapper comes in two versions, one for Python 2.7 and one for Python 3.x, so make sure that you install the appropriate version. You must also ensure that your Python and Java version have the same bitness, i.e., either 32 or 64 bits.
Java underwent quite some dramatic changes underneath the hood, starting with Java 9. To avoid any issues, you should use Java 8 with python-weka-wrapper.
Python 2.7
  Download and install Python 2.7 (installation is easy on Linux, but can be challenging on Windows and OSX)
  Download and install the python-weka-wrapper library
  Here are instructions for all this
Python 3.x
  Download and install Python 3.x (installation is easy on Linux, but can be challenging on Windows and OSX)
  Download and install the python-weka-wrapper3 library
  Here are instructions for all this
Data files:
  bodyfat.arff
  balance-scale.arff
Evaluate a classifier and output summary statistics and a confusion matrix:
  pww-crossvalidate_classifier.py
Plot the errors made by LinearRegression:
  pww-crossvalidate_classifier-errors-bubbles.py
Display multiple ROC curves, one for each class:
  pww-display_roc-multiple.py

<-- 5.13 Discussion -->
Share your experience ...
Peter said that setting up Python and arranging for it to access Weka is easy on Linux, but not so easy on Windows or OSX.
If you have done this, or tried to, please share your experiences with your fellow learners. Perhaps they can help!

<-- 5.14 Quiz -->
Investigating the python-weka-wrapper library 
Question 1
What library does the python-weka-wrapper use for plotting?
jFreeChart
matplotlib
Plotly
PyQtGraph
---
Correct answer(s):
matplotlib
---
Feedback correct:
This is mentioned in a comment in pww-display_roc-multiple.py when importing the weka.plot.classifiers module

<-- 5.14 Quiz -->
Investigating the python-weka-wrapper library 
Question 2
What python-weka-wrapper module is used to plot classifier errors generated by linear regression?
weka.plot.graph
weka.plot.classifiers
weka.plot.dataset
---
Correct answer(s):
weka.plot.classifiers
---
Feedback correct:
The script pww-crossvalidate_classifier-errors-bubbles.py uses the weka.plot.classifiers module

<-- 5.14 Quiz -->
Investigating the python-weka-wrapper library 
Question 3
The remaining questions are for people who intend to actually use the python-weka-wrapper.
Answer them by consulting the python-weka-wrapper home page and documentation, or by doing a Google search on the documentation site (include “site:pythonhosted.org/python-weka-wrapper/” in your search).
What Python library does the python-weka-wrapper use for communicating with the Java Virtual Machine (JVM)?
javabridge
jep
jpype
---
Correct answer(s):
javabridge
---
Feedback correct:
You can find this on the python-weka-wrapper home page

<-- 5.14 Quiz -->
Investigating the python-weka-wrapper library 
Question 4
How do you set command-line options for the classifier object cls in the python-weka-wrapper?
cls.options = “–A 42”
cls.options = [“–A”, “42”]
cls.setOptions(“–A 42”)
cls.setOptions([“–A”, “42”])
---
Correct answer(s):
cls.options = [“–A”, “42”]
---
Feedback correct:
You can discover this with a Google search for “option handling site:pythonhosted.org/python-weka-wrapper/”
---
Feedback incorrect:
This is what you would use in Jython, but not in the python-weka-wrapper

<-- 5.14 Quiz -->
Investigating the python-weka-wrapper library 
Question 5
What method of the python-weka-wrapper’s Evaluation class is used to test a built classifier on a dataset?
crossvalidate_model
evaluate_model
test_model
---
Correct answer(s):
test_model
---
Feedback correct:
You can discover this with a Google search for “test_model site:pythonhosted.org/python-weka-wrapper/”

<-- 5.14 Quiz -->
Investigating the python-weka-wrapper library 
Question 6
How do you start up the JVM with a custom heapsize of 512MB?
jvm.start(max_heap_size=”512m”)
You can’t
jvm.heap_size = “512m”
---
Correct answer(s):
jvm.start(max_heap_size=”512m”)
---
Feedback correct:
You can discover this with a Google search for “jvm.start site:pythonhosted.org/python-weka-wrapper/”

<-- 5.14 Quiz -->
Investigating the python-weka-wrapper library 
Question 7
Can you add your own jar files to the JVM that is used by the python-weka-wrapper?
Yes
No
---
Correct answer(s):
Yes
---
Feedback correct:
Here’s how to do it

<-- 5.14 Quiz -->
Investigating the python-weka-wrapper library 
Question 8
Which of the following does the distribution_for_instance method used for classifiers return?
Hint: Use the index link at the very right of the python-weka-wrapper documentation page to locate distribution_for_instance; you will have to do a further web search to determine what the “Return type” means.
dictionary
list
numpy array
---
Correct answer(s):
numpy array

<-- 5.14 Quiz -->
Investigating the python-weka-wrapper library 
Question 8
Which of the following does the distribution_for_instance method used for classifiers return?
Hint: Use the index link at the very right of the python-weka-wrapper documentation page to locate distribution_for_instance; you will have to do a further web search to determine what the “Return type” means.
dictionary
list
numpy array
---
Correct answer(s):

<-- 5.15 Video -->
A challenge, and some Groovy 
We begin by looking at a real-world challenge: the IDRC (International Diffuse Reflectance Conference) Shootout challenge. The training data – called “calibration data” – and test data is linked to below. The challenge is tough: in the step that follows the upcoming quiz Peter talks about his solution, and how he arrived at it. Then, as a second part, we look at another scripting language called “Groovy” and use it to replicate two of the earlier Python scripts.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
This lesson is slightly different to the other ones, because we will be looking at a real-world challenge, and then, as a second part, we’ll be looking at another scripting language called Groovy. The challenge is basically from the annual shoot-out from the Council for Near- Infrared Spectroscopy, and the shoot-out process basically works as follows. You build your data on the training data, which is called “calibration” in infrared spectroscopy terms. You evaluate your model on the separate dataset, which is the test dataset, and then you generate and submit predictions in the shoot-out process. However, we don’t do the last step of submitting our predictions, because that particular challenge has already finished.
But we’re still going to use the data that’s publicly available at the link below that you can download and then run. What are you going to do? Well, first, you’re going to download the CSV files for Dataset 1 and 2. I’m just going to go on their website here. Here’s Dataset 1 and 2. For each one of them, you download the CSV files, and you only need the calibration and the test set, we don’t need the validation one.
Then, you generate basically data for Weka in ARFF format: for building the model, calibration; and then for evaluating the model, the test set. The class attribute in the calibration dataset is called “reference value”, and you shouldn’t include the “sample #” in your model. This is now up to you to come up not only with the proper dataset and the compatible training and test set, but you should also then try and come up with a good regression scheme for predicting the reference value. But what do you have to beat? Well, in our case, you have to beat, on Dataset 1, a correlation coefficient of 0.8644 and a root mean squared error of 0.384.
And on Dataset 2 you have to beat a correlation coefficient of 0.9986 and a root mean squared error of 0.0026. Up for the challenge? Good luck!
Now to the second part: using Groovy, another scripting language. As I already mentioned in the introduction, Groovy also runs in the JVM and can be installed through the Package Manager, as well. If you haven’t done so already, please open up the Package Manager and install the kfGroovy package (it doesn’t matter what
version): “kf” for KnowledgeFlow Groovy. I’ve already done that, and I’m going to show you what the interface looks like. Once again, just like with the Jython console, you’ll find a Groovy console menu item under the Tools menu in the GUI Chooser. Once you’ve opened up that, you’ll find the appearance of the Groovy console very similar to that of the Jython console. On the top you write your script, and at the bottom you’ll see the output. However, in the Groovy console you cannot use multiple tabs, you have to open multiple instances; but for our purpose that is sufficient. Now before we start, just a few minor Groovy basics.
The grammar of Groovy is derived from Java, but with the exception that you don’t have to write any semicolons to finish a line, which makes it much nicer. Def, for definition, defines a variable; you don’t have to require any types or anything. Lists are very similar to the Python ones, square brackets and just comma- separated. There can be mixed types. Maps are also very similar to the Python ones, but they’re called “dictionaries” in Python. However, you don’t use curly brackets, you still use square brackets. Groovy also enhances the Java syntax. For example, you have multi-line strings by using triple single quotes. You can use string interpolation.
You can also have default import of commonly used packages, like java.lang, java.io, java.net, and so on. And, last but not least, closures. They are not quite the same as Java 8 lambdas, but they’re a very powerful tool. They’re basically anonymous code blocks, which can take parameters and return values as well, and can be assigned to variables. If you want to look up some differences between Java and Groovy, then follow the link. One really funky thing about Groovy that I very much like is looping.
Of course, you have the standard Java for-loop and while-loop, but you can also use – since everything is an object in Groovy – you can also use some additional methods called upto, times, and step, as long as you have number objects, like integers and so on. So if you look at upto, if you have 0.upto(10), that basically outputs all the numbers from 0 to 10, both included. If you do times, for example 5.times, that outputs the numbers from 0 to 5 with 5 excluded, so it outputs the numbers 0, 1, 2,3, 4. Last
but not least, you can also “step” through: if you have 0.step(10, 2), that means you’re going from 0 to 10 at step 2, so it outputs the numbers 0, 2, 4, 6, and 8.OK. So with the basics out of the way, we’re going to dive into writing one of the scripts we’ve already seen previously in Jython and python-weka-wrapper, and we’re going to make some predictions with a built classifier. Once again, as always, we’re going to have some imports, and, similar to Jython, we’re just going to import the whole classes. We once again do the trick with our environment variable; however, here we use System.getenv().
Then we load our training data, once again using the MOOC_DATA environment variable, using our shortcut variable, and loading the anneal_train dataset. Setting the class attribute once again as the last one, and we also load in the unlabeled data and set the class. Now we’re going to instantiate J48. We’re going to set some options. There’s a minor difference here to Jython. You actually have to specify that this is a string array. So even though you have a list of strings, you just have to say what you want to cast it to. And once again, build our classifier on the training data and output the built model, just for the fun of it.
Now we want to once again look at making predictions. First of all, we’re going to look at what labels we have. In this case, we’re going to use this previously mentioned .times that allows us for the number of values that the class attribute has, from 0 to times–1, and we’re basically adding the string label to it, which we can then also output with a simple println statement. We’re using the list’s join method, and we’re joining all those elements in the list with a comma, generating a comma-separated string. Once again, use our times method, but this time we’ll use
the number of instances in the dataset: for all the rows in the data we’ll call the classifier’s distributionForInstance in order to retrieve the class distribution. And then simply output what the class distribution is. And when we’re running this thing, you will first see that it actually loads the whole thing into JVM here on top. It just outputs; this is what we’re loading. Then after that you can see our J48 tree that we built on the training data. Then we have here our class labels, and finally the class distributions for all the rows in the data. Slightly different to Jython and python-weka-wrapper, but not too different.
As the second script, we’ll be looking at outputting multiple ROC curves on the balance-scale data. We’ll start from scratch. Once again, we have a bunch ofimports that we need. In this case, the Evaluation class again; we’re going to use NaiveBayes as the classifier again; ThresholdCurve, which allows us to compute the ROC curves and so on; DataSource for loading the data; and, once again we’re going to use JFreeChart for the plotting. First thing, we’re going to load the data in again using our environment variable, setting the class attribute as the last one. We’re going to instantiate NaiveBayes (there are no options necessary). Then we’re going to cross-validate it, after initializing an Evaluation object on the training data.
We’re going to do 10-fold cross-validation, and once again use a seed value of 1 for the random number generator. Having that done, we can now create our plot dataset once again. It’s just a simple XY dataset again, and, as you can see we’re going to use our .times again. So basically for all – since we want to do multiple – all the labels in the class. We can once again use the number of the labels that we have in the class attribute .times, and use the iterator once again to retrieve the curve data, then retrieve the data from the False Positive Rate column and the True Positive Rate column.
Turn it into lists, and we’re adding that as a data series to our plot dataset, including the AUC area. Having done that we can then create the plot, which is just an XY line chart, a simple one, with axes of False Positive Rate and True Positive Rate. As the last step, as usual, we’re going to create a frame, embed a chart panel with the plot and make the whole thing visible. And we run that. It takes a little while, and then we basically have our plot that we’ve already seen before. You’ve now seen quite a range of scripting languages that you can use on the Weka API, whether within the JVM or outside using Python itself.
And, last but not least, you also had some fun with a real world data challenge, and I hope you were much, much better than I was.
<End Transcript>

<-- 5.16 Article -->
Datasets and scripts
IDRC Shootout challenge:
  Information
  Dataset 1: calibration; test
  Dataset 2: calibration; test
Datasets:
  anneal.train.arff
  anneal.unlbl.arff
  balance-scale.arff
Build a classifier and make predictions on an unlabeled dataset:
  make_predictions-classifier.groovy
Display multiple ROC curves, one for each class:
  roc_multiple.groovy

<-- 5.17 Quiz -->
The data mining challenge
Question 1
How many instances are there in the calibration and test data for Datasets 1 and 2?
---
Correct answer(s):

<-- 5.17 Quiz -->
The data mining challenge
Question 2
Dataset 2 is tiny; it’s there just to help you set things up. Dataset1 is the real test.
Most of the attribute values in both datasets are spectral points. How many of these are there, for Datasets 1 and 2 respectively?
---
Correct answer(s):

<-- 5.17 Quiz -->
The data mining challenge
Question 3
There are an inordinate number of attributes in both datasets. Not surprisingly, this makes things difficult.
Prepare all four datasets one by one: load into the Weka Explorer; delete the Sample# attribute; move Reference Value, which is the class attribute, to the end (use the Reorder filter); rename it “reference value” (it is capitalized differently in the two datasets, which will become annoying if you use Python scripts), and save the result in ARFF format.
Load Dataset2_Cal into the Explorer and run LinearRegression, training on the Cal version and testing on the Test version. You will find it painfully slow with default parameters, even with few instances. The problem is the huge number of attributes. To make it feasible, set the attributeSelectionMethod in Linear Regression to No attribute selection and eliminateColinearAttributes to False. That will speed things up considerably.
What correlation coefficient does linear regression achieve on Dataset2?
0.968
0.986
0.998
0.999
1
---
Correct answer(s):
0.999
---
Feedback correct:
Weka reports a value of 0.9986.
---
Feedback incorrect:
That’s the value achieve when you train on the test set and evaluate using cross-validation
---
Feedback incorrect:
That’s the value achieved when you train on the test set and evaluate on the calibration set
---
Feedback incorrect:
That’s the value achieved with percentage split evaluation. You should evaluate on the test set.
---
Feedback incorrect:
That’s the value achieved when you test on the training set

<-- 5.17 Quiz -->
The data mining challenge
Question 4
What root mean square error does linear regression achieve on Dataset2?
0
0.002
0.0023
0.0026
0.0027
0.0082
---
Correct answer(s):
0.0026
---
Feedback incorrect:
That’s the value achieved when you test on the training set
---
Feedback incorrect:
That’s the mean absolute error, not the root mean square error
---
Feedback incorrect:
That’s the value achieved with 10-fold cross-validation. You should evaluate on the test set.
---
Feedback incorrect:
That’s the value achieved with percentage split evaluation. You should evaluate on the test set.
---
Feedback incorrect:
That’s the value achieve when you train on the test set and evaluate on the calibration set.

<-- 5.17 Quiz -->
The data mining challenge
Question 5
In the Explorer, select Visualize classifier errors from the right-click menu.
What does the graph look like?
Plot A
Plot B
Plot C
Plot D
---
Correct answer(s):
Plot C

<-- 5.17 Quiz -->
The data mining challenge
Question 6
Things are looking good for linear regression on Dataset2.
It achieves a low root mean square error and a high correlation coefficient, with a graph of predicted value versus actual value that lies satisfyingly close to the ideal diagonal line.
Now run linear regression on Dataset1, again training on Cal and testing on Test.
You may find that Weka simply stops without outputting any evaluation statistics, due to a “heap size” overflow. (You can find out what the problem is by starting Weka with the console window.) This can be solved by increasing the heap size. There are various ways of doing this. For example, most Java virtual machines support the environment variable _JAVA_OPTIONS (don’t forget the underscore at the start): set it to –Xmx2048m, which allocates a heap of 2Gb instead of the default 1Gb. On Windows, you can set environment variables by using the Windows search functionality to search for variables and selecting Edit environment variables for your account. Having set _JAVA_OPTIONS, re-start Weka.
What correlation coefficient and root mean square error does linear regression achieve on Dataset1?
0.25 and 0.49
0.95 and 0.37
0.99 and 0.01
---
Correct answer(s):
0.25 and 0.49
---
Feedback correct:
On various Java versions on Windows, Mac, and Linux we have found correlation coefficients of 0.2489, 0.2502 and 0.2567; and root mean square errors of 0.4855, 0.4920 and 0.4923. This is weird! It turns out that Java is not quite as portable as we thought, and small differences are sometimes encountered. However, this difference is rather large … perhaps because the problem is somehow ill-conditioned. Welcome to the real world!

<-- 5.17 Quiz -->
The data mining challenge
Question 7
Visualize the classifier errors again.
Which of the graphs in Q.5 does it resemble?
Plot A
Plot B
Plot C
Plot D
---
Correct answer(s):
Plot B

<-- 5.17 Quiz -->
The data mining challenge
Question 8
Things are looking bad for linear regression on Dataset1.
It gets a larger root mean square error and a far smaller correlation coefficient than for Dataset2, with a graph of predicted versus actual value that shows absurd errors for small actual values. Dataset2 is much easier to deal with than Dataset1. Unfortunately, this means it is of little use for quick experiments that might point to ways of getting good results for Dataset1.
Can you get a better result on Dataset1? You can try many things, though you might find them frustratingly (or impossibly) slow. Suggestion: how about IBk? – you could experiment with different values of k (KNN). Can you do better than this? You might want to try LWL (locally weighted learning), another instance-based method, which can be viewed as a generalization of IBk. However, Weka’s default KNN value for LWL is –1, which uses all the neighbors and is likely to be very slow.
Here’s a Python script for Dataset1 that was prepared by an expert data miner. Execute it in Weka’s Jython interface.
What correlation coefficient and root mean square error does it achieve?
0.25 and 0.49
0.87 and 0.38
0.950 and 0.37
0.99 and 0.01
---
Correct answer(s):
0.87 and 0.38
---
Feedback correct:
(Weka gives 0.8664 and 0.384)
---
Feedback incorrect:
These are the figures you got for linear regression. This script does far better than that.

<-- 5.17 Quiz -->
The data mining challenge
Question 9
Can you replicate the result of Q.8 in the Explorer?
Look at the classifier configuration string which appears in the Python output. You need to use Weka’s LWL (locally weighted learning) classifier with an appropriate KNN parameter value (–K 250); along with a LinearNNSearch (the default) with default parameters; and a GaussianProcesses classifier with an appropriate noise parameter (–L 0.01) and an RBFKernel with default parameters.
Visualize the classifier errors again.
Which of the graphs in Q.5 does this resemble?
Plot A
Plot B
Plot C
Plot D
---
Correct answer(s):
Plot D

<-- 5.17 Quiz -->
The data mining challenge
Question 9
Can you replicate the result of Q.8 in the Explorer?
Look at the classifier configuration string which appears in the Python output. You need to use Weka’s LWL (locally weighted learning) classifier with an appropriate KNN parameter value (–K 250); along with a LinearNNSearch (the default) with default parameters; and a GaussianProcesses classifier with an appropriate noise parameter (–L 0.01) and an RBFKernel with default parameters.
Visualize the classifier errors again.
Which of the graphs in Q.5 does this resemble?
Plot A
Plot B
Plot C
Plot D
---
Correct answer(s):

<-- 5.18 Article -->
The data mining challenge: An expert speaks
How might one come up with a solution like the one in the final question of the preceding Quiz? Here are some comments from Peter, our expert data miner, who wrote challenge.py.
I work on spectral data of soil samples (remember the last lesson of Week 1?) for a living, which has given me extensive experience in this area – and, of course, I chose this challenge! I looked at the rules on the IDRC 2014 Shootout home page and discovered that the dataset has been collected from round the globe, which suggests that you want to build local models from closely related data.
Therefore I used a locally weighted classifier,  LWL. Its default learning method is the decision stump, which is a very basic classifier – useless! In my experience Gaussian processes using the RBF kernel are usually quite good for spectral data. The only problem is that LWL is memory hungry, which is why I chose a smallish neighborhood of 150 instances. But – hey – you might be able to do better!

<-- 5.19 Discussion -->
The data mining challenge
Well, now you have seen a good solution to the data mining challenge, along with an explanation by an expert of how he arrived at it.
What do you think? Can you do better?

<-- 5.20 Quiz -->
Feelin' Groovy?
Question 1
The make_predictions-classifier.groovy script discussed in the previous video (A challenge, and some Groovy) does the same job as the make_predictions-classifier.py script discussed in the earlier Building models video: they both create a J48 model from anneal_train.arff and use it to obtain predictions for anneal_unlbl.arff.
But there are several differences. Examine both scripts; run them; and then answer the following questions.
They use the same classifier but with different parameters.
True
False
---
Correct answer(s):
False
---
Feedback correct:
They both use the J48 classifier with the parameter “–C 0.3”

<-- 5.20 Quiz -->
Feelin' Groovy?
Question 2
Which script prints the model?
Neither
Python
Groovy
Both
---
Correct answer(s):
Groovy
---
Feedback correct:
The model is printed by the line println(cls) in the Groovy script

<-- 5.20 Quiz -->
Feelin' Groovy?
Question 3
Which script prints the labels of the class attribute?
Neither
Python
Groovy
Both
---
Correct answer(s):
Groovy
---
Feedback correct:
The labels of the class attribute are printed by the line println(“#: “ + labels.join(“,”)) in the Groovy script

<-- 5.20 Quiz -->
Feelin' Groovy?
Question 4
Which script prints the prediction probabilities for each instance?
Neither
Python
Groovy
Both
---
Correct answer(s):
Both
---
Feedback correct:
The last code line of each script prints the prediction probabilities

<-- 5.20 Quiz -->
Feelin' Groovy?
Question 5
Let’s change the make_predictions-classifier.groovy script to print J48’s tree size for various values of the M parameter, which determines the minimum number of instances allowed at each leaf.
Delete the last few statements in the script to stop it printing class labels and predictions for the test set. Change the line println(cls) to println(cls.measureTreeSize()), which prints just the tree size (41 nodes). Change the setOptions part of make_predictions-classifier.groovy to set the M parameter instead of C.
Use this Groovy “for” loop
for (i in 1..10) {
 ...
}
to increase the value of the M parameter from 1 to 10, printing the tree size each time.
You’d expect the size to decay steadily as M increases. But it doesn’t. For what value does it rise?
2
3
4
5
6
7
---
Correct answer(s):
6
---
Feedback correct:
For M=6 it rises (to 42) for the first time, and decays again thereafter

<-- 5.20 Quiz -->
Feelin' Groovy?
Question 6
The remaining activities are for people who intend to actually use Groovy for scripting.
Try these Groovy statements to see which of them outputs 1, 3, 5.
0.step(6, 2) {println(it+1)}
1.step(5, 2) {println(it)}
---
Correct answer(s):
0.step(6, 2) {println(it+1)}
---
Feedback correct:
Now you can figure out what “step” does
---
Feedback incorrect:
This just prints 1 and 3

<-- 5.20 Quiz -->
Feelin' Groovy?
Question 7
Try printing the structures below.
Which one is not a valid Groovy list?
[1, 2, 3]
(1, 2, 3)
[1, “2”, 3]
---
Correct answer(s):
(1, 2, 3)
---
Feedback correct:
Try printing it with “println((1,2,3))”; you’ll get an error

<-- 5.20 Quiz -->
Feelin' Groovy?
Question 8
This is a Groovy “map”:
    m = [10: "me", 11: "you", 12: "others"]
Determine by experiment which of these expressions retrieves the value you.
m[11]
m[“11”]
m(11)
m(“11”)
---
Correct answer(s):
m[11]
---
Feedback correct:
Print it out:
    m = [10: "me", 11: "you", 12: "others"]
    println(m[11])

<-- 5.20 Quiz -->
Feelin' Groovy?
Question 9
Which of these statements iterates over all row indices in dataset data?
1.upto(data.numInstances()) {…}
data.numInstances().times {…}
---
Correct answer(s):
data.numInstances().times {…}
---
Feedback correct:
“.times” is used in both make_predictions-classifier.groovy and roc_multiple.groovy. Copy the first few lines of make_predictions-classifier.groovy (up to and including the line beginning data =), and append data.numInstances().times {println(it)}. The loop variable in Groovy is called “it”.

<-- 5.20 Quiz -->
Feelin' Groovy?
Question 10
How do you combine all elements in list a into a single string, using a comma as separator?
a.combine(“,”)
a.join(“,”)
a.toString(“,”)
---
Correct answer(s):
a.join(“,”)
---
Feedback correct:
It’s used in make_predictions-classifier.groovy.
Try
    println([1,2,3].join(","))

<-- 5.20 Quiz -->
Feelin' Groovy?
Question 10
How do you combine all elements in list a into a single string, using a comma as separator?
a.combine(“,”)
a.join(“,”)
a.toString(“,”)
---
Correct answer(s):

<-- 5.21 Video -->
Course summary 
We’ve covered a lot of ground in this course. Congratulations for getting this far, and double congratulations if you’ve managed to do all the Quizzes! I encourage you to keep learning; one good way is to challenge yourself by tackling data mining competitions. Kaggle offers many competitions, past and present, some with attractive prizes! Finally, a little word on ethics. I urge you to be ethical in your use of data mining, partly because there are good business reasons for doing things ethically, but mainly because I want to encourage you to have personal integrity and conduct yourself in an ethical way.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! It’s me, back again. I’ve probably got a bit of a tan since I saw you last. I’ve been out sailing around the coast of New Zealand in my beautiful yacht, Beulah – here she is – for a couple of weeks while the other guys have been recording the lessons. Anyway, I’m just back here to close out the course. We’ve done a lot in this course. We’ve covered a lot of ground, and you’ve learned a lot. Congratulations for getting this far, and double congratulations if you’ve managed to do all the activities [quizzes].This is what we’ve done in Advanced Data Mining with Weka. A couple of things have been missed out, multi-instance learning, and latent semantic analysis.
You’ll have to learn those yourself, I’m afraid. We didn’t do a lesson on one-class classification, but there was a good activity [quiz] on that. We’ve done some extra things. We’ve done some scripting in the Python and Groovy languages, and we’ve done some applications. The applications we’ve looked at have been particularly enlightening, I think. The first one was Geoff Holmes talking about infrared data from soil samples. He explained that it was hard to achieve sufficiently good performance for practical application. In the activity [quiz], you didn’t get there. You need to do more work on those datasets.
You need to investigate dealing with outliers and improving the quality of the data and some more tweaking of the classifiers and filters in that huge space of experimentation. Then Tony Smith talked about bioinformatics, the problem of signal peptide prediction, and he emphasized that domain knowledge is vital. You need to collaborate with experts. That’s true, of course, for all applications. You need to know whether you’re looking for an accurate prediction or an explanatory model, and overfitting, of course, is a big issue in all applications. Then Pamela talked about functional MRI neuroimaging data. You know, what’s going on in your brain! It was a 3D – a 4D – dataset, the 3 dimensions of your head plus an extra dimension of time.
Again, the performance we got in the activity [quiz] was not all that high, and there were various things that we might consider doing to improve that, most of which would involve domain experts to help interpret the data. This is a common thread through all the applications. A very interesting finding was, in an early competition, just the demographic data alone did well – in fact, it won the competition! It’s extremely important to evaluate what you’re doing and try the simple models first. We’ve been saying that all along. Finally, Mike told us about image classification and the specialist feature extraction techniques for images.
In fact, when I asked him to do this lesson, we didn’t have the feature extraction package that we now have in Weka. He created it in order to do the lesson. This is typical in applications. You need different extraction techniques for different kinds of data. I’m interested in enabling you to carry on learning, to keep learning in the future. One really good way is to look at data mining competitions. There’s a website called Kaggle. Let me just find it for you. Just do a Google search for Kaggle, and here we have it. Kaggle Competitions. There are a large number of competitions here. The first group, this group here, these are the featured competitions, and here you can win money.
This AI Science Challenge is worth $80,000, for example, and the Home Depot Product Search Relevance for $40 ,000. You can win real money doing data mining with these competitions. The second group of competitions are for recruitment purposes. You can get jobs if you do well with the Airbnb challenge or the Telstra challenge, or the Yelp challenge. They’ll offer you a job in data mining, so that’s pretty cool. Here are some featured datasets. Actually, the Iris dataset you’re very familiar with
from the first courses, but here are some interesting ones: the Ocean Ship Logbooks, and Salaries in the San Francisco area.
And some datasets for playing around: here’s the San Francisco crime classification dataset; sounds very interesting. And this last group, “Getting Started”, contains tutorial/educational datasets. You can play around with these and look at what other people have done. These are all current competitions. You can find past competitions by looking for ... “completed competitions”, that’s the phrase. Let’s just look for those. Here we’ve got competitions from two years ago. $500,000, two years ago. Sorry, you’re too late for that, but anyway, someone won half a million two years ago. There’s big money in competitions. Here’s $250,000, again a couple of years old. So there are a lot of past competitions.
On the Kaggle website, we have not just those competitions, but information about completed competitions, past solutions, interviews with winners on the Kaggle blog, and descriptions of winners’ solutions. So there’s a lot of information there. If you want to keep learning about data mining, Kaggle would be a good place to start. I have to finish with a little word on ethics. Don’t forget! I’m always saying this. Ethics of data mining is very much in the news these days. Here are just a few web quotes I got with a very quick search. “More than ever, knowingly or unknowingly, consumers disseminate personal data in daily activities.” Well, we all know that. “As companies seek
to capture data about consumer habits, privacy concerns have flared.” Yes. “Data mining: where legality and ethics rarely meet.” That’s an interesting little title, and the point of that article was that just because you’re doing things legally in accordance with the law doesn’t necessarily mean you’re doing things ethically. I would like you to do things ethically, because you’re an ethical person. It’s the right thing to do. You have personal integrity. But if that’s not enough for you, there are good business reasons for doing things ethically. “Big data might be big business, but overzealous data mining can seriously destroy your brand.” You have to be very careful when you’re doing data mining.
And, the final one, “What big data needs: A code of ethical practices”. So please be aware of ethical issues when you do your data mining. Well, that’s it. This is the end of the course. I hope to meet you again in some other place, some other time. I look forward to that. Meanwhile, enjoy your data mining. And while you’re doing that, I’ll go back to doing something I really love, and play some music. Bye for now!
<End Transcript>

<-- 5.22 Discussion -->
Reflect on this week’s Big Question
This week’s Big Question is, “How can you script Weka”?
So, can you? What do you think of the Python and Groovy scripting languages? Let your fellow learners know how you got on!

<-- 5.23 Quiz -->
Post-course assessment 
Question 1
The Hoeffding tree classifier works well for larger datasets, but for very small datasets you’d expect it to be outperformed by J48.
Use the Explorer to compare bagged J48 and bagged Hoeffding trees on datasets with 200, 400, 600, 800, 1000 instances from the LED24 data generator, evaluated by 10-fold cross-validation. (For the Hoeffding tree algorithm, use Weka’s trees.HoeffdingTree.) The reason for the bagging is to smooth the small-data result for these essentially unstable classifiers.
At what point do Hoeffding trees begin to outperform J48, in terms of the percentage of correctly classified instances?
200 instances
400 instances
600 instances
800 instances
1000 instances
---
Correct answer(s):
600 instances

<-- 5.23 Quiz -->
Post-course assessment 
Question 2
In MOA, set up Prequential evaluation and the BasicClassificationPerformanceEvaluator, with a WaveformGenerator stream of 1,000,000 instances, and evaluate:
A: Naive Bayes
B: Hoeffding Tree
C: Hoeffding Option Tree
D: Hoeffding Adaptive Tree
How does their performance compare, in terms of final mean accuracy? (Note: “Mean” accuracy, not “Current” accuracy.)
A < C < D < B
A < D < B < C
B < D < C < A
C < B < A < D
D < A < B < C
---
Correct answer(s):
A < D < B < C

<-- 5.23 Quiz -->
Post-course assessment 
Question 3
The breast-cancer.arff dataset  is unusual in that it’s very hard to rival the performance of J48, which gets 75.5% accuracy, evaluated using 10-fold cross-validation. For example, SMO gets 69.6%; LibSVM gets 70.6%.
Use Weka’s GridSearch meta-classifier (in package gridSearch) to optimize the Accuracy (N.B. this is not the default evaluation measure) of LibSVM by varying the cost (X property) and gamma (Y property) parameters over the range , , , , , , , , , . You should get an accuracy of 74.5%.
What are the values of the cost and gamma parameters that GridSearch outputs?
Select all the answers you think are correct.
cost = –2
cost = 0.25
cost = 1
cost = 2
gamma = –2
gamma = 0.25
gamma = 1
gamma = 2
---
Correct answer(s):
cost = 2
gamma = 0.25

<-- 5.23 Quiz -->
Post-course assessment 
Question 4
Load the glass.arff dataset into the Weka Explorer and use R to find which attribute (excluding Type) has the largest inter-quartile range.
(Hint: R was used to find inter-quartile ranges in the Week 3 Quiz Using the Explorer’s R console.)
What is the attribute?
RI
Na
Mg
Al
Si
K
Ca
Ba
Fe
---
Correct answer(s):
Mg

<-- 5.23 Quiz -->
Post-course assessment 
Question 5
From the Weka R console, use ggplot2 to plot kernel density estimates for the attribute K in the glass dataset, simultaneously for all the class values, by giving each type of glass a different color. (Hint: include color=Type as one of the arguments of the aesthetics function.)
Increase the width of the kernels by setting the adjust parameter to 5 in the call to geom_density().
Which class exhibits the highest peak in density?
build wind float
build wind non-float
vehic wind float
vehic wind non-float
containers
tableware
headlamps
---
Correct answer(s):
build wind non-float

<-- 5.23 Quiz -->
Post-course assessment 
Question 6
Perform a 10-fold cross-validation with R’s implementation of the nearest-neighbor classifier, which is in Weka’s MLRClassifier and called classif.knn, on the iris.arff data.
Set the number of nearest neighbors to 5 by entering k=5 into the learnerParams field.
How many instances are incorrectly classified?
1
2
3
4
5
6
7
---
Correct answer(s):
7

<-- 5.23 Quiz -->
Post-course assessment 
Question 7
Recall that the best cross-validated accuracy we obtained for the functional MRI neuro-imaging problem in the Quiz at the end of Week 3 was 65.9% (with SMO).
This is for an 8-class dataset. However, Haxby et al. (2001) evaluated the accuracy of eight 2-class datasets, bottle vs non-bottle, cat vs non-cat, chair vs non-chair, etc.
In the Weka Explorer, open the Haxby_Subj1_Training.arff dataset (use this version rather than the one you created in the Quiz, because it may differ slightly depending on your operating system).
Use the MakeIndicator filter to perform Haxby’s evaluation, setting numeric to False to ensure a nominal class. Use SMO for classification, with the usual 10-fold cross-validation. Do this eight times, once for each class value.
Which classes give the largest percentage accuracies? (There is a tie; check both correct answers.)
Select all the answers you think are correct.
bottle
cat
chair
face
house
scissors
scrambledpix
shoe
---
Correct answer(s):
house
scrambledpix

<-- 5.23 Quiz -->
Post-course assessment 
Question 8
Using the Map-Reduce structure, Distributed Weka uses a Voted ensemble as the “Reduce” component for all of the following classifiers except for one. Which one?
AdaBoostM1
AttributeSelectedClassifier
Naive Bayes
J48
PART
SMO
---
Correct answer(s):
Naive Bayes

<-- 5.23 Quiz -->
Post-course assessment 
Question 9
In the KnowledgeFlow interface, load the “Spark: create an ARFF header” template and create an ARFF header for the glass dataset.
But first use the Explorer to generate a CSV version of the dataset, and delete the header row.
Then, in the KnowledgeFlow interface, load the “Spark: create an ARFF header” template and configure the ArffHeaderSparkJob as follows:
    on the Spark Configuration tab, enter the pathname for the CSV file into the inputFile property;
    on the ArffHeaderSparkJob tab, set the attributeNames property to a list of the attribute names, clear the attributeNamesFile property, and set the outputHeaderFileName to glass.arff.
Execute the flow and look at the result in the TextViewer. The mean value of RI is 1.52. What is the mean of Na?
0.50
1.44
2.68
8.96
13.41
72.65
---
Correct answer(s):
13.41

<-- 5.23 Quiz -->
Post-course assessment 
Question 10
In the KnowledgeFlow interface, load the “Spark: cross-validate two classifiers” template, and simplify it as follows:
    delete the WekaClassifierEvaluationSparkJob2 and the RandomlyShuffleDataSparkJob
    make  a “success” connection from the ArffHeaderSparkJob to the WekaClassifierEvaluationSparkJob.
Run, and check that the percentage of correctly classified instances is 95.6%.
Configure the ArffHeaderSparkJob to have 1 input slice. What is the percentage of correctly classified instances now?
95.4%
95.5%
95.6%
98.8%
99.2%
99.8%
---
Correct answer(s):
95.4%

<-- 5.23 Quiz -->
Post-course assessment 
Question 11
In the Explorer, open the vehicles dataset that comes with the imageFilters package that you downloaded when learning to classify images. The problem is to build a car detector and identify which images are misclassified.
In order to build a car detector, convert from a 3-class to a 2-class problem by using Weka’s MergeTwoValues filter to replace the PLANE and TRAIN classes with a single class.
Apply the EdgeHistogramFilter to the dataset.
The goal is to identify misclassified images, so do not remove the filename attribute from the dataset. Instead, use the FilteredClassifier with filter RemoveType and the default type of string (since filename is the only string attribute). Set the classifier to Naive Bayes.
Under More Options, change Output Predictions from Null to PlainText; then click PlainText and set the attributes field to 1. This prints the filename of the image alongside each prediction.
Which of these PLANE_TRAIN images are misclassified as CAR?
Select all the answers you think are correct.
train1
train15
train17
plane1
plane15
plane17
---
Correct answer(s):
train1
train17
plane15

<-- 5.23 Quiz -->
Post-course assessment 
Question 12
The dataset is unbalanced, because there are 40 PLANE_TRAIN images but only 20 CAR images, so it is appropriate to look at the F-Measure instead of the accuracy.
Repeat the experiment of the previous question with different filters, still using Naive Bayes.
Which filter achieves the greatest weighted-average F-Measure?
AutoColorCorrelogramFilter
BinaryPatternsPyramidFilter
ColorLayoutFilter
EdgeHistogramFilter
FCTHFilter
FuzzyOpponentHistogramFilter
GaborFilter
JpegCoefficientFilter
PHOGFilter
SimpleColorHistogramFilter
---
Correct answer(s):
PHOGFilter

<-- 5.23 Quiz -->
Post-course assessment 
Question 13
When this Python program is run in the Weka Jython console window, it outputs cross-validation statistics for the iris dataset.
What classifier does it use?
Bagged classifier with the Discretize filter
Bagged J48 classifier with the Discretize filter
Bagged Naive Bayes classifier with the Discretize filter
Filtered classifier with the Discretize filter and Bagged J48 classifier
Filtered classifier with the Discretize filter and Bagged Naive Bayes classifier
J48 classifier
J48 classifier with the Discretize filter
Naive Bayes classifier
Naive Bayes classifier with the Discretize filter
---
Correct answer(s):
Filtered classifier with the Discretize filter and Bagged Naive Bayes classifier

<-- 5.23 Quiz -->
Post-course assessment 
Question 14
How many code lines (including import lines, but not including blank lines) of the Python program in the previous question can be deleted without affecting the outcome?
1
2
3
4
5
6
---
Correct answer(s):
3

<-- 5.23 Quiz -->
Post-course assessment 
Question 15
When this Groovy program is run in the Weka Groovy console window, it outputs the percentage accuracy of J48 on the glass.arff dataset, evaluated using 10-fold cross-validation.
Change the program to evaluate the IBk classifier on the breast-cancer.arff dataset; and use this Groovy “for” loop
for (i in 1..10) {
 ...
}
to increase the number of nearest neighbors (the K parameter) from 1 to 10, printing the accuracy each time.
What value of K achieves the greatest accuracy?
1
2
3
4
5
6
7
8
9
10
---
Correct answer(s):
4

<-- 5.23 Quiz -->
Post-course assessment 
Question 15
When this Groovy program is run in the Weka Groovy console window, it outputs the percentage accuracy of J48 on the glass.arff dataset, evaluated using 10-fold cross-validation.
Change the program to evaluate the IBk classifier on the breast-cancer.arff dataset; and use this Groovy “for” loop
for (i in 1..10) {
 ...
}
to increase the number of nearest neighbors (the K parameter) from 1 to 10, printing the accuracy each time.
What value of K achieves the greatest accuracy?
1
2
3
4
5
6
7
8
9
10
---
Correct answer(s):

<-- 5.24 Article -->
Farewell
Thanks for taking this course. We hope you’ve enjoyed it.
We’ve introduced you to some more advanced topics in practical data mining, following on from Data Mining with Weka and More Data Mining with Weka. The aim is to increase your knowledge, understanding, and abilities in practical data mining, lifting you to the wizard level of skill in data mining with Weka.
We’ve shown you how to use popular packages that extend Weka’s functionality. You’ve learned about forecasting time series and mining data streams. You’ve connected up the popular R statistical package and learn how to use its extensive visualisation and preprocessing functions from Weka. You’ve script Weka in Python – all from within the friendly Weka interface. And you’ve learned how to distribute data mining jobs over several computers using Apache SPARK.
We claimed that after completing it you would be able to mine your own data in more subtle and sophisticated ways – and understand what it is that you are doing.
What do you think? How did you get on? What did you learn? What did you struggle with? Have your views on data mining changed? What do you want to learn next? Please let us know by filling in the post-course survey. We welcome your suggestions on what could be improved!
This course will run again starting in May 2018. Please tell anyone you think might benefit from it.
No matter where you go, there you are. (Sometimes attributed to Confucius)

<-- 5.25 Article -->
Index
(A full index to the course appears at the end of Week 1.)
      Topic
      Step
      Datasets
      anneal
      5.5, 5.7, 5.11, 5.15
      balance-scale
      5.8, 5.10, 5.11, 5.15
      bodyfat
      5.8, 5.10, 5.11
      breast-cancer
      5.10
      iris
      5.2, 5.7
      Classifiers
      BayesNet
      5.8
      GaussianProcesses
      5.17, 5.18
      IBk
      5.17
      J48
      5.5, 5.8, 5.11
      LinearRegression
      5.8, 5.11, 5.17
      LWL (Locally weighted learning)
      5.17, 5.18
      NaiveBayes
      5.7, 5.8
      Filters
      AddNoise
      5.4
      Discretize
      5.4, 5.7
      Packages
      jfreechartOffScreenRenderer
      5.2, 5.3, 5.8
      kfGroovy
      5.20
      tigerJython
      5.2, 5.3, 5.4
      Python classes
      DataSource, Filter, os, Remove
      5.2
      DataSource, Evaluation, J48, os, Random
      5.5
      BayesNet, ChartFactory, ChartPanel, ChartUtilities, DataSource, DefaultXYDataset, DefaultXYZDataset, Evaluation, File, GraphVisualizer, JFrame, J48, LinearRegression, NaiveBayes, os, PlaceNode2, PlotOrientation, ThresholdCurve, TreeVisualizer, XYBubbleRenderer
      5.8
      Evaluation, J48, SMOreg, ZeroR
      5.10
      Evaluation, Classifier, converters, jvm, os, Random
      5.11
      Groovy imports
      ChartFactory, ChartPanel, DataSource, DefaultXYDataset, Instance, Instances, JFrame, J48, NaiveBayes, PlotOrientation, Random, ThresholdCurve
      5.15
      Plus
      Environment variable
      5.2, 5.3, 5.7, 5.17
      Ethics
      5.21
      Groovy
      5.15–5.20
      IDRC Shootout challenge
      5.15, 5.17, 5.18
      Kaggle
      5.21
      Python/Jython
      5.2–5.14
      Python-weka-wrapper
      5.11
      Scripting
      5.1–5.20

