<-- 1.0 Todo -->
A little bit of everything
What's data mining? What's Weka? What's the course about?
Everybody talks about data mining and "big data" nowadays. This course introduces you to practical data mining. Weka is a powerful yet easy to use tool for machine learning and data mining that can also tackle large problems.
1.1
What's data mining? What's Weka?
video (05:55)
1.2
About this course
article
1.3
Welcome! Please introduce yourself
discussion
1.4
Data mining applications
quiz
1.5
More data mining applications
discussion
What's it like to do data mining?
Each week we’ll focus on a “Big Question” relating to data mining. This is the question for this week.
1.6
What's it like to do data mining?
article
Exploring the Explorer
In this activity you will install Weka on your computer, start the Weka Explorer, and load, view, and edit datasets. (Note: You may need admin access to install Weka.)
1.7
Installing Weka: preview
article
1.8
Installing Weka
video (05:33)
1.9
Installing Weka
discussion
1.10
The weather data
video (04:08)
1.11
Looking at a dataset
quiz
1.12
Forecasting electricity demand
discussion
Exploring datasets
Classification (also called "supervised learning") is a common kind of data mining problem. Datasets contain "instances," which are described in terms of a fixed set of features, or "attributes".
1.13
More weather
video (04:27)
1.14
The Iris dataset
quiz
1.15
The glass data
video (05:10)
1.16
More irises
quiz
Building a classifier
Now you will learn how to use Weka's popular J48 classifier, which builds decision trees. J48 is a reimplementation of a classic classifier algorithm called C4.5.
1.17
Building a classifier
video (08:14)
1.18
Using J48
quiz
Using a filter
WEKA contains "filters" that help with cleaning and preparing your data. Some filters operate on attributes; others operate on instances.
1.19
Using a filter
video (06:51)
1.20
Using filters
quiz
Visualizing your data
With Weka's visualization tool you can clean the data and remove anomalous instances (outliers). You can also visualize the errors that classifiers make.
1.21
Visualizing your data
video (06:54)
1.22
Finding misclassified instances
quiz
1.23
Reflect on this week's Big Question
discussion
1.24
Index
article

<-- 1.1 Video -->
What's data mining? What's Weka?
Everybody talks about data mining and “big data” nowadays. Example applications range from analyzing the contents of your supermarket basket in order to entice you to spend more in your next shopping expedition, to helping a couple conceive a baby by enhancing the chance of successful artificial insemination. Weka is a powerful yet easy-to-use tool for machine learning and data mining that you will soon download and experiment with. During this course you will learn how to load data, filter it to clean it up, explore it using visualizations, apply classification algorithms, interpret the output, and evaluate the result. You will also learn that New Zealand is at the top of the world, and you may be at the bottom!
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! Welcome to the course Data Mining with Weka. I’m Ian Witten from the University of Waikato in New Zealand and I’m presenting the videos for this course which is being prepared by the Department of Computer Science at the University of Waikato. Data mining is a mature technology that a lot of people are beginning to take very seriously, and a lot of other people find it mysterious. The real aim of this course is to take the mystery out of data mining. This is a practical course on how to use the Weka workbench, which you will download as part of the course, for data mining.
We explain the basic principles of several popular data mining algorithms and how to use them in practical applications. In the world today, we’re overwhelmed with data. Every time you swipe your credit card, every item you checkout out at the supermarkets, every time you send a text, make a phone call, or send an email, or type a key on a computer, even every time you walk past a security camera – it all generates a little bit of data in a database. Data mining is about going from the raw data to information, information that can be used to make predictions, predictions that are useful in the real world. Let me give you an example. You’re at the supermarket checkout.
The till records every item you bought. At the end, you hand over your loyalty card, and they give you a couple of percent off, and you give them your name and address, and, indirectly, access to all sorts of demographic information about you and people like you. Everybody likes a good bargain. It’s been a good day today, because, thanks to those coupons they sent you in the mail last week, you’ve been able to stock up on some things you wouldn’t normally have bought, but you bought today because they’re such a good deal. Next week they’ll send you some more coupons, and you’ll go shopping again and buy some more stuff.
They do little experiments on you, you know, they try to figure out how much more you would buy if the price was just that little bit less. These coupons are a mechanism for personalized pricing. They’ve got access to all sorts of data from you, and people like you, in order to do these experiments and figure these things out.
Everybody wins: you get your bargains; they sell more stuff. It sounds like a good deal to me. Here’s another application. Suppose you and your partner want a child, but you can’t have one. It’s fun trying, but it can get a little bit frustrating, and, ultimately, very frustrating, perhaps even tragic. In artificial insemination, they take some eggs from the woman’s ovaries, and they fertilize them with partner or donor sperm, and then they select from amongst the embryos that are produced some to implant back into the womb. You want to select the ones with the best chance of success of producing a live birth, but you don’t want too many live births.
The embryologist has access to all sorts of data on these embryos. I think there are 50–100 pieces of information that they record about individual embryos, and they have historical data on which ones produced a live birth – a success. So here’s an ideal situation for data mining. We have lots of historical data; we have data on the present situation; and we want to select those embryos that have the best chance of success. Now, that’s a good application for data mining, bringing a live child to a couple who wants one. I talk about “data mining” and “machine learning”. Data mining is the application, and machine learning is the algorithms we use.
We’re talking about using machine learning algorithms for the purposes of data mining. The next question – this is Data Mining with Weka – “What’s Weka?” This is a weka here, this little bird. It’s a flightless bird, like its better known cousin the kiwi, found only in the islands of New Zealand. This is what it sounds like, coming to you from New Zealand. However, in our context, Weka is a data mining workbench. It’s an acronym for the Waikato Environment for Knowledge Analysis. We just call it Weka. It contains a large number of algorithms for classification, and a lot of algorithms for data preprocessing, feature selection, clustering, finding association rules, things like that.
It’s a very comprehensive workbench, and it’s free open source software that you will download as part of this course in the next lesson. It runs on any computer. It’s written in Java, and runs on Linux, Windows, Mac. You’ll be able to download it and run it on your workstation and use it during the course. You’re going to learn how to load data into Weka and look at it. You’re going to learn about preprocessing, cleaning up data using filters, exploring it using visualizations, applying classification algorithms, interpreting the output, understanding evaluation methods – evaluation is very important in this area – understand various representations for models, how popular machine learning algorithms work, and be aware of common pitfalls with data mining.
The ultimate goal really is to empower you to use Weka on your own data, and, most importantly, to understand what it is you are doing. That’s it. I just thought I’d show you were I am. I’m in New Zealand, that’s where Weka is from. That’s where I’m sitting right now. This is the world as we see it in New Zealand. We’re at the top, you’re probably down at the bottom somewhere. We’re at the top, in the center, and that arrow to the North Island of New Zealand is where the University of Waikato is. I’ll see you again in the next lesson. I’m looking forward to that. Goodbye for now.
<End Transcript>

<-- 1.2 Article -->
About this course
This course introduces you to practical data mining using the Weka workbench. We explain the basic principles of several popular algorithms and how to use them in practical applications. The aim of the course is to dispel the mystery that surrounds data mining.
After completing it you will be able to mine your own data – and understand what it is that you are doing!
Course structure
Teachers open the door. You enter by yourself. (Chinese proverb)
This is structured as a five week course:
  Week 1: A little bit of everything
  Week 2: Evaluation
  Week 3: Simple classifiers
  Week 4: More classifiers
  Week 5: Putting it all together
Each week focuses on a “Big Question.” For example, Week 1’s Big Question is: What’s it like to do data mining? The week covers a handful of activities that together address the question. Each activity comprises:
  5-10 minute video
  Quiz. But no ordinary quiz! In order to answer the questions you have to undertake some practical data mining task. You don’t learn by watching someone talk; you learn by actually doing things! The quizzes give you an opportunity to do a lot of data mining.
I hear and I forget. I see and I remember. I do and I understand. (Confucius)
You will get additional benefits by purchasing an upgrade, including access to the tests:
  Mid-class test at the end of Week 2
  Post-class test at the end of Week 5
This week …
In Week 1 you will get started with data mining. You will install Weka, explore its interface, explore some data sets, build a classifier, interpret the output, use filters, and visualize data sets. At the end of the week you will know what it’s like to do data mining!
Right now …
Please take the time to fill in the pre-course survey.
Teaching team
  Lead educator, Ian Witten
  Educator, David Nichols
  Educator, Jemma König
Production team
  Video editing, Peter Oliver and Louise Hutt
  Captions, Jennifer Whisler
  Music: Mozart’s Divertimento No. 2, Allegro, performed by Woodside Clarinets: Paul King, Sarah Shieff and Ian Witten
Support
  Share what you are learning, including difficulties, problems and solutions, with others in the class in a weekly discussion focused on the Big Question of the week and what you have learned
  Other discussions from time to time
  Transcripts are supplied for all videos
  Slides for all videos can be downloaded as a PDF file
Software requirements
You will download and install the free Weka software during Week 1. It runs on any computer, under Windows, Linux, or Mac. It has been downloaded millions of times and is being used all around the world.
(Note: Depending on your computer and system version, you may need admin access to install Weka.)
Prerequisite knowledge
You need no programming experience for this course. And no math, though some high-school statistical concepts are used (means and variances, maybe confidence intervals).

<-- 1.3 Discussion -->
Welcome! Please introduce yourself
Hey! I’m Ian.
Formally I’m Professor Ian Witten from the University of Waikato in New Zealand, but everyone calls me Ian. I grew up in Northern Ireland, studied at Cambridge University, and worked at the Universities of Essex in England and Calgary in Canada before moving to paradise (aka New Zealand) 25 years ago. I became interested in data mining way back when, and also in open source software and open education. Data, they say, is the “new oil”: it affects all of us economically, socially, and politically.
I made this course because everyone needs to know about the data revolution (which far outshines the “computer revolution”) and what you can do with data – and what they can do with data. That’s why I’m so glad to have you here and learning!
A bird does not sing because it has an answer. It sings because it has a song. (Chinese proverb)
Who are you? Where are you from? What do you do? Why are you here? What are you hoping to learn? What sort of data do you have and why do you need to analyze it? Can you share an example?
See what other learners say and let them know if you share their interests (use ‘Reply’).
(We find the diversity of backgrounds, interests and locations of the people who join this course absolutely fascinating.)
You can make and reply to comments on almost every step of the course by clicking the pink plus icon. There will also be specific discussion steps from time to time (like this one and the regular ‘Weekly Reflections’ steps). Please join in!
There’s more information on FutureLearn discussions here.

<-- 1.4 Quiz -->
Data mining applications
Question 1
Can data mining be applied to … customer relationship management?
Yes!
No
---
Correct answer(s):
---
Feedback correct:
To establish a productive relationship with a customer, businesses collect data and analyse it. This can help in acquiring and retaining customers, developing customer loyalty, and implementing customer focused strategies.

<-- 1.4 Quiz -->
Data mining applications
Question 2
Can data mining be applied to …  financial data analysis?
Yes!
No
---
Correct answer(s):
---
Feedback correct:
Data mining can contribute to solving business problems in banking and finance by finding patterns, causalities, and correlations in business information and market prices that are not immediately apparent to managers.

<-- 1.4 Quiz -->
Data mining applications
Question 3
Can data mining be applied to …  supermarket basket analysis?
Yes!
No
---
Correct answer(s):
---
Feedback correct:
If you buy certain items you are more likely to buy other items. Understanding purchasing behavior helps the retailer understand the buyer’s needs and change the store layout accordingly.

<-- 1.4 Quiz -->
Data mining applications
Question 4
Can data mining be applied to …  education?
Yes!
No
---
Correct answer(s):
---
Feedback correct:
Mining data generated by courses such as this one can help predict students’ future learning success, assess the effect of educational support, and advance scientific knowledge about learning. Perhaps individual learning patterns can be discovered and used to personalize the presentation of material.

<-- 1.4 Quiz -->
Data mining applications
Question 5
Can data mining be applied to …  healthcare?
Yes!
No
---
Correct answer(s):
---
Feedback correct:
Data mining can help identify best practices that improve care and reduce costs. It can be used to predict the volume of patients in different categories, which helps hospitals develop processes to ensure that patients receive appropriate care at the right place and at the right time.

<-- 1.4 Quiz -->
Data mining applications
Question 6
Can data mining be applied to …  criminal investigation?
Yes!
No
---
Correct answer(s):
---
Feedback correct:
You may find
1.18 Using J48
useful.
---
Feedback correct:
Crime analysis involves exploring and detecting crimes and their relationships with criminals. Crime datasets are large, comprehensive and complex. Textual reports can be analyzed using text mining.
---
Feedback incorrect:
You may find
1.18 Using J48
useful.

<-- 1.4 Quiz -->
Data mining applications
Question 7
Can data mining be applied to …  making babies?
Yes!
No
---
Correct answer(s):

<-- 1.4 Quiz -->
Data mining applications
Question 7
Can data mining be applied to …  making babies?
Yes!
No
---
Correct answer(s):
---
Feedback correct:
Yes! Selecting embryos for artificial insemination, amongst other possibilities that we’ll leave to your imagination.

<-- 1.5 Discussion -->
More data mining applications
Now we’ve got you thinking about possible data mining applications, we invite you to share your thoughts on other data mining/machine learning applications with fellow learners.
Learning is as high as the mountains and as wide as the seas. (Chinese proverb)
Just to get you started, what about …
  Telecommunication industry
  Manufacturing engineering
  Fraud detection
  Intrusion detection
  Lie detection
  Customer segmentation
  Financial banking
  Corporate surveillance
  Research analysis
  Bioinformatics
  Scientific applications

<-- 1.6 Article -->
What's it like to do data mining?
You’ll be asking what kind of problems might be addressed? What kind of input do you need about the problem? What kind of output can you expect from a data mining system? What steps are involved in actually doing the mining?
The questions are the same regardless of what system you might use for data mining. We’ll illustrate what it’s like to use the Weka data mining workbench to look at some simple datasets, preprocess them, analyze them, and visualize the result.
Expect a whirlwind tour. This week we’ll take you swiftly – inevitably a little superficially – through the whole process. You’ll install Weka, check out its interface, explore some data sets, build a classifier, interpret the output, use filters, and visualize data. In subsequent weeks we’ll look more closely at the various steps and options, and what it all means.
At the end of the week you will be able to demonstrate use of Weka for the key tasks of examining a dataset, classifying it with a decision tree, filtering it with various filters, and visualizing the dataset and the result of learning.

<-- 1.7 Article -->
Installing Weka: preview
In the following video Ian downloads and installs the Weka system – and encourages you to do the same.
(Note: Depending on your computer and system version, you may need admin access to install Weka.)
    Go to http://www.cs.waikato.ac.nz/ml/weka
    Click the Download button
    Now choose which one to download:
        the latest version, currently Weka-3-8-2
        (use the latest “stable” version; Weka-3-9 is a “developer” version)
        the appropriate link for your computer; Windows, Mac, or Linux
        if you don’t know what Java is, you probably want the file that includes a Java VM (= virtual machine)
Once it’s downloaded, Ian opens it to get a standard setup “wizard”.
    Just keep clicking “Next”! Install it in the default place – and remember the name of that place!
    After installation, Ian unchecks the box that says “Start Weka” and clicks “Finish”.
    Then he goes to where Weka was installed and
        creates a shortcut and puts it on the desktop for future use. This should be to the Weka program, but he mistakenly creates a shortcut to the Weka folder instead which he later corrects
        makes a copy of the data folder (within the Weka folder) and puts it in a convenient place for future use
        renames it “Weka datasets”
Having installed Weka, Ian notes the different interfaces in Weka: Explorer, which is what we’ll be using; the Experimenter; the KnowledgeFlow interface; and the command-line interface.
In the Explorer there are several panels: Preprocess; Classify; Clustering; Association rules;
Attribute selection; and Visualization.

<-- 1.8 Video -->
Installing Weka
Here’s how to download the Weka data mining workbench and install it on your own computer. For future convenience, create a shortcut to the program and put it somewhere handy – like the desktop. The Weka download comes with a folder containing sample data files that we’ll be using throughout the course; put this somewhere handy too – like the My Documents folder. Weka has four interfaces: the “Explorer” is the one we’ll be using throughout this course.
(In the video Ian downloads Weka 3.6, but you should download the latest stable version of the software.)
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! Welcome back to the course on Data Mining with Weka. I’m Ian, up here in New Zealand. First we’re going to download the Weka system. This is something you’re going to have to do on your computer. We’re going to download it from this URL. Without delay, let’s go straight there. Here we are. This is www.cs.waikato.ac.nz/ml/weka. You can read about Weka here. I’m going to go straight to the Download button and download and install Weka on my computer. I’m running on a Windows machine here, but there are versions down at the bottom you can see for Mac OS X and Linux and so on. You need to download the appropriate version for your machine.
I’m going to download a self-extracting executable without the Java Virtual Machine – I already have the Java Virtual Machine on my computer. I’m going to click here, but you’re going to need to do whatever’s appropriate for your computer.
While it’s downloading, let’s have a word about the pronunciation of the word ‘Weka’. It’s called Weh-kuh. We don’t like calling it ‘weaker’ system. It’s not ‘weaker’, it’s Weka, pronounced to rhyme with ‘Mecca’. That’s the name of the bird; that’s the name of our software. Weka.
I think it has downloaded now, and I’m going to open it.
This is a standard setup wizard. I’m just going to keep clicking “Next”. Yes, I’m happy with the GNU Public License. I’m going to have a full install. I’m going to install it in the default place – you need to remember the name of this place; we’re going to need to visit there in a moment. We’re going to install the whole thing. This is going to take a couple of minutes. I’m just off for a cup of coffee; I’ll be back in a second.
Now, it’s installed. Let’s carry on here. I want to click “Finish”, but actually I’m not going to start Weka. I’m going to uncheck that, and click “Finish”, because there are a couple of things I want to do first. Let’s go and see where Weka is. It’s on my computer in Program Files.
I’m going to create a shortcut to that, because we’re going to be using it a lot in this course. I’m just going to put it on the desktop.
Then, I’m going to do one more thing. I’m going to go inside this folder, and I’m going to look at the data folder. This contains a bunch of datasets we’re going to be using. I’m going to take this folder and copy it and put it somewhere convenient.
Let’s cut that, and I’m going to put it in the “My Documents” folder.
I’m going to rename it “Weka datasets”.
I’m all set. I’ve finished installing Weka.
I’ve got my shortcut to Weka here. Oops – I made my shortcut to the wrong place; I meant to make the shortcut to this here. Let me just make a shortcut here. Create shortcut, put it on the desktop. That’s the one I want.
Now, when I click here, it will open Weka. Back to the slide. There are four interfaces in Weka. The Explorer is the one that we’ll be using throughout this course. We’re just using the Explorer, but also there’s the Experimenter for large-scale performance comparisons for different machine learning methods on different datasets. There’s the KnowledgeFlow interface, which is a graphical interface to the Weka tools; and there’s a command-line interface. But we’re just going to use the Explorer. So let’s get on with it. Here’s the Explorer.
Across the top, there are five panels: the Preprocess panel; the Classify panel (these are greyed out because I haven’t opened a file yet), where you build classifiers for datasets; Clustering, another procedure Weka is good at, although we won’t be talking about clustering in this course; Association rules; Attribute selection; and Visualization. In this course, we’ll be using mainly the Preprocess panel to open files and so on, the Classify panel to experiment with classifiers, and the Visualize panel to visualize our datasets.
<End Transcript>

<-- 1.9 Discussion -->
Installing Weka
Have you installed Weka on your computer yet?
If not, do it now! Download the latest (stable) version of Weka from here.
How did it go? It should be easy, but you never know … Please share your experiences, and any tips, with fellow learners. If you have problems, post them. (Don’t forget to include details such as the computer, maybe system version too.) If you’ve figured out the solution, post that. And if you can help someone else, please do so! You’re a community.
OK, I’ll kick it off:
Problem. I downloaded Weka but my Mac says it cannot open it because it comes from an unrecognised developer.
Solution. Yep, Macs are picky about new software. Open System Preferences and click Security & Privacy. You’ll probably find it says to allow apps downloaded from the Mac App Store (and maybe from identified developers as well). Temporarily click Anywhere. Then start Weka. It should open OK, and in fact your Mac will remember that you’ve OK’d it. Then, to be safe, restore the original setting in Security & Privacy.

<-- 1.10 Video -->
The weather data
Here’s how to load a dataset into the Weka Explorer interface, and look around it to see what’s there. It’s a tiny “toy” dataset, but all these operations work equally well on large, real life, ones. You can also edit the dataset, and – if you like – change it.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
I’m going to open a dataset. The dataset I’m going to open is called the “weather data”; it’s a little toy dataset that we’ll be seeing a lot of in this course. It’s got 14 instances, 14 days, and for each of these days, we have recorded the values of five attributes.
Four are to do with the weather: Outlook, Temperature, Humidity, and Windy. The fifth, Play, is whether or not we’re going to play a particular, unspecified game. Actually, what we’re going to be doing is predicting the Play attribute from the other attributes. But let’s not worry about that at the moment. Let’s open the dataset and take a look at it in Weka. Here’s “My Documents”. Here are the Weka datasets; this is what I copied. I’m going to open weather.nominal.arff. All Weka data files are called ARFF files; we’ll talk about that later on. This is the “weather” data. Just ignore these colorful bars at the moment.
There are 14 instances; these correspond to the 14 days that we saw in the dataset on the slide.
For each day we have five attributes: outlook, temperature, humidity, windy, and play. If you select one of these attributes – outlook is selected at the moment – we can see the values. The values for the outlook attribute are “sunny”, “overcast”, and “rainy”.
These are the number of times they appear in the dataset: 5 sunny days, 4 overcast days, and 3 rainy days, for a total of 14 days, 14 instances. If we look at the “temperature” attribute, “hot”, “mild”, and “cool” are the possible values, and these are the number of times they appear in the dataset. Let’s go to the “play” attribute. There are two values for play, yes and no. Now let’s look at these two bars here. Blue corresponds to yes, and red corresponds to no. If you look at one of the other attributes, like “outlook”, you can see that when the outlook is sunny – this is like a histogram – there are three “no” instances and two “yes” instances.
When the outlook is overcast, there are four “yes” instances and zero “no” instances. These are like a histogram of the attribute values in terms of the attribute we’re trying to predict. It makes it useful to click around and visualize your data. We’ve opened the weather data, weather.nominal.arff.
We’ve looked at the attribute values and the attributes in Weka. There’s one more thing I want to do before we summarize here. If I go to the Edit panel, I see the data in the form that it was on the slide, with the 14 days down here and the 5 attributes across here. This is another view of the data. I can actually change this dataset. If I click here, I can change this “no” to “yes”. Or, if I click here, I can change on this day the outlook from “rainy” to “sunny”.
(If only it were so easy in real life to change a day from rainy to sunny!) Then I can click OK, and we’ve got this edited dataset, which we could save if we’d like. We haven’t saved any of this; the dataset on the disk is still the same as it was. I’m not going to save it, and I don’t think you should save it, because we’re going to be using this dataset quite a bit in this course. Bye for now!
<End Transcript>

<-- 1.11 Quiz -->
Looking at a dataset
Question 1
How many instances are there in the contact-lenses dataset?
5
8
24
32
---
Correct answer(s):
24
---
Feedback correct:
The Preprocess panel tells you how many data rows, or “instances,” are in the currently loaded data set.
---
Feedback incorrect:
The Preprocess panel tells you how many data rows, or “instances,” are in the currently loaded data set.

<-- 1.11 Quiz -->
Looking at a dataset
Question 2
How many attributes are there in the contact-lenses dataset?
4
5
12
24
---
Correct answer(s):
5
---
Feedback correct:
The Preprocess panel shows you how many columns, or “attributes,” are in the current data set.
In this case the attributes are age, spectacle-prescrip, astigmatism, tear-prod-rate, and contact-lenses.
---
Feedback incorrect:
Almost right. It’s not quite clear whether the class (in this case contact-lenses) should be counted as an attribute or not. But for the purposes of this question we do count it as an attribute.
---
Feedback incorrect:
The Preprocess panel shows you how many columns, or “attributes,” are in the current data set.
In this case the attributes are age, spectacle-prescrip, astigmatism, tear-prod-rate, and contact-lenses.

<-- 1.11 Quiz -->
Looking at a dataset
Question 3
How many possible values are there for the age attribute?
2
3
5
8
---
Correct answer(s):
3
---
Feedback correct:
The “Selected attribute” box shows a summary of the currently selected attribute.
For categorical attributes, it lists all available values – in this case, young, pre-presbyopic, and presbyopic.
---
Feedback incorrect:
The “Selected attribute” box shows a summary of the currently selected attribute.
For categorical attributes, it lists all available values – in this case, young, pre-presbyopic, and presbyopic.

<-- 1.11 Quiz -->
Looking at a dataset
Question 4
Which of these attributes has reduced as a possible value?
age
spectacle-prescrip
astigmatism
tear-prod-rate
contact-lenses
---
Correct answer(s):

<-- 1.11 Quiz -->
Looking at a dataset
Question 4
Which of these attributes has reduced as a possible value?
age
spectacle-prescrip
astigmatism
tear-prod-rate
contact-lenses
---
Correct answer(s):
tear-prod-rate
---
Feedback correct:
By selecting different attributes in the “Attributes” box you can quickly investigate their possible values.
The tear-prod-rate attribute has values reduced and normal.
---
Feedback incorrect:
By selecting different attributes in the “Attributes” box you can quickly investigate their possible values.

<-- 1.12 Discussion -->
Forecasting electricity demand
In the electricity supply industry, it is important to determine future demand for power as far in advance as possible.
If accurate estimates can be made for the maximum and minimum load for each hour, day, month, season, and year, utility companies can make significant economies in areas such as setting the operating reserve, maintenance schedule, and fuel inventory management.
Given appropriate historical data, forecasting the demand for electricity for a particular hour, day, month, and year is a data mining problem.
Brainstorm suggestions for suitable input – the attributes, or features – for this problem. Assume that whatever historical data you need is available, and also weather forecasts for the forecasting period.
Think about this:
    Periodicity in electrical load may occur at several fundamental frequencies – a yearly one is obvious (why?), what are some others?
    What about minor variations that might occur on holidays?
    What about the weather?
    What about overall growth?

<-- 1.13 Video -->
More weather
Key concepts when talking about datasets are instances, attributes, and the class (which is conventionally the last attribute). Attributes can be nominal or numeric (there are also other types). In a classification problem the goal is to produce automatically some kind of model that can determine the class of new instances. These concepts are illustrated using the weather dataset.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! Welcome back for another five minutes in New Zealand with Data Mining with Weka. We looked at this data file in the last lesson. It’s the weather data, a toy dataset of course. It has 14 days, or instances, and each instance, each day, is described by five attributes, four to do with the weather, and the last attribute, which we call the “class” value -- the thing that we’re trying to predict, whether or not to play this unspecified game. This is called a classification problem. We’re trying to predict the class value. Let’s open up Weka. It’s here on my desktop. I’m going to go into the Explorer. We always use the Explorer. I’m going to open the file.
I put the datasets in the My Documents folder, so I can see them here. Just open the Weka datasets and the nominal weather data. There’s the weather data in Weka. As we saw last time, you can see the size of the dataset, the number of instances (14), you can see the attributes, you can click any of these attributes and get the values for those attributes up here in this panel. You also get at the bottom a histogram of the attribute values with respect to the different class values. The different class values are blue for “yes”, play, and red for “no”, don’t play. By default, the last attribute in Weka is always the class value.
You can change this if you like. If you change it here you can decide to predict a different one other than the last attribute. That’s the weather dataset, and we’ve already explored that. As I said, it’s a classification problem, sometimes called a “supervised learning” problem – “supervised” because you get to know the class values of the training instances. We take as input a data set as classified examples; these examples are independent examples with a class value attached. The idea is to produce automatically some kind of model that can classify new examples. That’s the “classification” problem. Here is what the examples look like.
This is an “instance”, with the different attribute values a fixed set of features; and then we add to that the class to get the classified example. That’s what we have to have in our training dataset. These attributes, or features, can be discrete or continuous. What we looked at in the weather data were discrete; we call them “nominal” attribute values when they belong to a certain fixed set. Or they can be numeric, or “continuous”, values. Also, the class can be discrete or continuous. We’re looking at a discrete class, “yes” or “no”, in the case of the weather data. Another kind of machine learning problem would involve continuous classes, where you’re trying to predict a number.
That’s called a “regression” problem in the trade.
I’m going to have a look at a similar dataset to the weather dataset: the numeric weather dataset. Let me just open that in Weka, weather.numeric.arff. Here it is. It’s very similar, almost identical in fact, with 14 instances, 5 attributes, the same attributes. Maybe I should just look at this dataset in the edit panel. You can see here that two of the attributes – temperature and humidity – are numeric attributes, whereas previously they were nominal attributes. So here there are numbers. What we see when we look at the attributes values for outlook, just as before, we have sunny, overcast and rainy. For temperature, though, we can’t enumerate the values, there are too many numbers to enumerate.
We have the minimum and maximum value, mean, and standard deviation. That’s what Weka gives you for numeric values.
<End Transcript>

<-- 1.14 Quiz -->
The Iris dataset
Question 1
How many instances are there in the iris dataset?
100
150
200
250
---
Correct answer(s):
150
---
Feedback correct:
The Preprocess panel tells you how many data rows, or Instances, are in the currently loaded data set.
---
Feedback incorrect:
The Preprocess panel tells you how many data rows, or Instances, are in the currently loaded data set.

<-- 1.14 Quiz -->
The Iris dataset
Question 2
How many attributes are there in the iris dataset?
4
5
7
10
---
Correct answer(s):
5
---
Feedback correct:
The Preprocess panel also shows you how many attributes, or columns, are in the current data set. In this case the attributes are sepallength, sepalwidth, petallength, petalwidth, and class.
---
Feedback incorrect:
The Preprocess panel also shows you how many attributes, or columns, are in the current data set. In this case the attributes are sepallength, sepalwidth, petallength, petalwidth, and class.
Almost right. It’s not quite clear whether the class should be counted as an attribute or not. But for the purposes of this question we do count it as an attribute. So please try again.
---
Feedback incorrect:
The Preprocess panel also shows you how many attributes, or columns, are in the current data set. In this case the attributes are sepallength, sepalwidth, petallength, petalwidth, and class.

<-- 1.14 Quiz -->
The Iris dataset
Question 3
How many possible values does the class attribute in the iris dataset have?
1
2
3
50
---
Correct answer(s):
3
---
Feedback correct:
By default the last attribute is the class, which in this case has values Iris-setosa, Iris-versicolor and Iris-virginica.
---
Feedback incorrect:
By default the last attribute is the class, which in this case has values Iris-setosa, Iris-versicolor and Iris-virginica.

<-- 1.14 Quiz -->
The Iris dataset
Question 4
Does the class Iris-setosa tend to have high or low values of sepallength?
low
high
---
Correct answer(s):
---
Feedback correct:
When you select the class attribute, you see that Iris-setosa is depicted blue (hover your mouse over each colored bar). Selecting the sepallength attribute, you can see that blue is distributed towards the left, towards lower values.

<-- 1.14 Quiz -->
The Iris dataset
Question 5
Does the class Iris-virginica tend to have high or low values of petalwidth?
low
high
---
Correct answer(s):

<-- 1.14 Quiz -->
The Iris dataset
Question 5
Does the class Iris-virginica tend to have high or low values of petalwidth?
low
high
---
Correct answer(s):
high
---
Feedback correct:
When you select the class attribute, you see that class Iris-virginia is depicted in cyan (that is, light blue). Selecting the petalwidth attribute, you can see that cyan is distributed towards the right, towards higher values. (The petallength attribute gives an equally good indication of the class.)

<-- 1.15 Video -->
The glass data
The glass dataset is a more realistic dataset with 214 instances and 10 attributes. Each instance represents a piece of glass, and its class is the type of the glass. There are 7 possible types, corresponding to different glass manufacturing processes. We interpret the attributes, and check their values for reasonableness. Weka datasets use a format called ARFF, and we take a look at the raw glass.arff data file.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
I’m going to look at a different dataset. I’m going to look at the “glass” dataset, which is a rather more extensive dataset. It’s a real world dataset, not a terribly big one. Let’s open it. Here we’ve got 214 instances and 10 attributes. Here are the 10 attributes, it’s not clear what they are. Let’s look at the “class”, by default the last attribute shown. There are seven values for the class, and the labels of these values give you some indication of what this dataset is about. We have “headlamps”, “tableware” (starting from the bottom), “containers”. Then we have “building” and “vehicle” windows, both “float” and “non-float”.
You may not know this, but there are different ways of making glass, and the floating process is a way of making glass. These are seven different kinds of glass. What are the attribute values? I don’t know what you remember about physics, and I guess it doesn’t matter if you don’t remember. RI stands for the refractive index. It’s always a good idea to check for reasonableness when you’re looking at datasets. It’s really important to get down and dirty with your data. Here we’re looking at the values of the refractive index—a minimum of 1.511, a maximum of 1.534. It’s good to think about whether these are reasonable values for refractive index.
If you go to the web and have a look around, you’ll find that these are good values for the refractive index. Na. If you did chemistry, you’ll recognize Na as sodium. Here, it looks like these are percentages, the different percentages of sodium, Magnesium, Mg, and so on. We would expect Silicon (Si) to make up the majority of glass. It varies between 69.81% and 75.41%. These are percentages of different elements in the glass. We can confirm our guesses here by looking at the data file itself. Let me just find the “glass” data. It’s in Weka datasets, and it’s glass.arff. This is the ARFF file format. It starts with a bunch of comments about the glass database.
These lines beginning with percentage signs (%) are comments. You can read about this. We don’t have time to read it now. You can see about the attributes and it does say that the attributes are refractive index, sodium, magnesium, and so on. And the type of glass, just like I said, is about windows, containers, and tableware, and so on. We get down to the end of the comments, and here we have stuff for Weka. This is the ARFF format. The relation has a name, you’ll see it printed in the interface when you look. The attributes are defined, they are real valued attributes, numeric attributes. The “type” attribute is nominal, and the different values of type are enumerated here in quotes.
That defines the relation and the attributes. Then we have an ‘@data’ line, and following that in the ARFF format, are simply the instances, one after the other, with the attribute values all on one line, ending with the class by default. This is the class value for the first instance. I think there are 214 instances here. There’s the last one. That’s the ARFF format. It is a very simple, textual file format. Now we’ve confirmed our guesses about these numbers being percentages and different elements. We can think about this some more. It’s important then, that these numbers are reasonable. If they went negative, for example, that would indicate some kind of corrupted value—you can’t have a negative percentage.
We’re expected silicon to be the majority component; we’re expecting the refractive index to be in this kind of range. It’s always a good idea when you get a dataset to just click around in the Weka interface and make sure things look real. Rather small amounts of aluminum in glass; I guess that’s not surprising; I don’t know very much about glass myself. We’re just checking for reasonableness here—a very good thing to do. That’s it then. In this lesson, we’ve looked at the classification problem. We’ve looked at the nominal weather data and the numeric weather data. We’ve talked about nominal versus numeric attributes, and we’ve talked about the ARFF file format.
We’ve looked at the glass.arff dataset, and I’ve talked about sanity checking of attributes, and the importance of getting down and dirty with your data. We’ll see you soon. Bye!
<End Transcript>

<-- 1.16 Quiz -->
More irises
Question 1
Do an image search on the web to find pictures of Iris setosa, Iris virginica and Iris versicolor to see what the different types look like.
Label the images below according to their type by choosing the correct sequence.
---
Correct answer(s):
Iris setosa
Iris versicolor
Iris virginica
---
Feedback correct:
Hint: Since this course is not about web searching, here’s a tip: you can find these images at http://www.statlab.uni-heidelberg.de/data/iris/

<-- 1.16 Quiz -->
More irises
Question 2
Which of these attributes, taken by itself, gives the best indication of the class?
sepallength
sepalwidth
petalwidth
---
Correct answer(s):
petalwidth
---
Feedback correct:
When you look at the class distribution for petalwidth, you can see that it has the least overlap of colors for all the bars.

<-- 1.16 Quiz -->
More irises
Question 3
Weka can read Comma Separated Values (.csv) format files by selecting the appropriate File Format in the Open file dialog. Ascertain by experiment how Weka determines the attribute names and value sets by creating a small spreadsheet file, saving it in Comma Separated Values (.csv) format, and loading it into Weka.
What should be the first row of a .csv format file that contains the weather data?
outlook,temperature,humidity,windy,play
sunny,hot,high,FALSE,no
sunny,hot,high,TRUE,no
rainy,mild,high,TRUE,no
---
Correct answer(s):
outlook,temperature,humidity,windy,play
---
Feedback correct:
The first row of the .csv file should give the attribute names, separated by commas, which for the weather data are outlook, temperature, humidity, windy and play.

<-- 1.16 Quiz -->
More irises
Question 3
Weka can read Comma Separated Values (.csv) format files by selecting the appropriate File Format in the Open file dialog. Ascertain by experiment how Weka determines the attribute names and value sets by creating a small spreadsheet file, saving it in Comma Separated Values (.csv) format, and loading it into Weka.
What should be the first row of a .csv format file that contains the weather data?
outlook,temperature,humidity,windy,play
sunny,hot,high,FALSE,no
sunny,hot,high,TRUE,no
rainy,mild,high,TRUE,no
---
Correct answer(s):

<-- 1.17 Video -->
Building a classifier
A classifier identifies an instance’s class, based on a training set of data. Weka makes it very easy to build classifiers. There are many different kinds, and here we use a scheme called “J48” (regrettably a rather obscure name, whose derivation is explained at the end of the video) that produces decision trees. You can visualize the trees in Weka to see what attributes they use and what tests they makes on them. Like all classifiers, J48 has a configuration panel in which you can set parameters that affect how it operates.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! Now, we’re going to actually build a classifier. We’re going to use a system called J48—I’ll tell you why it’s called J48 in a minute—to analyze the “glass” dataset that we looked at in the last lesson. I’ve got the glass dataset open here. I going to go to the Classify panel. I choose a classifier here. There are different kinds of classifiers. Weka has bayes classifiers, functions classifiers, lazy classifiers, meta classifiers, and so on. We’re going to use a tree classifier. J48 is a tree classifier. I’m going to open trees and click J48. Here is the J48 classifier. Let’s run it. If we just press start, we’ve got the dataset, we’ve got the classifier, and lo and behold, it’s done it.
It’s a bit of an anticlimax, really. Weka makes things very easy for you to do. The problem is understanding what it is that you have done. Let’s take a look. Here is some information about the datasets, the glass dataset, the number of instances and attributes. Then it’s printed out a representation of a tree here. We’ll look at these trees later on, but just note that this tree has 30 leaves and 59 nodes altogether. The overall accuracy is 66.8%. So it’s done pretty well. Down at the bottom, we’ve got a confusion matrix. Remember there were about seven different kinds of glass. This is “building windows made of float glass”.
You can see that 50 of these have been classified as ‘a’, which is correctly classified. 15 of them have been classified as ‘b’, which is “building windows non-float glass”, so those are errors, and 3 have been classified as ‘c’, and so on. This is a confusion matrix. Most of the weight is down the main diagonal, which we like to see because that indicates correct classifications. Everything off the main diagonal indicates a misclassification. That’s the confusion matrix. Let’s investigate this a bit further. We’re going to open a configuration panel for J48. Remember I chose it by clicking the Choose button. Now, if I click it here, I get a configuration panel.
I clicked J48 in this menu, and I get a configuration panel, which gives a bunch of parameters. I’m not going to really talk about these parameters. Let’s just look at one of them, the “unpruned” parameter, which by default is false. What we’ve just done is to build a pruned tree, because “unpruned” is False. We can change this to make it true and build an unpruned tree. We’ve changed the configuration. We can run it again. It just ran again, and now we have a potentially different result. Let’s just have a look. We have 67% correct classification. What did we have before? These are the runs. This is the previous run, and there we had 66.8%.
Now, in this run that we’ve just done with the unpruned tree, we got 67% accuracy, and the tree is the same size. That’s one option. I’m just going to look at another option, and then we’ll look at some trees. I’m going to click the configuration panel again, and I’m going to change the “minNumObj” parameter. What is that? That is the minimum number of instances per leaf. I’m going to change that from 2 up to 15 to have larger leaves. These are the leaves of the tree here, and these numbers in brackets are the number of instances that get to that leaf.
When there are two numbers, this means that one incorrectly classified instance got to this leaf and five correctly classified instances got there. You can see that all of these leaves are pretty small, with sometimes just two or three or—here is one with 31 instances. We’ve constrained now this number, the tree is going to be generated, and this number is always going to be 15 or more. Let’s run it again. Now we’ve got a worse result, 61% correct classification, but a much smaller tree, with only 8 leaves. Now, we can visualize this tree.
If I right-click on the line—these are the lines that describe each of the runs that we’ve done, and this is the third run—if I right-click on that, I get a little menu, and I can visualize the tree. There it is. If I right-click on empty space, I can fit this to the screen. This is the decision tree. This says first look at the Barium (Ba) content. If it’s large, then it must be headlamps. If it’s small, then Magnesium (Mg). If that’s small, then let’s look at potassium (K), and if that’s small, then we’ve got tableware. That sounds like a pretty good thing to me; I don’t want too much potassium in my tableware.
This is a visualization of the tree and it’s the same tree that you can see by looking here. This is a different representation of the same tree. I’ll just show you one more thing about this configuration panel, the “More” button. This gives you more information about the classifier, about J48. It’s always useful to look at that to see where these classifiers have come from. In this case, let me explain why it’s called J48. It’s based on a famous system called C4.5, which was described in a book. The book is referenced here. In fact, I think I’ve got on my shelf here.
This book here, “C4.5: Programs for Machine Learning” by an Australian computer scientist called Ross Quinlan. He started out with a system called ID3—I think that might have been in his PhD thesis—and then C4.5 became quite famous. This kind of morphed through various versions into C4.5. It became famous; the book came out, and so on. He continued to work on this system. It went up to C4.8, and then he went commercial. Up until then, these were all open source systems. When we built Weka, we took the latest version of C4.5, which was C4.8, and we rewrote it. Weka’s written in Java, so we called it J48. Maybe it’s not a very good name, but that’s the name that stuck.
There’s a little bit of history for you. We’ve talked about classifiers in Weka. I’ve shown you where you find the classifiers. We classified the “glass” dataset. We looked at how to interpret the output from J48, in particular the confusion matrix. We looked at the configuration panel for J48.
We looked at a couple of options: pruned versus unpruned trees and the option to avoid small leaves. I told you how J48 really corresponds to the machine learning system that most people know as C4.5. C4.5 and C4.8 were really pretty similar, so we just talk about J48 as if it’s synonymous with C4.5. See you again soon!
<End Transcript>

<-- 1.18 Quiz -->
Using J48
Question 1
Use the confusion matrix to determine how many headlamps instances were misclassified as build wind float?
1
2
3
6
7
---
Correct answer(s):
3
---
Feedback correct:
The build-wind-float results are in column “a” of the confusion matrix, and headlamps is in row “g”.
Row “g” lists 3 misclassifications for column “a”.
You may find
1.17 Building a classifier
useful.
---
Feedback incorrect:
The build-wind-float results are in column “a” of the confusion matrix, and headlamps is in row “g”.

<-- 1.18 Quiz -->
Using J48
Question 2
Open the labor.arff dataset (which was downloaded when you installed Weka), go to the Classify panel, and run the J48 classifier (with default parameters).  What is the percentage of correctly classified instances?
73.6842%
73.7%
74%
---
Correct answer(s):
74%
---
Feedback correct:
Weka gives the result as 73.6842 percent. However, the apparent accuracy is misleading, and it would be more realistic to round to a smaller number of significant digits, say 73.7 or – even better – 74.
When entering answers in future we ask you to round (round, not truncate) any percentages to the nearest integer. So here, the answer would be 74%.

<-- 1.18 Quiz -->
Using J48
Question 3
Now turn pruning off in the J48 configuration panel by setting unpruned to True and run it again. What is the percentage of correctly classified instances now?
21%
45
74%
79%
78.9474%
---
Correct answer(s):

<-- 1.18 Quiz -->
Using J48
Question 3
Now turn pruning off in the J48 configuration panel by setting unpruned to True and run it again. What is the percentage of correctly classified instances now?
21%
45
74%
79%
78.9474%
---
Correct answer(s):
79%
---
Feedback correct:
The actual percentage is 78.9474%, but (as advised in the previous question) we will round that to 79%.
---
Feedback incorrect:
That’s the percentage of incorrectly classified instances.
---
Feedback incorrect:
That’s the number of correctly classified instances, not the percentage.
---
Feedback incorrect:
That’s the answer for the previous question, with unpruned set to False
You may find
1.17 Building a classifier
useful.
---
Feedback incorrect:
It’s true that the actual percentage is 78.9474%, but (as advised in the previous question) you should round that to 79%.

<-- 1.19 Video -->
Using a filter
Weka include many filters that can be used before invoking a classifier to clean up the dataset, or alter it in some way. Filters help with data preparation. For example, you can easily remove an attribute. Or you can remove all instances that have a certain value for an attribute (e.g. instances for which humidity has the value high). Surprisingly, removing attributes sometimes leads to better classification! – and also simpler decision trees.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello! In the last lesson, we looked at using a classifier in Weka, J48.
In this lesson, we’re going to look at another of Weka’s principal features: filters. One of the main messages of this course is that it’s really important when you’re data mining to get close to your data, and to think about preprocessing it, or filtering it in some way, before applying a classifier. I’m going to start by using a filter to remove an attribute from the weather data. Let me start up the Weka Explorer and open the weather data.
I’m going to remove the “humidity” attribute: that’s attribute number 3. I can look at filters; just like we chose classifiers using this Choose button on the Classify panel, we choose filters by using the Choose button here. There are a lot of different filters. Allfilter and MultiFilter are ways of combining filters. We have supervised and unsupervised filters. Supervised filters are ones that use the class value for their operation. They aren’t so common as unsupervised filters, which don’t use the class value. There are attribute filters and instance filters. We want to remove an attribute. So we’re looking for an attribute filter. There are so many filters in Weka that you just have to learn to look around and find what you want.
I’m going to look for removing an attribute. Here we go, “Remove”. Now, before, when we configured the J48 classifier, we clicked here. I’m going to click here, and we can configure the filter. This is “A filter that removes a range of attributes from the dataset”. I can specify a range of attributes here. I just want to remove one. I think it was attribute number 3 we were going to remove. I can invert the selection and remove all the other attributes and leave 3, but I’m just going to leave it like that. Click OK, and watch “humidity” go when we apply the filter. Nothing happens until you apply the filter.
I’ve just applied it, and here we are, the “humidity” attribute has been removed. Luckily I can undo the effect of that and put it back by pressing the “Undo” button. That’s how to remove an attribute.
Actually, the bad news is there is a much easier way to remove an attribute: you don’t need to use a filter at all. If you just want to remove an attribute, you can select it here and click the “Remove” button at the bottom. It does the same job. Sorry about that. But filters are really useful, and can do much more complex things than that. Let’s, for example, imagine removing, not an attribute, but let’s remove all instances where humidity has the value “high”. That is, attribute number 3 has this first value. That’s going to remove 7 instances from the dataset. There are 14 instances altogether, so we’re going to get left with a reduced dataset of 7 instances.
Let’s look for a filter to do that. We want to remove instances, so it’s going to be an instance filter. I just have to look down here and see if there is anything suitable. How about RemoveWithValues? – the RemoveWithValues filter. I can click that to configure it, and I can click “More” to see what it does. Here it says it “Filters instances according to the value of an attribute”, which is exactly what we want. We’re going to set the “attributeIndex”; we want the third attribute (humidity), and the first value. We can remove a number of different values; we’ll just remove the first value. Now we’ve configured that. Nothing happens until we apply the filter.
Watch what happens when we apply it. We still have the “humidity” attribute there, but we have zero elements with high humidity. In fact, the dataset has been reduced to only 7 instances. Recall that when you do anything here, you can save the results. So we could save that reduced dataset if we wanted, but I don’t want to do that now. I’m going to undo this.
We removed the instances where humidity is high. We have to think about, when we’re looking for filters, whether we want a supervised or an unsupervised filter, whether we want an attribute filter or an instance filter, and then just use your common sense to look down the list of filters to see which one you want. Sometimes when you filter data you get much better classification. Here’s a really simple example. I’m going to open the “glass” dataset that we saw before. Here’s the glass dataset. I’m going to use J48, which we did before. It’s a tree classifier.
I’m going to start that, and I get an accuracy of 66.8%. Let’s remove Fe, that is, Iron. Remove this attribute, and we get a smaller dataset. Go and run J48 again. Now we get an accuracy of 67.3%. So we’ve improved the accuracy a little bit by removing that attribute. Sometimes the effect is pretty dramatic. Actually, in this dataset, I’m going to remove everything except the refractive index and Magnesium (Mg). I’m going to remove all of these attributes, and am left with a much smaller dataset with two attributes. Apply J48 again.
Now I’ve got an even better result, 68.7% accuracy. I can visualize that tree, of course – remember? – by right-clicking here and visualizing the tree, and have a look and see what it means. It’s much easier to visualize trees when they are smaller. This is a good one to look at and consider what the structure of this decision is. That’s it for now. We’ve looked at filters in Weka; supervised versus unsupervised, attribute versus instance filters. To find the right filter you need to look. They can be very powerful, and judiciously removing attributes can both improve performance and increase comprehensibility. Bye for now!
<End Transcript>

<-- 1.20 Quiz -->
Using filters
Question 1
How many attributes does the anneal dataset have?
17
39
772
898
---
Correct answer(s):
39
---
Feedback correct:
This is shown in the “Current relation” box on the “Preprocess” panel.
---
Feedback incorrect:
That was for the labor dataset. You should have loaded the anneal dataset.
---
Feedback incorrect:
That’s the number of instances, not the number of attributes, as shown in the “Current relation” box on the “Preprocess” panel.

<-- 1.20 Quiz -->
Using filters
Question 2
Apply the unsupervised attribute filter RemoveUseless. How many attributes does the anneal dataset have now?
30
32
36
39
---
Correct answer(s):
32
---
Feedback correct:
Select the filter by choosing unsupervised, attribute, and then RemoveUseless.
---
Feedback incorrect:
Select the filter by choosing unsupervised, attribute, and then RemoveUseless.
---
Feedback incorrect:
Select the filter by choosing unsupervised, attribute, and then RemoveUseless.
Don’t forget to APPLY the filter once you have selected it (Apply button)!

<-- 1.20 Quiz -->
Using filters
Question 3
Identify one of the attributes that was removed by clicking Undo and then Apply. Now figure out why it was removed.
The attribute name was too short
Only one of the attribute’s values actually appears in the dataset
The attribute only had two possible values
---
Correct answer(s):
Only one of the attribute’s values actually appears in the dataset
---
Feedback correct:
An attribute that has the same value for all instances in the dataset doesn’t yield any additional information, and Weka therefore deems it to be useless.
---
Feedback incorrect:
The attribute’s name has no influence.
---
Feedback incorrect:
The number of possible values has no influence.

<-- 1.20 Quiz -->
Using filters
Question 4
Open the glass.arff dataset (which was downloaded when you installed Weka). Apply the unsupervised attribute filter Normalize. What is the new range (i.e. minimum and maximum) of the Na attribute?
[–1, 1]
[0, 1]
[–∞, ∞]
---
Correct answer(s):
[0, 1]
---
Feedback correct:
The Normalize filter scales attributes into the range [0, 1].

<-- 1.20 Quiz -->
Using filters
Question 5
Undo the effect of the Normalize filter and bring up its configuration panel. Set the scale to 3 and the translation option to 1. Apply the filter again. What is the Na attribute’s range now?
[1, 4]
[0, 1]
[1, 3]
---
Correct answer(s):
[1, 4]

<-- 1.20 Quiz -->
Using filters
Question 6
Undo the change and check that you have reverted to the original dataset. Now apply the unsupervised attribute filter Standardize. What are the new mean and standard deviation of the K attribute?
The mean is 0.497 and the standard deviation is 0.652
The mean is –0.762 and the standard deviation is 8.76
The mean is 1 and the standard deviation is 0
The mean is 0 and the standard deviation is 1
The mean is 1.518 and the standard deviation is 0.003
---
Correct answer(s):
The mean is 0 and the standard deviation is 1
---
Feedback correct:
The Standardize filter alters the range of an attribute to give it a mean of 0 and standard deviation of 1.

<-- 1.20 Quiz -->
Using filters
Question 7
Undo all changes to the glass dataset again. Now determine which attribute set gives the highest classification accuracy using J48.
removing Fe, Si, Al, K
removing Fe, Mg, Rl
removing Fe, Si, Mg, K
---
Correct answer(s):
removing Fe, Si, Al, K

<-- 1.20 Quiz -->
Using filters
Question 7
Undo all changes to the glass dataset again. Now determine which attribute set gives the highest classification accuracy using J48.
removing Fe, Si, Al, K
removing Fe, Mg, Rl
removing Fe, Si, Mg, K
---
Correct answer(s):

<-- 1.21 Video -->
Visualizing your data
For successful data mining you must “know your data”; examine it in detail in every possible way. Weka’s Visualize panel lets you look at a dataset and select different attributes – preferably numeric ones – for the x- and y-axes. Instances are shown as points, with different colors for different classes. You can sweep out a rectangle and focus the dataset on the points inside it. You can also apply a classifier and vlsualize the errors it makes by plotting the “class” against the “predicted class”.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
One of the constantly recurring themes in this course is the necessity to get close to your data, look at it in every possible way. We’re going to use the Visualize panel. I’m going to open the Iris dataset.
I’m using it because it has numeric attributes, four numeric attributes: sepallength, sepalwidth, petallength, petalwidth.
The class are three kinds of iris flower: Iris-setosa, Iris-versicolor, and Iris-virginica. Let’s go to the Visualize panel and visualize this data. There is a matrix of two dimensional plots, a 5×5 matrix of plots. If I can select one of these plots, I’m going to be looking at a plot of sepalwidth on the x-axis and petalwidth on the y-axis. That’s a plot of the data. The colors correspond to the three classes. I can actually change the colors – if I don’t like those, I could select another color. But I’m going to leave them the way they are. I can look at individual data points by clicking on them.
This is talking about instance number 86 with a sepallength of 6, sepalwidth of 3.4, and so on. That’s a versicolor, which is why this spot is colored red. We can look individual instances. We can change the x- and y-axes by changing the menus here. Better still, if we click on this little set of bars here, these represent the attributes. I’m going to click on this and the x-axis will change to sepallength. Here the x-axis is sepalwidth. Here the x-axis is petallength, and so on. If I right-click, then it will change the y-axis to sepallength. So I can quickly browse around these different plots. There is a Jitter slider.
Sometimes points sit right on top of each other, and jitter just adds a little bit of randomness to the x- and the y-axes. With a little bit of jitter on here, the darker spots represent multiple instances. If I click on one of those, I can see that that point represents three separate instances, all of class Iris-setosa, and they all have the same value of petallength and sepalwidth – both of which are being plotted on this graph. The sepalwidth and petallength are 3.0 and 1.4 for each of the three instances. If I click another one here, here are two with very similar sepalwidths and petallengths, both have the class versicolor.
The jitter slider helps you distinguish between points that are in fact very close together. Another thing we can do is select bits of this dataset. I’m going to choose “select rectangle” here. If I draw a rectangle now, I can select these points. If I were to “submit” this rectangle, then all other points would be excluded and just these points would appear on the graph, with the axis re-scaled appropriately. Here we go. I’ve submitted that rectangle, and you can see that there are just the red points and green points there.
I can save that if I wanted as a different dataset, or I can reset it and maybe try another kind of selection like this, where I’m going to have some blue points, some red, and some green points, and see what that looks like. This might be a way of cleaning up outliers in your data, by selecting rectangles and saving the new dataset. That’s visualizing the dataset itself. What about visualizing the result of a classifier? Let’s get rid of this Visualize panel and go back to the Preprocess panel. I’m going to use a classifier. I’m going to use, guess what, J48. Let’s find it under “trees”. I’m going to run it.
Then if I right-click on this entry here in the “log” area, I can view classifier errors. Here we’ve got the class plotted against the predicted class. The square boxes represent errors. If I click on one of these, I can of course change the different axes if I want. I can change the x-axis and the y-axis. But I’m going to go back to “class” and “predictedclass”.
If I click on one of these boxes, I can see where the errors are. There are two instances where the predicted class is versicolor and the actual class is virginica. We can see these in the confusion matrix. The actual class is virginica, and the predicted class is versicolor, that’s ‘b’. This “2” entry in the confusion matrix is represented by these two instances here. If I look at another point, say this one, here I’ve got one instance, which is in fact a setosa, predicted to be a versicolor. I can look at this plot and find out where the misclassifications are actually occurring, the errors in the confusion matrix.
Get down and dirty with your data: visualize it. You can do all sorts of things. You can clean it up, detect outliers. You can look at the classification errors. For example, there’s a filter that allows you to add the classifications as a new attribute. Let’s just go and have a look at that. I’m going to go and find a filter. We’re going to add an attribute. It’s supervised because it uses the “class”.
Add an attribute: AddClassification. Here I get to choose in the configuration panel the machine learning scheme. I’m going to choose J48, of course, and I’m going to output the classification – make that “true”. That’s configured it, and I’m going to apply it. It will add a new attribute. It’s done it, and this attribute is the classification according to J48. Weka is very powerful. You can do all sorts of things with classifiers and filters.
<End Transcript>

<-- 1.22 Quiz -->
Finding misclassified instances
Question 1
Choose the J48 tree classifier, and run it (with default parameters). How many instances are misclassified?
1
2
4
6
---
Correct answer(s):
6
---
Feedback correct:
Six out of the 150 instances are misclassified.
---
Feedback incorrect:
That’s the percentage of instances that are misclassified. You were asked for the actual number of instances.

<-- 1.22 Quiz -->
Finding misclassified instances
Question 2
Visualize the classifier errors by right-clicking on the Result list, and use the visualization to determine the instance numbers of the misclassified instances. Which are they?
15, 73, 92, 98, 109, 119
4, 8, 91, 98, 109, 119
15, 72, 92, 98, 108, 120
---
Correct answer(s):
15, 73, 92, 98, 109, 119
---
Feedback correct:
Misclassified instances are shown as square boxes on the visualization plot, and clicking the box shows their instance number. If multiple instances are shown, be sure to choose only those whose predictedClass differs from their class.
---
Feedback incorrect:
Misclassified instances are shown as square boxes on the visualization plot, and clicking the box shows their instance number. If multiple instances are shown, be sure to choose only those whose predictedClass differs from their class.

<-- 1.22 Quiz -->
Finding misclassified instances
Question 3
Now switch the classifier to SimpleLogistic, which you will find in the functions category, and run it (with default parameters). How many instances are misclassified now?
3
6
9
12
15
---
Correct answer(s):
9
---
Feedback correct:
On this dataset, SimpleLogistic gives more errors than J48.
---
Feedback incorrect:
That’s the percentage of instances that are misclassified. You were asked for the actual number of instances.

<-- 1.22 Quiz -->
Finding misclassified instances
Question 4
Which instances of type Iris-versicolor are misclassified as Iris-virginica?
15, 73, 92, 98, 109, 119
15, 73, 119, 132, 135, 147, 148
80, 92
---
Correct answer(s):
15, 73, 119, 132, 135, 147, 148

<-- 1.22 Quiz -->
Finding misclassified instances
Question 4
Which instances of type Iris-versicolor are misclassified as Iris-virginica?
15, 73, 92, 98, 109, 119
15, 73, 119, 132, 135, 147, 148
80, 92
---
Correct answer(s):

<-- 1.23 Discussion -->
Reflect on this week's Big Question
The Big Question this week is, “What’s it like to do data mining?”
We said that by the end you would be able to demonstrate the use of Weka for the key tasks of examining a dataset, classifying it with a decision tree, filtering it with various filters, and visualizing the dataset and the result of learning.
So: can you?
And how about explaining (to your partner, siblings, parents or kids) … what it’s like to do data mining?
Tell your fellow learners how you get on!

<-- 1.24 Article -->
Index
At the end of each week is an index of topics covered that week.
A full index to the course appears under DOWNLOADS, below.
      Topic
      Step
      Datasets
      Anneal
      1.20
      Contact lens
      1.11
      Glass
      1.15, 1.18, 1.19, 1.20
      Iris
      1.14, 1.16, 1.21, 1.22
      Weather
      1.10, 1.13
      Classifiers
      J48
      1.17, 1.18, 1.20, 1.22
      SimpleLogistic
      1.22
      Filters
      AddClassification
      1.21
      Normalize
      1.20
      Remove
      1.19
      RemoveUseless
      1.20
      RemoveWithValues
      1.19
      Standardize
      1.20
      Plus …
      Classify panel
      1.17, 1.18
      Preprocess panel
      1.10, 1.11, 1.13, 1.14, 1.15, 1.16, 1.19
      Reading CSV files
      1.16
      Visualize panel
      1.21
      Visualize predictions
      1.22
      Weka installation
      1.7, 1.8, 1.9

<-- 2.0 Todo -->
Evaluation
How do I evaluate a classifier’s performance?
This week's Big Question!
2.1
How do I evaluate a classifier's performance?
article
Be a classifier!
WEKA incorporates many different classification algorithms. One, called the “UserClassifier,” enables you to build your own decision tree for classification. How well can you do? It’s a challenge!
2.2
Extending Weka with "packages"
article
2.3
What could possibly go wrong?
discussion
2.4
Be a classifier!
video (08:52)
2.5
Build your own classifier
quiz
2.6
How well did you do?
discussion
Training and testing
Evaluating what has been learned is an essential part of data mining. You should never evaluate on the training set! – the results will be overly optimistic. If you have a single dataset, hold some data back for testing.
2.7
Training and testing
video (04:56)
2.8
Partitioning datasets for training and testing
quiz
Repeated training and testing
Ideally, training and test sets are sampled independently from a large population. Different samples give slightly different performance estimates. More reliable results are obtained by averaging over several experimental runs.
2.9
Repeated training and testing
video (05:54)
2.10
Growing random numbers from seeds
article
2.11
Realistic performance estimates
quiz
Baseline accuracy
How do you know how well your machine learning method is doing? You should always compare it with the “baseline accuracy” obtained by simple methods. ZeroR is an extremely simple method that serves as a useful baseline. 
2.12
Baseline accuracy
video (07:21)
2.13
Comparing with the baseline
quiz
Cross-validation
Cross-validation, a standard evaluation technique, is a systematic way of running repeated percentage splits. In “stratified” cross-validation, training and test sets have the same class distribution as the full dataset.
2.14
Cross-validation
video (05:50)
2.15
Repeated cross-validation
quiz
Cross-validation results
Cross-validation is better than randomly repeating percentage split evaluations. It gives a more reliable performance estimate – that is, one with lower variance. Ten-fold cross-validation is a standard evaluation method.
2.16
Cross-validation results
video (06:30)
2.17
 More cross-validation
quiz
2.18
Reflect on this week's Big Question
discussion
How are you getting on?
We're well into the course now. Let's just take stock.
2.19
Mid-course assessment
test
This is a test step, it helps you verify your understanding. If you want to earn a Certificate of Achievement on this course you need to complete this test and any others, scoring an average of 70% or above.
To take tests you need to upgrade this course.  It costs $94, which also gets you:
Unlimited access to the course for as long as it exists on FutureLearn, so you can learn at your own pace
A Certificate of Achievement when you’re eligible, to prove what you’ve learned
Upgrade
Find out more
2.20
How are you getting on?
discussion
2.21
Index
article

<-- 2.1 Article -->
How do I evaluate a classifier's performance?
This week is all about evaluation.
Last week you downloaded Weka and looked around the Explorer and a few datasets. You used the J48 classifier. You used a filter to remove attributes and instances. You visualized some data, and classification errors. Along the way you encountered a few datasets: the weather data (both nominal and numeric versions), the glass data, and the iris data.
As you have seen, data mining involves building a classifier for a dataset. (Classification is the most common problem, though not the only one.) Given an example or “instance”, a classifier’s job is to predict its class. But how good is it? Predicting the class of the instances that were used to train the classifier is pretty trivial: you could just store them in a database. But we want to be able to predict the class of new instances, ones that haven’t come up before.
The aim is to estimate the classifier’s performance on new, unseen, instances. Testing using the training data is flawed because it “predicts” data that was used to build the classifier in the first place. We need to go beyond what we see in the training data and predict outcomes – class values – for data that has never been seen before. The only way to know whether a classifier has any value is to test it on fresh data.
But where does the fresh data come from? It’s a conundrum, and that is what we explore this week. The basic idea is to split the data into two parts: the training data and the test data. The training data is used to  build the model – the rules, if you like – that say how instances should be classified. After this is done, the test data is used to evaluate the model (rules). To do this, the model built during the training phase is applied to each test instance, and the result is the predicted class for that instances. The system compares this to the real class value defined for the instance and calculates what percentage are correct.
In the first activity you’re going to experience what it’s like to actually be a classifier yourself, by constructing a decision tree interactively. In subsequent activities we’ll look at evaluation, including training and testing, baseline accuracy, and cross-validation.
At the end of the week you will know how to evaluate the performance of a classifier on new, unseen, instances. And you will understand how easy it is to fool yourself into thinking that your system is doing better than it really is.

<-- 2.2 Article -->
Extending Weka with "packages"
Before we begin, you need to learn a little bit more about Weka.
Weka has hundreds of different classifiers and scores of filtering algorithms. To simplify the interface, when you download Weka it comes with a small set of key classifiers and filters, along with a simple mechanism that allows you to add new methods that are stored in separate “packages”.
The Package Manager is accessed from the Tools menu in Weka’s GUI Chooser panel, which appears when you start up Weka:
The very first time it is accessed, the Manager downloads information about all available packages. This requires an internet connection.
Here is the resulting display:
The top half is a scrollable list of packages, beneath which is information about the currently selected package – in this case the userClassifier package.
To install (or uninstall) a package, click the install (or uninstall) button near the top of the window. When you do this, you will be asked to close any Weka application windows – like the Explorer. But first wait for the package to finish installing (look at the progress bar). When you reopen the Explorer, the new package will be incorporated.
You can display all packages, or just the ones that are installed, or just the ones that are not installed. The list gives the package name, the broad category it belongs to, the version currently installed (if any), and the most recent version. It can be sorted, in ascending or descending order, by clicking the package or the category column header.
For successful operation the Package Manager requires Weka version 3.8.1 or later. If yours predates this you should install the latest version.
Why not install the userClassifier package now? You’ll be needing it very soon.

<-- 2.3 Discussion -->
What could possibly go wrong?
Ha!
Hopefully nothing. If you’ve successfully installed the userClassifier package, feel free to ignore this discussion.
But you never know with computers – particularly since Weka’s package manager reaches out to access packages over the internet. For example, if you are operating inside your institution’s firewall Weka may be prevented from accessing the Web because you haven’t supplied a password.
How did it go? It should be easy, but you never know … Please share your experiences, and any tips, with fellow learners. If you have problems, post them. (Don’t forget to include details such as the computer, maybe system version too.) If you’ve figured out the solution, post that. And if you can help someone else, please do so! You’re a community.
OK, I’ll kick it off:
Problem. I installed the userClassifier package and everything seemed to go fine. But when I restart the Explorer and look at the classifiers in the Classify panel, the UserClassifier isn’t there!
Solution. Hmmm. Are you sure you’re looking in the right place? Under trees? Where J48 is? Yes?? … well, maybe it didn’t install. When you clicked the Package manager’s install button, and you were asked to close any Weka application windows, did you close the Explorer, and re-open it once installation was complete? Yes? And, before closing, did you first wait for the package to finish installing (look at the progress bar)? It might take a little time. Computers are very picky about all these things: you have to get it exactly right.
Problem. The Weka package manager won’t start!
Solution. The most likely reason is that your computer doesn’t have direct access to the Internet and Java needs to be told to use a “proxy server” to access the web. The best way to achieve this is to configure an environment variable that provides the proxy details. Mumbo jumbo? See here for help.

<-- 2.4 Video -->
Be a classifier!
What’s it like to be a classifier? Load up the segment-challenge.arff dataset and examine the class values. Then choose the UserClassifier, a tree classifier. (If you can’t see it under trees, return to the preceding article and install the userClassifier package.) Specify the supplied test set segment-test.arff. Start the UserClassifier, and select the Data Visualizer panel. Select Rectangle, sweep out a rectangle, and submit it. Now select the Tree Visualizer and look at the resulting tree. Switch between these two panels, building up a tree, and then accept it. Ian’s tree classified nearly 79% of the instances correctly. How well does yours do?
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
First of all, we’re going to see what it’s like to be a classifier. We’re going to construct a decision tree ourselves, interactively.
I’m going to open up Weka here: the Weka Explorer.
I’m going to load a dataset, the segment-challenge dataset: segment-challenge.arff – that’s the one I want.
We’re going to look at this dataset. Let’s first of all look at the class. The class values are brickface, sky, foliage, cement, window, path, and grass. It looks like this is an image analysis dataset. When we look at the attributes, we see things like the centroid of columns and rows, pixel counts, line densities, means of intensities, and various other things, saturation, hue.
And the class, as I said before, is different kinds of texture: bricks, sky, foliage, and so on. That’s the segment challenge dataset. I’m going to select the “user classifier”. The user classifier is a tree classifier. We’ll see what it does in just a minute. That’s the user classifier. Before I start – this is really quite important – I’m going to use a supplied test set. I’m going to set the test set, which is used to evaluate the classifier, to be segment-test. The training set is segment-challenge; the test set is segment-test. Now we’re all set. I’m going to start the classifier.
What we see is a window with two panels: the Tree Visualizer and the Data Visualizer. Let’s start with the Data Visualizer. We looked at visualization in the last class, how you can select different attributes for x and y. I’m going to plot the region-centroid-row against the intensity-mean.
That’s the plot I get.
Now we’re going to select a class. I’m going to select Rectangle.
If I draw out with my mouse a rectangle here, I’m going to have a rectangle that’s pretty well pure reds, as far as I can see. I’m going to “submit” this rectangle. You can see that that area has gone and the picture has been rescaled. I’m building up a tree here. If I look at the Tree Visualizer, I’ve got a tree.
We’ve split on these two attributes, region-centroid-row and intensity-mean. Here we’ve got “sky”, these are all sky classes. Here we’ve got a mixture of brickface, foliage, cement, window, path, and grass. We’re going to build up this tree. What I want to do is take this node and refine it a bit more. Here’s the Data Visualizer again. I’m going to select a rectangle containing these items here, and submit that. They’ve gone from this picture. You can see that here, I’ve created this split, another split on region-centroid-row and intensity-mean, and here, this is almost all “path” – 233 “path” instances – and then a mixture here. This is a pure node we’ve got over there. This is almost a pure node here.
This is the one I want to work on. I’m going to cover some of those instances now. Let’s take this lot here and submit that. Then I’m going to take this lot here and submit that. Maybe I’ll take those ones there and submit that. This little cluster here seems pretty uniform. Submit that. I haven’t actually changed the axes, but, of course, at any time, I could change these axes to better separate the remaining classes. I could mess around with these.
Actually, a quick way to do it is to click here on these bars: left click for x and right click for y. I can quickly explore different pairs of axes to see if I can get a better split.
Here’s the tree I’ve created. I’m going to fit it to the screen. It looks like this. You can see that we have successively elaborated down this branch here. When I finish with this, I can “accept” the tree. Actually, before I do that, let me just show you that we were selecting rectangles here,
but I’ve got other things I can select: a polygon or a polyline. If I don’t want to use rectangles, I can use polygons or polylines. If you like, you can experiment with those to select different shaped areas.
There’s an area I’ve got selected … I just can’t quite finish it off … ah right, I right-clicked to finish it off. I could submit that. I’m not confined to rectangles; I can use different shapes. I’m not going to do that. I’m satisfied with this tree for the moment. I’m going to “accept” the tree. Once I do this, there is no going back, so you want to be sure. If I accept the tree, “Are you sure?” – yes. Here, I’ve got a confusion matrix, and I can look at the errors. My tree classifies 78% of the instances correctly, nearly 79% correctly, and 21% incorrectly. That’s not too bad, especially considering how quickly I built that tree.
It’s over to you now. I’d like you to play around and see if you can do better than this by spending a little bit longer on getting a nice tree. I’d like you to reflect on a couple of things. First of all, what strategy you’re using to build this tree. Basically, we’re covering different regions of the instance space, trying to get pure regions, to create pure branches. This is like a bottom-up covering strategy. We cover this area, and this area, and this area. Actually, that’s not how J48 works. When it builds trees, it tries to do a judicious split through the whole dataset.
At the very top level, it’ll split the entire dataset into two in a way that doesn’t necessarily separate out particular classes, but makes it easier when it starts working on each half of the dataset, further splitting in a top-down manner in order to try and produce an optimal tree. It will produce trees much better than the one that I just produced with the user classifier. I’d also like you to reflect on what it is we’re trying to do here. Given enough time, you could produce a “perfect” tree for the dataset, but don’t forget that the dataset we’ve loaded is the training dataset.
We’re going to evaluate this tree on a different dataset, the test dataset, which hopefully comes from the same source, but is not identical to the training dataset. We’re not trying to precisely fit the training dataset; we’re trying to fit it in a way that generalizes the kinds of patterns exhibited in the dataset. We’re looking for something that will perform well on the test data. That highlights the importance of evaluation in machine learning.
<End Transcript>

<-- 2.5 Quiz -->
Build your own classifier
What is the best accuracy that you can achieve with the UserClassifier?
< 90%
90% – 96%
96.2%
> 96.2%
---
Correct answer(s):

<-- 2.6 Discussion -->
How well did you do?
Why not post the accuracy score that you obtained for the quiz question?
I think you can get 90% without too much difficulty. J48 achieves 96.1728%. Can you do better?
But please don’t waste your time obsessing over getting a high score. The point is not to win applause but merely to understand what it feels like to have to build a classification tree.
And, in fact, the real point is to recognize that you are not seeking a “perfect” tree for the dataset you are looking at (which you could no doubt produce with sufficient time and patience), but one that performs well on a different dataset, the test dataset.

<-- 2.7 Video -->
Training and testing
How can you evaluate how well a classifier does? Training set performance is misleading. It’s like asking a child to memorize 1+1=2, 1+2=3 and then testing them on exactly the same questions, whereas you really want them to be able to answer questions like 2+3=?. We want to generalize from the training data to get a more widely applicable classifier. To evaluate how well this has been done, it must be tested on an independent test set. If you only have one dataset, set aside part of it for testing and use the rest for training.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! Here we’re going to look at training and testing in a little bit more detail. Here’s the situation. We’ve got a machine learning algorithm, and we feed into it training data, and it produces a classifier – the basic machine learning situation. For that classifier, we can test it with some independent test data. We can put that into the classifier and get some evaluation results. And, separately, we can deploy the classifier in some real situation to make predictions on fresh data coming from the environment. It’s really important in classification, when you’re looking at your evaluation results, you only get reliable evaluation results if the test data is different from the training data.
That’s what we’re going to look at in this lesson. What if you only have one dataset? If you just have one dataset, you should divide it into two parts. Maybe use some of it for training and some of it for testing. Perhaps 2/3rds of it for training and 1/3rd of it for testing. It’s really important that the training data is different from the test data. Both training and test sets are produced by independent sampling from an infinite population. That’s the basic scenario here, but they’re different independent samples. It’s not the same data. If it is the same data, then your evaluation results are misleading. They don’t reflect what you should actually expect on new data when you deploy your classifier.
Here we’re going to look at the “segment” dataset, which we used in the last lesson. I’m going to open “segment-challenge”.
I’m going to use a supplied test set. First of all, I’m going to use the J48 tree learner. I’m going to use a supplied test set, and I will set it to the appropriate “segment-test”
file, segment-test.arff. I’m going to open that. Now we’ve got a test set, and let’s see how it does. In the last lesson, on the same data with the user classifier, I think I got 79% accuracy. J48 does much better; it gets 96% accuracy on the same test set. Suppose I was to evaluate it on the training set? I can do that by specifying under Test options “Use training set”. Now it will train it again and evaluate it on the training set. Which is not what you’re supposed to do, because you get misleading results. Here it’s saying the accuracy is 99% on the training set. That is not representative of what we would get using this on independent data.
If we had just one dataset, if we didn’t have a test set, we could do a percentage split. Here’s a percentage split. This is going to be 66% training data and 34% test data. It’s going to make a random split of the dataset. If I run that, I get 95%. That’s just about the same as what we got when we had an independent test set, just slightly worse. If I were to run it again, if we had a different split, we’d expect a slightly different result. But actually, I get exactly the same result, 95.098%. That’s because Weka, before it does a run, it reinitializes the random number generator. The reason is to make sure that you can get repeatable results.
If it didn’t do that, then the results that you got would not be repeatable. However, if you wanted to have a look at the differences that you might get on different runs, then there is a way of resetting the random number generator between each run. We’re going to look at that in the next lesson. That’s this lesson. The basic assumption of machine learning is that the training and test sets are independently sampled from an infinite population, the same population. If you have just one dataset, you should hold part of it out for testing, maybe 33% as we just did or perhaps 10%.
We would expect a slight variation in results each time if we hold out a different set, but Weka produces the same results each time by design by making sure it reinitializes the random number generator each time. We ran J48 on the segment-challenge dataset. Bye for now!
<End Transcript>

<-- 2.8 Quiz -->
Partitioning datasets for training and testing
Question 1
Choose the J48 classifier (default options), select Percentage split as test option, and determine the proportion of correctly classified instances when the following percentages are used for training set size: 10%, 20%, 40%. 60%, 80%. What do you observe?
There are some fluctuations, but performance generally increases with training set size.
Performance stays about the same.
Performance always increases as training set size increases.
Performance tends to decrease as training set size increases.
---
Correct answer(s):
---
Feedback correct:
You would expect this general trend but with some fluctuation, but in this case performance always increases.
---
Feedback incorrect:
That is what you’d generally expect, but in this case one of the other answers is more likely to apply.

<-- 2.8 Quiz -->
Partitioning datasets for training and testing
Question 2
Repeat Question 1 using the training set percentages of 90%, 95%, 98%, and 99%. What happens to the number of correctly classified instances?
Stays about the same.
Increases slowly.
Decreases a little bit.
Decreases dramatically.
---
Correct answer(s):
---
Feedback correct:
The numbers of correctly classified instances are 145, 72, 29, and 15 respectively. Although these numbers are decreasing, the percentage of correctly classified instances is increasing.
---
Feedback incorrect:
Remember, you should be looking at the number (not the percentage) of correctly classified instances

<-- 2.8 Quiz -->
Partitioning datasets for training and testing
Question 3
Repeating Question 1 with a training set percentage of 99% gives a figure of 100% accuracy on the test set.
Does this mean that this generates a perfect classifier for the segmentation problem?
Yes
No
---
Correct answer(s):
---
Feedback correct:
Although the classifier is indeed 100% correct on the test set, that set only contains 15 instances (1% of the 1500 instances in the data set). This is not a reliable predictor of performance on independent test data.

<-- 2.8 Quiz -->
Partitioning datasets for training and testing
Question 4
Based on the above experiments, what is your best estimate of J48’s true accuracy on the segment-challenge dataset?
50%
90%
95%
100%
---
Correct answer(s):
---
Feedback correct:
In fact, 96% might be an even better estimate.

<-- 2.8 Quiz -->
Partitioning datasets for training and testing
Question 5
What is the likelihood that J48 would make no mistakes on 15 independently chosen test instances, if its accuracy for each instance was 95%?
Very unlikely
About 50%
Almost 100%
---
Correct answer(s):
---
Feedback correct:
In fact, the probability of getting 15 independent decisions correct is 0.95×0.95×0.95×0.95×… [15 times], which is 0.95**15 = 0.46, or 46%. (0.95**15 is 0.95 to the power 15.) If you’re unfamiliar with this math, don’t worry, we won’t be using it again.

<-- 2.8 Quiz -->
Partitioning datasets for training and testing
Question 6
Which of the following statements seems to be correct, based on your experiments?
The more training data, the greater the classifier’s success rate.
Training set size has no influence on the classifier’s success rate.
The more test data, the greater the classifier’s success rate.
---
Correct answer(s):

<-- 2.8 Quiz -->
Partitioning datasets for training and testing
Question 7
When the percentage split option is used for evaluation, how good is the performance if (a) almost none of the data is used for testing; (b) almost all of the data is used for testing?
(a) has poor performance, (b) has good performance.
(a) has better performance than (b).
The performance for (a) and (b) are similar.
---
Correct answer(s):
---
Feedback correct:
Leaving almost no data for testing means more data for training, so the algorithm has seen most patterns in the data and performs quite well on the remaining test data.

<-- 2.9 Video -->
Repeated training and testing
You can evaluate a classifier by splitting the dataset randomly into training and testing parts; train it on the former and test it on the latter. Of course, different splits produce slightly different results. If you simply re-run Weka, it repeats the same split – but you can force it to make different splits by altering the random number generator’s “seed”. If you evaluate the classifier several times you can average the results – and calculate the standard deviation.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
In this lesson, we’re going to look a little bit more at training and testing. In fact, what we’re going to do is repeatedly train and test using percentage split. Now, in the last lesson, we saw that if you simply repeat the training and testing, you get the same result each time because Weka initializes the random number generator before it does each run to make sure that you know what’s going on when you do the same experiment again tomorrow. But there is a way of overriding that. So we’ll be using independent random numbers on different occasions to produce a percentage split of the dataset into a training and test set. I’m going to open the “segment-challenge” data again.
That’s what we used before. Notice there are 1500 instances here; that’s quite a lot. I’m going to go to Classify. I’m going to choose J48, our standard method, I guess. I’m going to use a percentage split, and because we’ve got 1500 instances, I’m going to choose 90% for training and just 10% for testing. I reckon that 10% – that’s 150 instances – for testing is going to give us a reasonable estimate, and we might as well train on as many as we can to get the most accurate classifier. I’m going to run this, and the accuracy figure I get – this is what I got in the last lesson – is 96.6667%. Now, this is misleadingly high accuracy here.
I’m going to call that 96.7%, or 0.967. And then I’m going to do it again and see how much variation we get in that figure, initializing the random number generator to different amounts each time.
If I go to the “More options” menu, I get a number of options which are quite useful: outputting the model, we’re doing that; outputting statistics; we can output different evaluation measures; we’re doing the confusion matrix; we’re storing the prediction for visualization; we can output the predictions if we want; we can do a cost-sensitive evaluation; and we can set the random seed for cross-validation or percentage split. That’s set by default to 1. I’m going to change that to 2, a different random seed. We could also output the source code for the classifier if we wanted, but I just want to change the random seed. Then I want to run it again.
Before we got 0.967, and this time we get 0.94, 94%. Quite different, you see.
If I were then to change this again to, say, 3, and run it again: again I get 94%. If I change it again to 4 and run it again, I get 96.7%. Let’s do one more. Change it to 5, run it again, and now I get 95.3%. Here’s a table with these figures in. If we run it 10 times, we get this set of results. Given this set of experimental results, we can calculate the mean and standard deviation. The sample mean is the sum of all of these error figures – or these success rates, I should say – divided by the number, 10 of them. That’s 0.949, about 95%. That’s really what we would expect to get.
That’s a better estimate than the 96.7% that we started out with. A more reliable estimate. We can calculate the sample variance. We take the deviation from the mean, we subtract the mean from each of these numbers, we square that, add them up, and we divide, not by n, but by n – 1. That might surprise you, perhaps. The reason for it being n – 1 is because we’ve actually calculated the mean from this sample. When the mean is calculated from the sample, you need to divide by n – 1, leading to a slightly larger variance estimate than if you were to divide by n.
We take the square root of that, and in this case we get a standard deviation of 1.8%. Now you can see that the real performance of J48 on the segment-challenge dataset is approximately 95% accuracy, plus or minus approximately 2%. Anywhere, let’s say, between 93–97% accuracy. These figures that you get, that Weka puts out for you, are misleading. You need to be careful how you interpret them, because the result is certainly not 95.3333%. There’s a lot of variation on all of these figures. Remember, the basic assumption is the training and test sets are sampled independently from an infinite population, and you should expect a slight variation in results – perhaps more than just a slight variation in results.
You can estimate the variation in results by setting the random-number seed and repeating the experiment. You can calculate the mean and standard deviation experimentally, which is what we just did.
<End Transcript>

<-- 2.10 Article -->
Growing random numbers from seeds
In the preceding video I talk about changing the random number seed in the Weka Explorer and getting a different result. Were you mystified? An explanation follows.
Here’s the issue. Many data mining processes depend on some random process – like randomly splitting a dataset into training and test sets. This creates a conflict between getting repeatable results and realistic results. Realistically, the results should be slightly different each time, depending on the exact split. But in practice that would be a nightmare: you want to be able to repeat experiments and get the same results.
Here’s Weka’s solution. It uses a random number generator (a simple little program), but it generates the same sequence of numbers each time, so that you can do the same thing tomorrow with the same result. The sequence is controlled by number called a “seed”. You change it in the Explorer’s Classify panel, under More options. The default value is 1, but you get a different sequence of random numbers by changing the seed to something else – like 2, or 3, or 42, or anything.

<-- 2.11 Quiz -->
Realistic performance estimates
Question 1
Select Percentage split as test option and set percentage for training to 80%. How many instances will be used for training, and how many for testing?
614 and 154.
615 and 153.
691 and 77.
1211 and 289 for training and testing respectively.
---
Correct answer(s):
---
Feedback correct:
The diabetes dataset has 768 instances, and an 80 : 20 split gives 614.4 : 153.6. Weka doesn’t truncate, it rounds to the nearest integer.
---
Feedback incorrect:
Don’t forget to set the percentage for training to 80%, not 90%.
---
Feedback incorrect:
Don’t forget to open the diabetes dataset, not the segment-challenge dataset.

<-- 2.11 Quiz -->
Realistic performance estimates
Question 2
Select the J48 classifier (default options) and evaluate it with the following seed values (More options):
1,  2,  3,  4,  5
What are the minimum and maximum values for the number of incorrectly classified instances?
20 and 29.
31 and 44.
71 and 80.
110 and 123.
---
Correct answer(s):
---
Feedback correct:
The number of incorrect instances for the five seed values are 37, 35, 38, 31 and 44.
---
Feedback incorrect:
Don’t forget to look at the number of incorrectly classified instances, not the percentage.
---
Feedback incorrect:
Don’t forget to look at the number of incorrectly classified instances.

<-- 2.11 Quiz -->
Realistic performance estimates
Question 3
What is the mean of the accuracy for these five seed values?
63.3%
75.3%
76.0%
95.0%
---
Correct answer(s):
---
Feedback correct:
The random seeds generate the following accuracies: 76.0, 77.3, 75.3, 79.9, 71.4. These have a mean of 76.0%.
---
Feedback incorrect:
Did you try divide the total by 6? (You should divide it by 5)
---
Feedback incorrect:
Did you try divide the total by 4? (You should divide it by 5)

<-- 2.11 Quiz -->
Realistic performance estimates
Question 4
What is the standard deviation of the accuracy for these five seed values?
2.1
2.8
3.1
3.3
---
Correct answer(s):
---
Feedback correct:
The random seeds generate the following accuracies: 76.0, 77.3, 75.3, 79.9, 71.4. These have a standard deviation of 3.1.  Don’t forget to divide by 4 and not by 5; otherwise you will get a slightly smaller figure (2.8).
---
Feedback incorrect:
Don’t forget to divide by 4 and not by 5.

<-- 2.11 Quiz -->
Realistic performance estimates
Question 5
If you did the experiment of Q.2 with 10 different random seeds rather than 5, how would you expect this to affect the mean and standard deviation?
They would both stay about the same.
The mean would be a bit bigger but the standard deviation would be about the same.
The mean would be about the same and the standard deviation would be a little smaller.
Both the mean and standard deviation would be a bit smaller.
---
Correct answer(s):
---
Feedback correct:
The estimated mean converges to the true mean, and the estimated standard deviation converges to the true standard deviation.
---
Feedback correct:
Correct! The estimated mean converges to the true mean, and the estimated standard deviation converges to the true standard deviation. However, the n – 1 factor in the denominator of the variance formula means that the standard deviation should decrease, although the effect is very small.

<-- 2.12 Video -->
Baseline accuracy
The diabetes dataset has several attributes and a class that is either tested_negative or tested_positive (for diabetes). With Percentage split evaluation (66% training set, 34% test set), J48 yields 76% correctly classified instances. You can try other classifiers such as NaiveBayes (77%), IBk (73%), PART (74%). These results can be compared with a simple classifier called a “baseline”; the ZeroR baseline yields 65%. But in other situations the baseline does equally well – and sometimes much better than – more sophisticated classifiers. Beware!
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello again! In this lesson we’re going to look at an important new concept called “baseline accuracy”. We’re going to use a new dataset, the “diabetes” dataset. I’ve got Weka here, and I’m going to open diabetes.arff.
There it is. Have a quick look at this dataset. The class is tested_negative or tested_positive for diabetes. We’ve got attributes like “preg”, which I think has to do with the number of times they’ve been pregnant; “age”, which is the age. Of course, we can learn more about this dataset by looking at the ARFF file itself. Here’s the diabetes dataset. You can see it’s diabetes in Pima Indians.
There’s a lot of information here.
The attributes: number of times pregnant, plasma, glucose concentration, diabetes pedigree function, and so on. I’m going to use percentage split. I’m going to try a few different classifiers. Let’s look at J48 first, our old friend J48.
We get 76% with J48.
I’m going to look at some other classifiers. You learn about these classifiers later on in this course, but right now we’re just going to look at a few. Look at NaiveBayes classifier in the “bayes” category, and run that. Here we get 77%, a little bit better, but probably not significant. Let’s choose, in the “lazy” category, IBk. Again, we’ll learn about this later on. Here we get 73%, quite a bit worse. We’ll use one final one, PART, “partial rules” in the “rules” category. Here we get 74%. We’ll learn about these classifiers later, but they are just different classifiers, alternative to J48.
You can see that J48 and NaiveBayes are pretty good, probably about the same: the 1% difference between them probably isn’t significant. IBk and PART are probably about the same performance; again, 1% between them. There’s a fair gap, I guess, between those bottom two and the top two, which probably is significant. I’d like to think about these figures. 76%, is it good to get 76% accuracy? If we go back and look at this dataset, the class, we see that there are 500 negative instances and 268 positive instances.
If you had to guess, you’d guess it would be “negative”, and you’d be right 500/768 – the sum of these two things, the total number of instances – you’d be right that fraction of the time, 500/768 if you always guess “negative”, and that works out to 65%. Actually, there’s a “rules” classifier called ZeroR, which does exactly that. The ZeroR classifier just looks for the most popular class and guesses that all the time. If I run this on the training set, that will give us the exact same number, 500/768, which is 65%. It’s a very, very simple, kind of trivial classifier, that always just guesses the most popular class.
It’s OK to evaluate that on the training set, because it’s hardly using the training set at all to form the classifier. That’s what we would call the “baseline”. The baseline gives 65% accuracy, and J48 gives 76% accuracy. It’s significantly above the baseline, but not all that much above the baseline. It’s always good when you’re looking at these figures to consider what the very simplest kind of classifier, the baseline classifier, would get you. Sometimes, baseline might give you the best results. I’m going to open a dataset here. We’re not going to discuss this dataset. It’s a bit of a strange dataset, not really designed for this kind of classification. It’s called “supermarket”.
I’m going to open “supermarket”, and without even looking at it I’m just going to apply a few schemes here. I’m going to apply ZeroR, and I get 64%. I’m going to apply J48, and I think I’ll use a percentage split for evaluation because it’s not fair to use the training set here. Now I get 63%. That’s worse than the baseline! If I try NaiveBayes (these are the ones I tried before) I get again 63%, worse than the baseline. If I choose IBk – this is going to take a little while here, it’s a rather slow
scheme – here we are; it’s finished now: only 38%! That’s way, way worse than the baseline.
We’ll just try PART, partial decision rules: here we get 63%. The upshot is that the baseline actually gave a better performance than any of these classifiers, and one of them was really atrocious compared with the baseline. This is because, for this dataset, the attributes are not really informative.
The rule here is: don’t just apply Weka to a dataset blindly. You need to understand what’s going on. When you do apply Weka to a dataset, always make sure that you try the baseline classifier, ZeroR, before doing anything else. In general, simplicity is best. Always try simple classifiers before you try more complicated ones. Also, you should consider, when you get these small differences, whether the differences are likely to be significant. We saw 1% differences in the last lesson that were probably not at all significant. You should always try a simple baseline. You should look at the dataset. We shouldn’t blindly apply Weka to a dataset; we should try to understand what’s going on.
<End Transcript>

<-- 2.13 Quiz -->
Comparing with the baseline
Question 1
The iris.arff dataset consists of three classes (Iris-setosa, Iris-versicolor, Iris-virginica), with 50 instances each. What is the accuracy of ZeroR on this dataset when testing on the training set?
10%
33%
50%
66%
---
Correct answer(s):
33%
---
Feedback correct:
It’s a tie, so ZeroR chooses one of the three classes as the the majority class and always predicts it, achieving a success rate of 50/150, or 33.3%.

<-- 2.13 Quiz -->
Comparing with the baseline
Question 2
In practice, what is ZeroR’s success rate on the iris dataset when evaluated using the default (66%) Percentage split?
29%
31%
71%
96%
---
Correct answer(s):
29%
---
Feedback correct:
There is some statistical variation from the expected figure because the training and test sets are randomly sampled. (Note: to get this result, Weka’s random seed should have its default value of 1).
---
Feedback incorrect:
Is your seed value set to 2 or 3? Make sure the default seed is set at a value of 1.
---
Feedback incorrect:
Make sure you are looking that the Correctly Classified Instances and not the Incorrectly Classified Instances.

<-- 2.13 Quiz -->
Comparing with the baseline
Question 3
Open the glass.arff dataset, go to the Classify tab, and use Percentage split with the default value of 66% as the evaluation method.
What is ZeroR’s accuracy (in percent)?
27%
28%
33%
34%
---
Correct answer(s):
27%
---
Feedback correct:
(Weka gives the result as 27.3973%)
---
Feedback incorrect:
Did you use percentage split (66%) and random seed of 1?

<-- 2.13 Quiz -->
Comparing with the baseline
Question 4
What is J48’s accuracy on the glass dataset, using default parameter values?
38%
57%
58%
62%
---
Correct answer(s):
58%
---
Feedback correct:
(Weka gives the result as 57.5342%)
---
Feedback incorrect:
Did you use Percentage split (66%) and random seed of 1?

<-- 2.13 Quiz -->
Comparing with the baseline
Question 5
What is NaiveBayes’ accuracy on the glass dataset, using default parameter values?
39%
43%
49%
51%
---
Correct answer(s):
49%
---
Feedback correct:
(Weka gives the result as 49.3%)
---
Feedback incorrect:
Did you use Percentage split (66%) and random seed of 1?

<-- 2.13 Quiz -->
Comparing with the baseline
Question 6
Open the segment-challenge.arff dataset, go to the Classify tab, and evaluate using segment-test.arff as the test set.
What is ZeroR’s accuracy?
12%
14%
16%
88%
---
Correct answer(s):
---
Feedback correct:
(Weka gives the result as 11.6049%)
---
Feedback incorrect:
Did you use Supplied test set?
---
Feedback incorrect:
Are you looking at the Incorrectly classified Instances instead of the Correctly classified Instances?

<-- 2.13 Quiz -->
Comparing with the baseline
Question 6
Open the segment-challenge.arff dataset, go to the Classify tab, and evaluate using segment-test.arff as the test set.
What is ZeroR’s accuracy?
12%
14%
16%
88%
---
Correct answer(s):

<-- 2.13 Quiz -->
Comparing with the baseline
Question 7
What is IBk’s accuracy for segment-challenge evaluated on segment-test, using default parameter values?
31%
95%
96%
98%
---
Correct answer(s):
---
Feedback correct:
(Weka gives the result as 95.8025 percent)
---
Feedback incorrect:
Are you using Percentage split? You should be using the Supplied test set.
---
Feedback incorrect:
Are you using the Supplied test set?

<-- 2.13 Quiz -->
Comparing with the baseline
Question 8
What is PART’s accuracy for segment-challenge evaluated on segment-test, using default parameter values?
11%
88%
95%
96%
---
Correct answer(s):
---
Feedback correct:
(Weka gives the result as 95.679%)
---
Feedback incorrect:
Did you use the Supplied test set?

<-- 2.14 Video -->
Cross-validation
Cross-validation, a standard evaluation technique, is a systematic way of running repeated percentage splits. Divide a dataset into 10 pieces (“folds”), then hold out each piece in turn for testing and train on the remaining 9 together. This gives 10 evaluation results, which are averaged. In “stratified” cross-validation, when doing the initial division we ensure that each fold contains approximately the correct proportion of the class values. Having done 10-fold cross-validation and computed the evaluation results, Weka invokes the learning algorithm a final (11th) time on the entire dataset to obtain the model that it prints out.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! In this lesson, I want to introduce you to the standard way of evaluating the performance of a machine learning algorithm, which is called “cross-validation”. A couple of lessons back, we looked at evaluating on an independent test set, and we also talked about evaluating on the training set (don’t do that). We also talked about evaluating using the “holdout” method by taking the one dataset and holding out a little bit for testing and using the rest for training. There is a fourth option on Weka’s Classify panel, which is called “cross-validation”, and that’s what we’re going to talk about here. Cross-validation is a way of improving upon repeated holdout. We tried using the holdout method with different random-number seeds each time.
That’s called “repeated holdout”. Cross-validation is a systematic way of doing repeated holdout that actually improves upon it by reducing the variance of the estimate. We take a training set and we create a classifier. Then we’re looking to evaluate the performance of that classifier, and there’s a certain amount of variance in that evaluation, because it’s all statistical underneath. We want to keep the variance in the estimate as low as possible. Cross-validation is a way of reducing the variance, and a variant on cross-validation called “stratified cross-validation” reduces it even further. I’m going to explain that in this class. In a previous lesson, we held out 10% for the testing and we repeated that 10 times. That’s the “repeated holdout” method.
We’ve got one dataset, and we divided it independently 10 separate times into a training set and a test set. With cross-validation, we divide it just once, but we divide into, say, 10 pieces. Then we take 9 of the pieces and use them for training and the last piece we use for testing. Then with the same division, we take another 9 pieces and use them for training and the held-out piece for testing. We do the whole thing 10 times, using a different segment for testing each time. In other words, we divide the dataset into 10 pieces, and then we hold out each of these pieces in turn for testing, train on the rest, do the testing and average the 10 results.
That would be 10-fold cross-validation. Divide the dataset into 10 parts (these are called “folds”); hold out each part in turn; and average the results. So each data point in the dataset is used once for testing and 9 times for training. That’s 10-fold cross-validation. “Stratified” cross-validation is a simple variant where, when we do the initial division into 10 parts, we ensure that each fold has got approximately the correct proportion of each of the class values.
Of course, there are many many many different ways of dividing a dataset into 10 equal parts: we just make sure we choose a division that has approximately the right representation of class values in each of the folds. That’s stratified cross-validation. It helps reduce the variance in the estimate a little bit more. Then, once we’ve done the cross-validation, what Weka does is run the algorithm an eleventh time on the whole dataset. That will produce a classifier that we might deploy in practice. We use 10-fold cross-validation in order to get an evaluation result and estimate of the error, and then finally we do classification one more time to get an actual classifier to use in practice.
That’s what I wanted to tell you. Cross-validation is better than repeated holdout, and we’ll look at that in the next lesson. Stratified cross-validation is even better. Weka does stratified cross-validation by default. And with 10-fold cross-validation, Weka invokes the learning algorithm 11 times, one for each fold of the cross-validation and then a final time on the entire dataset. A practical rule of thumb is that if you’ve got lots of data you can use a percentage split, and evaluate it just once. Otherwise, if you don’t have too much data, you should use stratified 10-fold cross-validation. How big is lots? Well, this is what everyone asks. How long is a piece of string, you know?
It’s hard to say, but it depends on a few things. It depends on the number of classes in your dataset. If you’ve got a two-class dataset, then if you had, say 100–1000 data points, that would probably be good enough for a pretty reliable evaluation if you did 90% and 10% split into the training and test set. If you had, say 10,000 data points in a two-class problem, then I think you’d have lots and lots of data, you wouldn’t need to go to cross-validation. If, on the other hand, you had 100 different classes, then that’s different, right? You would need a larger dataset, because you want a fair representation of each class when you do the evaluation.
It’s really hard to say exactly; it depends on the circumstances. If you’ve got thousands and thousands of data points, you might just do things once with holdout. If you’ve got less than a thousand data points, even with a two-class problem, then you might as well do 10-fold cross-validation. It really doesn’t take much longer. Well, it takes 10-times as long, but the times are generally pretty short.
<End Transcript>

<-- 2.15 Quiz -->
Repeated cross-validation
Question 1
What is the minimum and maximum value of the results obtained by ZeroR on the iris dataset using cross-validation with 10, 11, 12, 13, 14, and 15 folds?
They’re all 33%
They’re all 50%
28% and 35%
28% and 33%
---
Correct answer(s):
---
Feedback correct:
14-fold cross-validation gives 28%, and 10-fold cross-validation gives 33%.

<-- 2.15 Quiz -->
Repeated cross-validation
Question 2
It’s curious that all values obtained in the previous question were less than or equal to ZeroR’s true accuracy value of 33% on this dataset. Is this a coincidence?
Yes
No
---
Correct answer(s):
---
Feedback correct:
It’s not a coincidence. For each fold of the cross-validation, ZeroR chooses the majority class in the training set. Because there are a fixed number (50) of each class in the entire dataset, a class that is in the majority in the training set cannot be in the majority in the test set. Thus the success rate cannot possibly exceed 33% on any fold of the cross-validation.

<-- 2.15 Quiz -->
Repeated cross-validation
Question 3
Suppose the accuracy of ZeroR on the iris dataset were evaluated using cross-validation with 5, 10, and 25 folds. Without doing the experiment, what would you expect the accuracies to be?
All exactly 33%
All around about 33%, some bigger, some smaller
All 33% or perhaps a little bit smaller
---
Correct answer(s):
---
Feedback correct:
With 5 folds the test set contains 30 instances; with 10 folds it contains 15; and with 25 folds it contains 6. These are all exact multiples of the number of classes (3). The number of instances in the training set is also an exact multiple of the number of classes in each case. Because Weka does stratified cross-validation, for each fold it is able to get an equal number of each class in both training and test sets. Thus the accuracy for each fold of the cross-validation is exactly 33.3%.

<-- 2.15 Quiz -->
Repeated cross-validation
Question 4
What would ZeroR’s success rate on the iris datset be if evaluated using 150-fold cross-validation? Think carefully about this first, and then confirm your answer using Weka.
0%
29%
33%
50%
66%
---
Correct answer(s):
---
Feedback correct:
Surprising, eh? Here’s why. The dataset contains 150 instances, so 150-fold cross-validation sets aside exactly one for testing, and chooses the majority class of the remaining 149 training instances. That class will never be the same as the left-out instance’s class, because the left-out class can never be the majority class in the training set.

<-- 2.16 Video -->
Cross-validation results
Cross-validation is better than randomly repeating percentage split evaluations. The reason is that each instance occurs exactly once in a test set, and is tested just once. Repeated random splits are liable to produce less reliable results: the average will be about the same but the variance is higher. This is confirmed with an experiment on the diabetes dataset: 10 repeated percentage splits yield a variance of 4.6%, as opposed to 0.9% with 10-fold cross-validation. Why 10-fold? Good question! It seems to be a reasonable compromise.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! Good to see you again. One of the things I like to do with my time is play music, and that little bit of Mozart you hear at the beginning of these videos, that’s me and three friends playing a clarinet quartet I play in an orchestra, and last night I was playing some jazz with a little trio. If you want to hear us play, if you go to Google and find my home page, type my name, “Ian Witten”, you’ll get me here, and every time you visit this page
I’ll play you a tune.
If you refresh the page, I’ll play you another tune.
That’s what I do. Anyway, that’s not what we’re here for. We learned about cross-validation in the last lesson. I said that cross-validation was a better way of evaluating your machine learning algorithm, evaluating your classifier, than repeated holdout, repeating the holdout method. Cross-validation does things 10 times. You can use holdout to do things 10 times, but cross-validation is a better way of doing things. Let’s just do a little experiment. I’m going to start up Weka and open the “diabetes” dataset. Here it is, diabetes.arff, and the baseline accuracy, which ZeroR gives me – that’s the default classifier, by the way, rules/ZeroR – if I just run that, well, it will evaluate it using cross-validation.
Actually, for a true baseline, I should just use the training set. That’ll just look at the chances of getting a correct result if we simply guess the most likely class, in this case 65.1%. That’s the baseline accuracy. That’s the first thing you should do with any dataset. Then we’re going to look at J48, which is down here under “trees”. There it is. I’m going to evaluate it with 10-fold cross-validation.
It takes just a second to do that. I get a result of 73.8%, and we can change the random-number seed like we did before. The default is 1; let’s put a random-number seed of 2. Run it again. I get 75%. Do it again. Change it to, say, 3 (I can choose anything I want, of course), run it again, and I get 75.5%. These are the numbers I get on this slide with 10 different random-number seeds. Those are the same numbers on this slide in the right-hand column, the 10 values I got, 73.8%, 75.0%, 75.5%, and so on.
I can calculate the mean, which for that right-hand column is 74.5%, and the sample standard deviation, which is 0.9%, using just the same formulas that we used before. Before we used these formulas for the holdout method, we repeated the holdout 10 times. These are the results you get on this dataset if you repeat holdout using 90% for training and 10% for testing – which is, of course, what we’re doing with 10-fold cross-validation. I would get those results there, and if I average those I get a mean of 74.8%, which is satisfactorily close to 74.5%, but I get a larger standard deviation, quite a lot larger standard deviation of 4.6%, as opposed to 0.9% with cross-validation.
Now, you might be asking yourself why use 10-fold cross-validation?
With Weka we can use 20-fold cross-validation, or anything: we just set the number of folds here beside the cross-validation box to whatever we want. So we could use 20-fold cross-validation.
What that would do is divide the dataset into 20 equal parts and repeat 20 times: take one part out, train on the other 95% of the dataset. And then do it a 21st time on the whole dataset. So why 10, why not 20? Well, that’s a good question really, and there’s not a very good answer. We want to use quite a lot of data for training, because in the final analysis we’re going to use the entire dataset for training. If we’re using 10-fold cross-validation, then we’re using 90% of the dataset. Maybe it would be a little better to use 95% of the dataset for training, with 20-fold cross-validation.
On the other hand, we want to make sure that what we evaluate on is a valid statistical sample. So in general, it’s not necessarily a good idea to use a large number of folds with cross-validation. Also, of course, 20-fold cross-validation will take twice as long as 10-fold cross-validation. The upshot is that there isn’t a really good answer to this question, but the standard thing to do is to use 10-fold cross-validation, and that’s why it’s Weka’s default. We’ve shown in this lesson that cross-validation really is better than repeated holdout. Remember, on the last slide we found that we got about the same mean for repeated holdout as for cross-validation, but we got a much smaller variance for cross-validation.
We know that the evaluation of this machine learning method J48 on this dataset, “diabetes”, gives 74.5% accuracy, probably somewhere between 73.5% and 75.5%. That is actually substantially larger than the baseline. So J48 is doing something for us, better than the baseline. Cross-validation reduces the variance of the estimate. Bye for now!
<End Transcript>

<-- 2.17 Quiz -->
 More cross-validation
Question 1
What is the mean value of the accuracy with random seeds of 11, 12, 13, 14, and 15?
79.8%
95.4%
95.6%
95.7%
---
Correct answer(s):
---
Feedback correct:
The random seeds generated the following accuracies: 96.1, 96.3, 94.5, 95.7, 96.1. This results in a mean of 95.7%.

<-- 2.17 Quiz -->
 More cross-validation
Question 2
What is the standard deviation of the accuracy?
0.60%
0.65%
0.73%
0.74%
---
Correct answer(s):
---
Feedback correct:
The random seeds generate the following raw accuracies: 96.0667, 96.2667, 94.4667, 95.6667, 96.1333. This results in a standard deviation of 0.7354%.
(Note: you should divide by 4, not 5, otherwise you will get the smaller estimate of 0.66%. Technically, you are calculating the standard deviation of a “sample” rather than a “population”. If you are using Excel, use STDEV.S, not STDEV.P.)
---
Feedback incorrect:
You should divide by 4, not 5, otherwise you get this smaller estimate.
Technically, you are calculating the standard deviation of a “sample” rather than a “population”. If you are using Excel, use STDEV.S, not STDEV.P.
---
Feedback incorrect:
The random seeds generate the following (rounded) accuracies: 96.1, 96.3, 94.5, 95.7, 96.1. This results in a standard deviation of 0.728%.
However, you should really average the raw accuracies, not the rounded ones. That leads you to a slightly different answer …
(Note: you should divide by 4, not 5, otherwise you will get the smaller estimate of 0.65%. Technically, you are calculating the standard deviation of a “sample” rather than a “population”. If you are using Excel, use STDEV.S, not STDEV.P.)

<-- 2.17 Quiz -->
 More cross-validation
Question 3
When you performed the above experiment, how many times did Weka run the J48 algorithm?
5
6
50
55
60
---
Correct answer(s):
---
Feedback correct:
Each 10-fold cross-validation involves running J48 eleven times, once for each fold and once at the end, on the entire dataset. And Weka did this five times.

<-- 2.17 Quiz -->
 More cross-validation
Question 4
With the same dataset, select Percentage split as the test option with 90% as the parameter. Evaluate J48 with the same seed values as before:
11, 12, 13, 14, 15
What is the mean of the accuracy?
80.5%
96.2%
96.5%
96.6%
---
Correct answer(s):
---
Feedback correct:
The random seeds generated the following accuracies: 98.7, 95.3, 98.0, 96.0, 94.7. This results in a mean of 96.5%.

<-- 2.17 Quiz -->
 More cross-validation
Question 5
What is the standard deviation of the accuracy?
1.25%
1.55%
1.73%
1.81%
---
Correct answer(s):
---
Feedback correct:
The random seeds generated the following accuracies: 98.7, 95.3, 98.0, 96.0, 94.7. This results in a standard deviation of 1.73%.
(Note: you should divide by 4, not 5, otherwise you will get the smaller estimate of 0.65%. Technically, you are calculating the standard deviation of a “sample” rather than a “population”. If you are using Excel, use STDEV.S, not STDEV.P.)
---
Feedback incorrect:
You should divide by 4, not 5, otherwise you get this smaller estimate.
Technically, you are calculating the standard deviation of a “sample” rather than a “population”. If you are using Excel, use STDEV.S, not STDEV.P.

<-- 2.17 Quiz -->
 More cross-validation
Question 6
When you performed the above experiment, how many times did Weka run the J48 algorithm?
5
6
10
50
55
60
---
Correct answer(s):
---
Feedback correct:
For each of the five runs Weka executes J48 twice, one on the 90% training set and again on the whole dataset (just as in cross-validation).
---
Feedback incorrect:
Not quite correct. For each run Weka executes J48 twice, one on the 90% training set and again on the whole dataset (just as in cross-validation).

<-- 2.18 Discussion -->
Reflect on this week's Big Question
The Big Question this week is, “How do I evaluate a classifier’s performance?”
At the beginning of the week we promised that by the end you would know how to evaluate the performance of a classifier on new, unseen, instances. And you would understand how easy it is to fool yourself into thinking that your system is doing better than it actually is.
So: how do you evaluate a classifier’s performance?
Try explaining (to your partner, siblings, parents or kids) how to evaluate the performance of a learning system when you don’t even know what data it will be used on. Can you convince them why evaluating it on the data used to train it is an absolutely terrible idea?
Tell your fellow learners how you get on.

<-- 2.19 Quiz -->
Mid-course assessment
Question 1
Open the blood_fat_corrupted.arff dataset.
Note: this is not in your Weka installation; you must download it from the above link. Make sure you save it as blood_fat_corrupted.arff and not blood_fat_corrupted.arff.txt, otherwise Weka won’t see it.
How many instances are there? How many attributes? What are the possible class values?
Select all the answers you think are correct.
26 instances
27 instances
2 attributes
3 attributes
class values low, medium, very_high
class values low, medium, high, very_high
class values weight, age, blood_fat
---
Correct answer(s):
27 instances
3 attributes
class values low, medium, high, very_high

<-- 2.19 Quiz -->
Mid-course assessment
Question 2
The blood_fat_corrupted dataset contains two corrupted values. What instances and attributes do they occur in?
Select all the answers you think are correct.
instance 5, attribute age
instance 5, attribute weight
instance 6, attribute age
instance 6, attribute weight
instance 7, attribute age
instance 7, attribute weight
instance 11, attribute age
instance 25 , attribute weight
---
Correct answer(s):
instance 5, attribute weight
instance 6, attribute age

<-- 2.19 Quiz -->
Mid-course assessment
Question 3
Open the soybean.arff dataset. Use Weka to determine J48’s classification accuracy, evaluated using a 95% split into training and test instances. What is the percentage of correctly classified instances?
90.5%
91.5%
93.3%
96.3%
97.1%
---
Correct answer(s):
97.1%

<-- 2.19 Quiz -->
Mid-course assessment
Question 4
What is J48’s classification accuracy on the soybean dataset, evaluated using 20-fold cross-validation with a random number seed of 42?
91.5%
92.2%
92.4%
92.9%
93.0%
---
Correct answer(s):
93.0%

<-- 2.19 Quiz -->
Mid-course assessment
Question 5
---
Correct answer(s):

<-- 2.19 Quiz -->
Mid-course assessment
Question 6
Determine the performance of ZeroR, NaiveBayes, IBk, and OneR on the soybean dataset, evaluated using a 90% split into training and test instances (with the default random number seed of 1). Which order do they come in?
---
Correct answer(s):

<-- 2.19 Quiz -->
Mid-course assessment
Question 7
Imagine a dataset with 150 instances and 3 class values. 25 of the instances belong to the first class, 50 to the second, and 75 to the third. What is the accuracy of ZeroR for this dataset, when evaluated on the training set?
25%
33%
50%
66%
75%
---
Correct answer(s):
50%

<-- 2.19 Quiz -->
Mid-course assessment
Question 8
For the dataset in the previous question (150 instances, of which 25, 50 and 75 belong to the three classes), would you expect exactly 50% accuracy if ZeroR were evaluated using (a) a 90% split into training and test instances, and (b) 10-fold cross-validation?
Select all the answers you think are correct.
(a) no
(a) yes
(b) no
(b) yes
---
Correct answer(s):
(a) no
(b) yes

<-- 2.19 Quiz -->
Mid-course assessment
Question 9
For the ionosphere.arff dataset, evaluate J48 five times using a 80% split into training and test instances, with the five random seeds 1, 4, 5, 6, and 8. What is the mean and standard deviation of the classification accuracy?
Select all the answers you think are correct.
mean is 86.0%
mean is 89.7%
standard deviation is 0.3%
standard deviation is 4.5%
standard deviation is 4.9%
standard deviation is 5.5%
---
Correct answer(s):
mean is 86.0%
standard deviation is 5.5%
---
Feedback correct:
You may find
1.18 Using J48
useful.

<-- 2.19 Quiz -->
Mid-course assessment
Question 10
If J48 was evaluated on the ionosphere dataset using a single 5-fold cross-validation, would you expect the performance estimate to be more reliable or less reliable than the mean value obtained in Question 9?
about the same
less reliable
more reliable provided the cross-validation is stratified
more reliable regardless of whether or not the cross-validation is stratified
---
Correct answer(s):
more reliable regardless of whether or not the cross-validation is stratified

<-- 2.19 Quiz -->
Mid-course assessment
Question 10
If J48 was evaluated on the ionosphere dataset using a single 5-fold cross-validation, would you expect the performance estimate to be more reliable or less reliable than the mean value obtained in Question 9?
about the same
less reliable
more reliable provided the cross-validation is stratified
more reliable regardless of whether or not the cross-validation is stratified
---
Correct answer(s):

<-- 2.20 Discussion -->
How are you getting on?
By now the course is well underway: two weeks down and three to go.
    What do you think of it so far?
    Do you want to discuss any issues that have arisen?
This is the place to do it!

<-- 2.21 Article -->
Index
(A full index to the course appears at the end of Week 1.)
      Topic
      Step
      Datasets
      Diabetes
      2.11, 2.12, 2.16
      Glass
      2.13
      Iris
      2.13, 2.15
      Segment-challenge/test
      2.4, 2.5, 2.7, 2.8, 2.9, 2.13
      Classifiers
      IBk
      2.12, 2.13
      J48
      2.7, 2.9, 2.11, 2.12, 2.13, 2.16, 2.17
      NaiveBayes
      2.12, 2.13
      PART
      2.12, 2.13
      UserClassifier
      2.4, 2.5
      ZeroR
      2.12, 2.13, 2.15, 2.16
      Packages
      UserClassifier
      2.2
      Plus …
      Baseline accuracy
      2.12, 2.13
      Cross-validation
      2.14, 2.15, 2.16, 2.17
      Installing packages
      2.2, 2.3
      Random number seed
      2.9, 2.10, 2.11
      Test options
      2.7, 2.8, 2.11, 2.13, 2.14, 2.15, 2.16, 2.17

<-- 3.0 Todo -->
Simple classifiers
How do simple classification methods work?
This week's Big Question!
3.1
How do simple classification methods work?
article
Simplicity first
Always try simple methods before complex ones! (A good maxim for life in general, not just data mining.) Sometimes, simple algorithms perform really well. We learn about OneR, a simple method that is sometimes quite effective. 
3.2
Simplicity first
video (07:17)
3.3
OneR vs. the baseline
quiz
Overfitting
“Overfitting” is a general problem that plagues all machine learning methods. It’s when a classifier fits the training data too tightly. The classifier works well on the training data but not on independent test data. 
3.4
Overfitting
video (07:51)
3.5
Overfitting
quiz
Using probabilities
Why not use all attributes, equally weighted, instead of a single one as OneR does. Bayes' Theorem provides a sound probabilistic foundation for this. "Naive" Bayes assumes that attributes are equally important, and independent.
3.6
Using probabilities
video (11:32)
3.7
Naive Bayes with dependent attributes
quiz
3.8
Problems with probabilities?
discussion
Decision trees
Decision trees are another simple classification method, based on a top-down, recursive, divide-and-conquer strategy. J48 (aka C4.5) finds a good attribute to split on at each stage using a measure called "information gain."
3.9
Decision trees
video (08:25)
3.10
Weka's output 
quiz
Pruning decision trees
Decision trees can easily overfit the training data, and pruning techniques are needed to guard against overfitting. There are various different methods. Unfortunately, this is where elegant algorithms get messy!
3.11
Pruning decision trees
video (09:45)
3.12
The effect of pruning
quiz
Nearest neighbor
How about storing the training instances and giving new instances the same classification as their nearest neighbor? A similarity function is needed to select the closest instance. Using several neighbors can improve performance.
3.13
Nearest neighbor
video (07:43)
3.14
Instance-based learning
quiz
3.15
Reflect on this week's Big Question
discussion
3.16
Index
article

<-- 3.1 Article -->
How do simple classification methods work?
Simplicity first!
That’s the underlying theme of this whole course on data mining. Always start by checking how well simple methods work on your dataset, before progressing to more complicated things. You will be surprised by how often methods that seem simplistic do better than ones that are far more sophisticated. Real life data mining abounds with systems that are much more complex than they need to be – and actually perform worse than simple methods.
Simplicity comes in many different flavors. You could choose a single attribute and base the decision on that, ignoring the others. Or you could assume that each attribute contributes to the final decision independently, and in equal measure. Or you could build a simple branching structure that tests a few attributes sequentially. Or you could store the training instances and give new instances the same classification as their nearest neighbor – or take into account several nearest neighbors.
At the end of this week you will be able to choose learning methods based on each of these “flavors” of simplicity. You will be able to explain how they work, apply them to a dataset of your choice, and interpret the output that they produce.
Warning! These methods are really simple. You might be disappointed. If so, just wait till next week!

<-- 3.2 Video -->
Simplicity first
In data mining you should always try simple things before you try more complicated things. There are many different kinds of simple structure. One is to create a one-level decision tree, or a set of rules that all test one particular attribute. Here one attribute does all the work: all you need to do is to choose which attribute. The method is called OneR, and it’s easy to see how it operates on the weather data. Sometimes, simple methods like this work surprisingly well.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
We're going to start out emphasizing the message that simple algorithms often work very well. In data mining, maybe in life in general, you should always try simple things before you try more complicated things. There are many different kinds of simple structure, For example, it might that one attribute in the dataset does all the work, everything depends on the value of one of the attributes. Or, it might be that all of the attributes contribute equally and independently. Or a simple structure might be a decision tree that tests just a few of the attributes. We might calculate the distance from an unknown sample to the nearest training sample, or a result my depend on a linear combination of attributes.
We're going to look at all of these simple structures in the next few lessons. There's no universally best learning algorithm. The success of a machine learning method depends on the domain. Data mining really is an experimental science. We're going to look at OneR rule learner, where one attribute does all the work. It's extremely simple, very trivial, actually, but we're going to start with simple things and build up to more complex things. OneR learns what you might call a one-level decision tree, or a set of rules that all test one particular attribute.
A tree that branches only at the root node depending on the value of a particular attribute, or, equivalently, a set of rules that test the value of that particular attribute. The basic version of OneR, there's one branch for each value of the attribute. We choose which attribute first, and we make one branch for each possible value of the attribute. Each branch assigns the most frequent class that comes down that branch. The error rate is the proportion of instances that don't belong to the majority class of their corresponding branch. We choose the attribute with the smallest error rate. Let's look at what this actually means. Here's the algorithm. For each attribute, we're going to make some rules.
For each value of the attribute, we're going to make a rule that counts how often each class appears, finds the most frequent class, makes the rule assign that most frequent class to this attribute value combination, and then we're going to calculate the error rate of this attribute's rules. We're going to repeat that for each of the attributes in the dataset, and choose the attribute with the smallest error rate. Here's the weather data again. What OneR does, is it looks at each attribute in turn, outlook, temperature, humidity, and wind, and forms rules based on that.
For outlook, there are three possible values: sunny, overcast, and rainy. We just count out of the 5 sunny instances, 2 of them are yeses and 3 of them are nos.
We're going to choose a rule, if it's sunny choose no. We're going to get 2 errors out of 5. For overcast, all of the 4 overcast values of outlook lead to yes values for the class play. So, we're going to choose the rule, if outlook is overcast, then yes, giving us 0 errors. Finally, for outlook is rainy, we're going to choose yes, as well, and that would also give us 2 errors out of the 5 instances. We've got a total number of errors if we branch on outlook of 4. We can branch on temperature and do the same thing. When temperature is hot, there are 2 nos and 2 yeses.
We just choose arbitrarily in the case of a tie, so we'll choose if it's hot, let's predict no, getting 2 errors. If temperature is mild, we'll predict yes, getting 2/6 errors, and if the temperature is cool, we'll predict yes, getting 1 out of the 4 instances as an error. And the same for humidity and wind. We look at the total error values; we choose the rule with the lowest total error value -- either outlook or humidity. That's a tie, so we'll just choose arbitrarily, and choose outlook. That's how OneR works, it's as simple as that. Let's just try it. Here's Weka. I'm going to open the nominal weather data.
I'm going to go to Classify. This is such a trivial dataset that the results aren't very meaningful, but if I just run ZeroR to start off with, I get an error rate of 64%. If I now choose OneR, and run that. I get a rule, and the rule I get is branched on outlook, if it's sunny then choose no, overcast choose yes, and rainy choose yes. We get 10 out of 14 instances correct on the training set. We're evaluating this using cross-validation. Doesn't really make much sense on such a small dataset. Interesting, though, that the success rate we get, 42% is pretty bad, worse than ZeroR.
Actually, with any 2-class problem, you would expect to get a success rate of at least 50%. Tossing a coin would give you 50%. This OneR scheme is not performing very well on this trivial dataset. Notice that the rule it finally prints out since we're using 10-fold cross-validation, it does the whole thing 10 times and then on the 11th time calculates a rule from the entire dataset and that's what it prints out. That's where this rule comes from. OneR, one attribute does all the work.
This is a very simple method of machine learning described in 1993, 20 years ago in a paper called "Very Simple Classification Rules Perform Well on Most Commonly Used Datasets" by a guy called Rob Holte, who lives in Canada. He did an experimental evaluation of the OneR method on 16 commonly used datasets. He used cross-validation just like we've told you to evaluate these things, and he found that the simple rules from OneR often outperformed far more complex methods that had been proposed for these datasets. How can such a simple method work so well? Some datasets really are simple, and others are so small, noisy, or complex that you can't learn anything from them.
So, it's always worth trying the simplest things first.
<End Transcript>

<-- 3.3 Quiz -->
OneR vs. the baseline
Question 1
Perform 10-fold cross-validation using ZeroR and OneR. Which classifier achieves higher accuracy?
OneR
ZeroR
Both have the same accuracy
---
Correct answer(s):
OneR
---
Feedback correct:
A single rule outperforms ZeroR by a large margin on the iris dataset (92% vs 33%).

<-- 3.3 Quiz -->
OneR vs. the baseline
Question 2
Which attribute does OneR use for generating the rule in the previous experiment, when using the full dataset?
sepallength
sepalwidth
petallength
petalwidth
class
---
Correct answer(s):
petalwidth
---
Feedback correct:
The Classifier model output prints details of the model that is generated on the full training set. In case of OneR, it prints the rule.
---
Feedback incorrect:
“Class” is not really an attribute, it’s what’s being predicted.

<-- 3.3 Quiz -->
OneR vs. the baseline
Question 3
Could there be a dataset for which ZeroR outperforms OneR?
No. OneR always outperforms ZeroR.
Yes. ZeroR sometimes outperforms OneR.
---
Correct answer(s):
Yes. ZeroR sometimes outperforms OneR.
---
Feedback correct:
If the class distribution is skewed or limited data is available, predicting the majority class can yield better results than basing a rule on a single attribute. This happens with the nominal weather dataset, weather.nominal.arff.

<-- 3.3 Quiz -->
OneR vs. the baseline
Question 4
Could there be a dataset for which ZeroR outperforms OneR when evaluated on the training data?
No
Yes
---
Correct answer(s):
---
Feedback correct:
OneR always outperforms (or, at worst, equals) ZeroR when evaluated on the training data. (Of course, evaluating on the training data doesn’t reflect performance on independent test data.)

<-- 3.3 Quiz -->
OneR vs. the baseline
Question 4
Could there be a dataset for which ZeroR outperforms OneR when evaluated on the training data?
No
Yes
---
Correct answer(s):

<-- 3.4 Video -->
Overfitting
“Overfitting” is a problem that plagues all machine learning methods. It occurs when a classifier fits the training data too tightly and doesn’t generalize well to independent test data. It can be illustrated using OneR, which has a parameter that tends to make it overfit numeric attributes. For example, on the numeric version of the weather data, and on the diabetes dataset, we get good performance on the training data, but lousy performance on independent test sets – or with cross-validation. That’s overfitting.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! Before we go on to talk about some more simple classifier methods, we need to talk about overfitting. Any machine learning method may 'overfit' the training data, that's when it produces a classifier that fits the training data too tightly and doesn't generalize well to independent test data. Remember the user classifier that you built at the beginning of Week 2 when you built a classifier yourself? Imagine tediously putting a tiny circle around every single training data point. You could build a classifier very laboriously that would be 100% correct on the training data, but probably wouldn't generalize very well to independent test data. That's overfitting. It's a general problem. We're going to illustrate it with OneR.
We're going to look at the numeric version of the weather problem, where temperature and humidity are numbers and not nominal values. If you think about how OneR works, when it comes to make a rule on the attribute temperature, it's going to make complex rule that branches 14 different ways perhaps for the 14 different instances of the dataset. Each rule is going to have zero errors; it's going to get it exactly right. If we branch on temperature, we're going to get a perfect rule, with a total error count of zero. In fact, OneR has a parameter that limits the complexity of rules. I'm not going to talk about how it works.
It's pretty simple, but it's just a bit distracting and not very important. The point is that the parameter allows you to limit the complexity of the rules that are produced by OneR. Let's open the numeric weather data.
We can go to OneR, and choose it. There's OneR, and let's just create a rule. Here the rule is based on the outlook attribute. This is exactly what happened in the last lesson with the nominal version of the weather data. Let's just remove the outlook attribute, and try it again.
Now let's see what happens when we classify with OneR.
Now it branches on humidity. If humidity is less than 82.5%, it's a yes day; if it's greater than 82.5%, it's a no day and that gets 10 out of 14 instances correct. So far so good, that's using the default setting of OneR's parameter that controls the complexity of the rules it generates. We can go and look at OneR, and remember you can configure a classifier by clicking on it. We see that there's a parameter called minBucketSize, and it's set to 6 by default, which is a good compromise value. I'm going to change that value to 1, and then see what happens. Run OneR again, and now I get a different kind of rule.
It's branching many different ways on the temperature attribute. This rule is overfitted to the dataset.
It's a very accurate rule on the training data, but it won't generalize well to independent test data.
Now let's see what happens with a more realistic dataset. I'll open diabetes, which is a numeric dataset.
All the attributes are numeric, and the class is either tested_negative or tested_positive. Let's run ZeroR to get a baseline figure for this dataset. Here I get 65% for the baseline. We really ought to be able to do better than that. Let's run OneR. The default parameter settings that is a value of 6 for OneR's parameter that controls rule complexity. We get 71.5%. That's pretty good. We're evaluating using cross-validation. OneR outperforms the baseline accuracy by quite a bit -- 71% versus 65%. If we look at the rule, it branches on "plas". This is the plasma-glucose concentration. So, depending on which of these regions the plasma-glucose concentration falls into, then we're going to predict a negative or a positive outcome.
That seems like quite a sensible rule. Now, let's change OneR's parameter to make it overfit. We'll configure OneR, find the minBucketSize parameter, and change it to 1.
When we run OneR again, we get 57% accuracy, quite a bit lower than the ZeroR baseline of 65%. If you look at the rule. Here it is. It's testing a different attribute, pedi, which -- if you look at the comments of the ARFF file -- happens to be the diabetes pedigree function, whatever that is. You can see that this attribute has a lot of different values, and it looks like we're branching on pretty well every single one. That gives us lousy performance when evaluated by cross-validation, which is what we're doing now. If you were to evaluate it on the training set, you would expect to see very good performance.
Yes, here we get 87.5% accuracy on the training set, which is very good for this dataset. Of course, that figure is completely misleading; the rule is strongly overfitted to the training dataset, and doesn't generalize well to independent test sets. That's a good example of overfitting. Overfitting is a general phenomenon that plagues all machine learning methods. We've illustrated it by playing around with the parameter of the OneR method, but it happens with all machine learning methods. It's one reason why you should never evaluate on the training set. Overfitting can occur in more general contexts.
Let's suppose you've got a dataset and you choose a very large number of machine learning methods, say a million different machine learning methods and choose the best for your dataset using cross-validation. Well, because you've used so many machine learning methods, you can't expect to get the same performance on new test data. You've chosen so many, that the one that you've ended up with is going to be overfitted to the dataset you're using. It's not sufficient just to use cross-validation and believe the results. In this case, you might divide the data three ways, into a training set, a test set, and a validation set. Choose the method using the training and test set.
By all means, use your million machine learning methods and choose the best on the training and test set or the best using cross-validation on the training set. But then, leave aside this separate validation set for use at the end, once you've chosen your machine learning method, and evaluate it on that to get a much more realistic assessment of how it would perform on independent test data. Overfitting is a really big problem in machine learning.
<End Transcript>

<-- 3.5 Quiz -->
Overfitting
Question 1
Open the weather.numeric.arff dataset and inspect the data using the Edit button of Weka’s Preprocess panel. What is the maximum accuracy of rules based on temperature and humidity respectively, in terms of the number of training instances predicted correctly?
Select all the answers you think are correct.
temperature : 12 correct instances
temperature : 13 correct instances
temperature : 14 correct instances
humidity: 10 correct instances
humidity : 12 correct instances
humidity : 14 correct instances
---
Correct answer(s):
temperature : 13 correct instances
humidity : 12 correct instances
---
Feedback correct:
If the dataset contains instances with the same attribute value but different classes, a rule based on that attribute will get one of them incorrect. This happens when temperature = 72 and 75, and when humidity = 70 and 90. However, the value of play is the same (yes) when temperature = 75, so that will not cause an error. But in the other 3 cases the two classes are different.
---
Feedback incorrect:
If the dataset contains instances with the same attribute value but different classes, a rule based on that attribute will get one of them incorrect. This happens when temperature = 72 and 75, and when humidity = 70 and 90. However, the value of play is the same (yes) when temperature = 75, so that will not cause an error. But in the other 3 cases the two classes are different.

<-- 3.5 Quiz -->
Overfitting
Question 2
The following questions investigate the effect of OneR’s minBucketSize parameter on performance and rule complexity by drawing graphs where –B (minBucketSize) ranges from 1 to 10.
Open the glass.arff dataset, go to the Classify tab, and select OneR. Make a rough paper-and-pencil plot of accuracy on the training data (on the vertical axis) against minBucketSize (on the horizontal axis) and compare it with the graphs below.
Which of these shapes do you get?
(a)
(b)
(c)
---
Correct answer(s):
(c)
---
Feedback correct:
As minBucketSize increases, accuracy on the training set steadily decreases.

<-- 3.5 Quiz -->
Overfitting
Question 3
Make a rough pencil-and-paper plot of cross-validation accuracy against minBucketSize. Which of these do you get?
(a)
(b)
(c)
---
Correct answer(s):
(b)
---
Feedback correct:
For this dataset, cross-validation accuracy is relatively independent of minBucketSize.

<-- 3.5 Quiz -->
Overfitting
Question 4
Now consider the complexity of the rule that OneR generates, measured by its size – the number of tests it involves. Will the rule’s complexity depend on whether the training set or cross-validation is used for evaluation?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
The rule that Weka shows is generated from the training set, and is not affected by the evaluation method.

<-- 3.5 Quiz -->
Overfitting
Question 5
Using paper and pencil, plot the size of the rule generated against minBucketSize. Which of these plots do you get?
(a)
(b)
(c)
---
Correct answer(s):

<-- 3.5 Quiz -->
Overfitting
Question 5
Using paper and pencil, plot the size of the rule generated against minBucketSize. Which of these plots do you get?
(a)
(b)
(c)
---
Correct answer(s):
(a)
---
Feedback correct:
When minBucketSize = 1 the rule is largest, and decreases in size as minBucketSize increases. In fact, the pattern for training set accuracy closely follows the pattern for rule size: a large rule gives by far the best training set accuracy. However, this is not true for cross-validation accuracy.

<-- 3.6 Video -->
Using probabilities
OneR assumes that there is one attribute that does all the work. Another simple strategy is the opposite, all attributes contribute equally and independently to the decision. This is called “Naive Bayes,” and is based on a classical statistical result called Bayes Theorem. The method makes two assumptions that are generally violated in practice: the attributes are equally important; and statistically independent. Despite this naivety, the method often works surprisingly well.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! The one bit of Data Mining with Weka that we're going to see a little bit of mathematics, but don't worry, I'll take you through it gently. The OneR strategy that we've just been studying assumes that there is one of the attributes that does all the work, that takes the responsibility for the decision. That's a simple strategy. Another simple strategy is the opposite, to assume all of the attributes contribute equally and independently to the decision. This is called the "Naive Bayes" method -- I'll explain the name later on.
There are two assumptions that underline Naive Bayes: that the attributes are equally important; and that they are statistically independent, that is, knowing the value of one of the attributes doesn't tell you anything about the value of any of the other attributes. This independence assumption is never actually correct, but the method based on it often works well in practice. There's a theorem in probability called "Bayes Theorem" after this guy Thomas Bayes from the 18th century. It's about the probability of a hypothesis H given evidence E. In our case, the hypothesis is the class of an instance and the evidence is the attribute values of the instance.
The theorem is that Pr[H|E] -- the probability of the class given the instance, the hypothesis given the evidence -- is equal to Pr[E|H] times Pr[H] divided by Pr[E]. Pr[H] by itself is called the prior probability of the hypothesis H. That's the probability of the event before any evidence is seen. That's really the baseline probability of the event. For example, in the weather data, I think there are 9 yes's and 5 no's, so the baseline probability of the hypothesis "play equals yes" is 9/14 and "play equals no" is 5/14. What this equation says is how to update the probability Pr[H] when you see some evidence, to get what's call the "a posteriori" probability of H -- that means "after the evidence".
The evidence in our case is the attribute values of an unknown instance; that's E. That's Bayes Theorem. Now, what makes this method "naive"? The naive assumption is -- I've said it before -- that the evidence splits into parts that are statistically independent. The parts of the evidence in our case are the four different attribute values in the weather data. When you have independent events, the probabilities multiply, so Pr[H|E] according to the top equation is the product of Pr[E|H], times the prior probability Pr[H], divided by Pr[E].
Pr[E|H] splits up into these parts: Pr[E1|H], the first attribute value; Pr[E2|H], the second attribute value; and so on, for all of the attributes. That's maybe a bit abstract. Let's look at the actual weather data. On the right-hand side is the weather data. In the large table at the top, we've taken each of the attributes. Let's start with "outlook". Under the "yes" hypothesis and the "no" hypothesis, we've looked at how many times the outlook is "sunny". It's sunny twice under yes and 3 times under no. That comes straight from the data in the table. Overcast. When the outlook is overcast, it's always a "yes" instance, so there were 4 of those, and zero "no" instances.
Then, rainy is 3 "yes" instances and 2 "no" instances. Those numbers just come straight from the data table giving the instance values. Then we take those numbers and underneath we make them into probabilities. Let's say
we know the hypothesis: let's say we know it's a "yes". Then the probability of it being "sunny" is 2/9ths, "overcast" is 4/9ths, and "rainy" 3/9ths -- simply because when you add up 2 plus 4 plus 3 you get 9. Those are the probabilities. If we know that the outcome is "no", the probabilities are "sunny" 3/5ths, "overcast" 0/5ths, and "rainy" 2/5ths. That's for the "outlook" attribute. That's what we're looking for, you see, the probability of each of these attribute values given the hypothesis H. The next attribute is temperature, and we just do the same thing with that to get the probabilities of the 3 values -- hot, mild, and cool -- under the "yes" hypothesis or the "no" hypothesis.
The same with humidity and windy. Play, that's the prior probability -- Pr[H]. It's "yes" 9/14ths of the time, "no" 5/14ths of the time -- even if you don't know anything about the attribute values. The equation we're looking at is this one below, and we just need to work it out. Here's an example. Here's an unknown day, a new day. We don't know what the value of "play" is, but we know it's sunny, cool, high, and windy. We can just multiply up these probabilities. If we multiply for the "yes" hypothesis, we get 2/9th times 3/9ths times 3/9ths times 3/9ths -- those are just the numbers on the previous slide, Pr[E1|H], Pr[E2|H], Pr[E3|H], Pr[E4|H], finally Pr[H], that is, 9/14ths.
That gives us a likelihood of 0.0053 when you multiply them. Then, for the "no" class we do the same, to get a likelihood of 0.0206. These numbers are not probabilities. Probabilities have to add up to 1. They are likelihoods. But we can get the probabilities from them by using the straightforward technique of normalization. Take those likelihoods for "yes" and "no" and we normalize them as shown below to make them add up to 1. That's how we get the probability of "play" on a new day, with different attribute values. Just to go through that again. The evidence is "outlook" is "sunny", "temperature" is "cool", "humidity" is "high", "windy" is "true" -- and we don't know what "play" is.
The probability of a "yes" given the evidence is the product of those 4 probabilities -- one for outlook, temperature, humidity and windy -- times the prior probability, which is just the baseline probability of a "yes". That product of fractions is divided by Pr[E]. We don't know what Pr[E] is, but it doesn't matter, because we can do the same calculation for Pr[E] of "no", which gives us another equation just like this, and then we can calculate the actual probabilities by normalizing them so that the two probabilities add up to 1. Pr[E] for "yes" plus Pr[E] for "no" equals 1.
It's actually quite simple when you look at it in numbers, and it's simple when you look at it in Weka, as well. I'm going to go to Weka here, and I'm going to open the nominal weather data, which is here. We've seen that before, of course, many times. I'm going to go to Classify. I'm going to use the NaiveBayes method. It's under this bayes category here. There are a lot of implementations of different variants of Bayes; I'm just going to use the straightforward NaiveBayes method here. I'll just run it. This is what we get. The success probability calculated according to cross-validation. More interestingly, we get the model.
The model is just like the table I showed you before divided under the "yes" class and the "no" class. We've got the four attributes -- outlook, temperature, humidity, and windy -- and then, for each of the attribute values, we've got the number of times that attribute value appears. Now, there's one little and important difference between this table and the one I showed you before. Let me go back to my slide and look at these numbers. You can see that for outlook under "yes" on my slide I've got 2, 4, and 3, and Weka has got 3, 5, and 4. That's 1 more each time, for a total of 12 instead of a total of 9.
Weka adds 1 to all of the counts. The reason it does this is to get rid of the zeros. In the original table under outlook, under "no", the probability of overcast given "no" is zero, and we're going to be multiplying that into things. What that would mean in effect, if we took that zero at face value, is that the probability of the class being "no" given any day for which the outlook was overcast would be zero. Anything multiplied by zero is zero. These zeros in probability terms have sort of a veto over all of the other numbers, and we don't want that.
We don't want to categorically conclude that it must be a "no" day on a basis that it's overcast, and we've never seen an overcast outlook on a "no" day before. That's called the "zero-frequency problem," and Weka's solution -- the most common solution -- is
very simple: just add 1 to all the counts. That's why all those numbers in the Weka table are 1 bigger than the numbers in the table on the slide. Aside from that, it's all exactly the same. We're avoiding zero frequencies by effectively starting all counts at 1 instead of starting them at 0, so they can't end up at 0. That's the Naive Bayes method. The assumption is that all attributes contribute equally and independently to the outcome. That works surprisingly well, even in situations where the independence assumption is clearly violated. Why does it work so well when the assumption is wrong? That's a good question. Basically, classification doesn't need accurate probability estimates.
We're just going to choose as the class the outcome with the largest probability. As long as the greatest probability is assigned to the correct class, it doesn't matter if the probability estimates are all that accurate. This actually means that if you add redundant attributes you get problems with Naive Bayes. The extreme case of dependence is where two attributes have the same values, identical attributes. That will cause havoc with the Naive Bayes method. However, Weka contains methods for attribute selection to allow you to select a subset of fairly independent attributes, after which you can safely use Naive Bayes.
<End Transcript>

<-- 3.7 Quiz -->
Naive Bayes with dependent attributes
Question 1
What is NaiveBayes’s accuracy on this dataset?
79%
84%
90%
91%
---
Correct answer(s):
90%
---
Feedback correct:
(Weka gives the result as 90.1149%.)
---
Feedback incorrect:
Are you using the default options and 10-fold cross validation as the evaluation method?

<-- 3.7 Quiz -->
Naive Bayes with dependent attributes
Question 2
Return to the Preprocess tab and copy the 12th attribute, education-spending, ten times, using the Copy filter.
(You will run into a problem that is easily solved with a little ingenuity. And don’t forget that the word last can be used as an attribute index.)
What is NaiveBayes’s accuracy on this new dataset, again evaluated using 10-fold cross-validation?
86%
87%
90%
91%
---
Correct answer(s):
87%
---
Feedback correct:
(Weka gives the result as 86.6667%)
---
Feedback incorrect:
There should be 27 attributes in the dataset

<-- 3.7 Quiz -->
Naive Bayes with dependent attributes
Question 3
Return to the Preprocess tab and copy the same attribute a further ten times. What is the accuracy now?
86%
87%
90%
91%
---
Correct answer(s):
86%
---
Feedback correct:
(Weka gives the result as 86.4368%)
---
Feedback incorrect:
There should be 37 attributes in the dataset

<-- 3.7 Quiz -->
Naive Bayes with dependent attributes
Question 4
You’re probably thinking that if you were to continue to make copies of the education-spending attribute and evaluate by 10-fold cross-validation, the accuracy would gradually decrease until it finally leveled off. And you’d be right! What percentage accuracy does it level off at?
84%
85%
86%
87%
---
Correct answer(s):
84%
---
Feedback correct:
Do not make more copies of this attribute. Use what you know about Naive Bayes to figure out a better, quicker, and more reliable way.
(To be precise, 84.1379%.)
Continuing to make copies of an attribute gives it an increasingly stronger contribution to the decision, until – in the extreme – the other attributes have no influence at all. You can determine this value by removing all attributes except education-spending (but leave the class, of course).
---
Feedback incorrect:
Do not make more copies of this attribute. Use what you know about Naive Bayes to figure out a better, quicker, and more reliable way.

<-- 3.7 Quiz -->
Naive Bayes with dependent attributes
Question 5
If Naive Bayes’s accuracy continually deteriorates as you add copies of a particular attribute (as it does here for education-spending), do you think it would improve if that attribute was completely removed from the dataset?
Yes
No
Maybe
---
Correct answer(s):

<-- 3.7 Quiz -->
Naive Bayes with dependent attributes
Question 5
If Naive Bayes’s accuracy continually deteriorates as you add copies of a particular attribute (as it does here for education-spending), do you think it would improve if that attribute was completely removed from the dataset?
Yes
No
Maybe
---
Correct answer(s):
Maybe
---
Feedback correct:
It might. (In the present case, accuracy drops a little when education-spending is removed, so it seems that this attribute is slightly relevant to the class.)

<-- 3.8 Discussion -->
Problems with probabilities?
This material on probabilities is a bit more abstract than the rest of the course. We invite you to discuss it: the idea of a probability, a hypothesis based on evidence, the prior and posterior probability, and what the “naïve” assumption really means.

<-- 3.9 Video -->
Decision trees
Another simple method is to build a decision tree from the training data. Start at the top, with the whole training dataset. Select which attribute to split on first; then create a branch for each of its values. This splits the training data into subsets. Repeat the procedure for each branch, selecting an attribute at each node based on just the instances that reach it. This top-down, recursive, divide-and-conquer strategy is adopted by J48 (aka C4.5), which uses a measure called “information gain” to choose the attribute at each stage.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! We're continuing our exploration of simple classifiers by looking at classifiers that produce decision trees. We're going to look at J48. We've used this classifier quite a bit so far. Let's have a look at how it works inside. J48 is based on a top-down strategy, a recursive divide and conquer strategy. You select which attribute to split on at the root node, and then you create a branch for each possible attribute value, and that splits the instances into subsets, one for each branch that extends from the root node. Then you repeat the the procedure recursively for each branch, selecting an attribute at each node, and you use only instances that reach that branch to make the selection.
At the end you stop, perhaps you might continue until all instances have the same class. The trick is, the question is, how do you select a good attribute for the root node. This is the weather data, and as you can see, outlook has been selected for the root node.
Here are the four possibilities: outlook, windy, humidity, and temperature. These are the consequences of splitting on each of these attributes. What we're really looking for is a pure split, a split into pure nodes. We would be delighted if we found an attribute that split exactly into one node where they are all yeses, another node where they are all nos, and perhaps a third node where they are all yeses again. That would be the best thing. What we don't want is mixtures, because when we get mixtures of yeses and nos at a node, then we've got to split again. You can see that splitting on outlook looks pretty good.
We get one branch with two yeses and three nos, then we get a pure yes branch for overcast, and, when outlook is rainy, we get three yeses and two nos. How are we going to quantify this to decide which one of these attributes produces the purest nodes? We're on a quest here for purity. The aim is to get the smallest tree, and top-down tree induction methods use some kind of heuristic. The most popular heuristic to produce pure nodes is an information theory-based heuristic. I'm not going to explain information theory to you, that would be another MOOC of its own -- quite an interesting one, actually.
Information theory was founded by Claude Shannon, an American mathematician and scientist who died about 12 years ago. He was an amazing guy. He did some amazing things. One of the most amazing things, I think, is that he could ride a unicycle and juggle clubs at the same time when he was in his 80's. That's pretty impressive. He came up the whole idea of information theory and quantifying entropy, which measures information in bits.
This is the formula for entropy: the sum of p log p's for each of the possible outcomes. I'm not really going to explain it to you. All of those minus signs are there because logarithms are negative if numbers are less than 1 and probabilities always are less than 1. So, the entropy comes out to be a positive number. What we do is we look at the information gain. How much information in bits do you gain by knowing the value of an attribute? That is, the entropy of the distribution before the split minus the entropy of the distribution after the split. Here's how it works out for the weather data. These are the number of bits.
If you split on outlook, you gain 0.247 bits. I know you might be surprise to see fractional numbers of bits, normally we think of 1 bit or 8 bits or 32 bits, but information theory shows how you can regard bits as fractions. These produce fractional numbers of bits. I don't want to go into the details. You can see, knowing the value for windy gives you only 0.048 bits of information. Humidity is quite a bit better; temperature is way down there at 0.029 bits. We're going to choose the attribute that gains the most bits of information, and that, in this case, is outlook. At the top level of this tree, the root node, we're going to split on outlook.
Having decided to split on outlook, we need to look at each of 3 branches that emanate from outlook corresponding to the 3 possible values of outlook, and consider what to do at each of those branches. At the first branch, we might split on temperature, windy or humidity. We're not going to split on outlook again because we know that outlook is sunny. All instances that reach this place, the outlook is sunny. For the other 3 things, we do exactly the same thing. We evaluate the information gain for temperature at that point, for windy and humidity, and we choose the best. In this case, it's humidity with a gain of 0.971 bits.
You can see that, if we branch on humidity, then we get pure nodes: 3 nos in one and 2 yeses in the other. When we get that, we don't need to split anymore. We're on a quest for purity. That's how it works. It just carries on until it reaches the end, until it has pure nodes. Let's open up Weka, and just do this with the nominal weather data. Of course, we've done this before, but I'll just do it again. It won't take long. J48 is the workhorse data mining algorithm. There's the data. We're going to choose J48. It's a tree classifier.
We're going to run this, and we get a tree -- the very tree I showed you before -- split
first on outlook: sunny, overcast and rainy. Then, if it's sunny, split on humidity, 3 instances reach that node. Then split on normal, 3 yes instances reach that node, and so on. We can look at the tree using Visualize the tree in the right-click menu. Here it is.
These are the number of yes instances that reach this node and the number of no instances. In the case of this particular tree, of course we're using cross validation here. It's done an 11th run on the whole dataset. It's given us these numbers by looking at the training set. In fact, this becomes a pure node here. Sometimes you get 2 numbers here -- 3/2 or 3/1. The first number indicates the number of correct things that reach that node, so in this case the number of nos. If there was another number following the 3, that would indicate the number of yeses, that is, incorrect things that reach that node. But that doesn't occur in this very simple situation.
There you have it, J48: top-down induction of decision trees. It's soundly based in information theory. It's a pretty good data mining algorithm. 10 years ago I might have said it's the best data mining algorithm, but some even better ones, I think, have been produced since then. However, the real advantage of J48 is that it's reliable and robust, and, most importantly, it produces a tree that people can understand. It's very easy to understand the output of J48. That's really important when you're applying data mining. There are a lot of different criteria you could use for attribute selection. Here we're using information gain. Actually, in practice, these don't normally make a huge difference.
There are some important modifications that need to be done to this algorithm to be useful in practice. I've only really explained the basic principles. The actual J48 incorporates some more complex stuff to make it work under different circumstances in practice. We'll talk about those in the next lesson.
<End Transcript>

<-- 3.10 Quiz -->
Weka's output 
Question 1
The More options menu on the Classify panel can be used to customize the output. Depending on the setup, Weka will generate one or more of the following sections:
a. === Run information === 
b. === Stratified cross-validation ===
c. === Evaluation on test set ===
d. === Evaluation on training set ===
e. === Summary ===
f. === Classifier model (full training set) ===
g. === Predictions on test data ===
h. === Detailed accuracy by class ===
i. === Confusion matrix ===
j. === Source code ===
Which of these sections is always present?
a. === Run information ===
b. === Stratified cross-validation ===
i. === Confusion matrix ===
---
Correct answer(s):
a. === Run information ===
---
Feedback correct:
Run information is always produced, to record what algorithm was evaluated, in what fashion, and on which dataset.

<-- 3.10 Quiz -->
Weka's output 
Question 2
Which of the sections is present when a separate test set is used?
a. === Run information === 
b. === Stratified cross-validation ===
c. === Evaluation on test set ===
d. === Evaluation on training set ===
e. === Summary ===
f. === Classifier model (full training set) ===
g. === Predictions on test data ===
h. === Detailed accuracy by class ===
i. === Confusion matrix ===
j. === Source code ===
b. === Stratified cross-validation ===
c. === Evaluation on test set ===
d. === Evaluation on training set ===
j. === Source code ===
---
Correct answer(s):
c. === Evaluation on test set ===
---
Feedback correct:
The “test set” in the section title gives it away.

<-- 3.10 Quiz -->
Weka's output 
Question 3
Which section involves using the Folds parameter?
a. === Run information === 
b. === Stratified cross-validation ===
c. === Evaluation on test set ===
d. === Evaluation on training set ===
e. === Summary ===
f. === Classifier model (full training set) ===
g. === Predictions on test data ===
h. === Detailed accuracy by class ===
i. === Confusion matrix ===
j. === Source code ===
b. === Stratified cross-validation ===
c. === Evaluation on test set ===
d. === Evaluation on training set ===
j. === Source code ===
---
Correct answer(s):
b. === Stratified cross-validation ===
---
Feedback correct:
The Folds parameter is used for stratified cross-validation.

<-- 3.10 Quiz -->
Weka's output 
Question 4
Now let’s take a closer look at the options available in the More options dialog. These are:
When using cross-validation, Weka prints a model built on the full dataset. The statistics, however, are calculated from the various train/test splits. This can be confusing, because the model stays the same regardless of the number of folds or the value of the random seed. Which of the options allows you to suppress the classifier model (section (f) in the list in Q.1, 2, 3)?
Output model
Error plot point size proportional to margin
Output source code
---
Correct answer(s):
Output model
---
Feedback correct:
The Output model option toggles whether or not the model built on the full dataset is printed.

<-- 3.10 Quiz -->
Weka's output 
Question 5
Which option generates Java code that represents the model produced by the classifier (if the classifier offers this capability)?
Output model
Error plot point size proportional to margin
Output source code
---
Correct answer(s):
Output source code
---
Feedback correct:
This option prints Java code for the model built on the full training set (section (j) in the list in Q.1, 2, 3). This code can be embedded in other Java applications.

<-- 3.10 Quiz -->
Weka's output 
Question 6
Which option should be configured in order to see how the learned classifier deals with each instance in a supplied test set (section (g) in the list in Q.1, 2, 3)?
Output confusion matrix
Store predictions for visualization
Output predictions
---
Correct answer(s):
Output predictions
---
Feedback correct:
There are various ways in which the output can be configured, e.g., as plain text.

<-- 3.10 Quiz -->
Weka's output 
Question 7
If you plan to visualize the predictions made by the classifier, which option do you need to set?
Output model
Store predictions for visualization
Error plot point size proportional to margin
Output source code
---
Correct answer(s):
Store predictions for visualization
---
Feedback correct:
This option makes Weka save the classifier’s predictions on the test data. With a very large test set, you might want to turn this off to reduce storage requirements.

<-- 3.10 Quiz -->
Weka's output 
Question 8
Which option should you configure if you want the classifier’s predictions (section (g) in the list in Q.1, 2, 3) to show values of attributes other than the class?
Output per-class stats
Output confusion matrix
Output predictions
---
Correct answer(s):

<-- 3.10 Quiz -->
Weka's output 
Question 8
Which option should you configure if you want the classifier’s predictions (section (g) in the list in Q.1, 2, 3) to show values of attributes other than the class?
Output per-class stats
Output confusion matrix
Output predictions
---
Correct answer(s):
Output predictions
---
Feedback correct:
For instance, you could choose PlainText output of predictions. Also, you can specify a comma-separated range of attribute indices whose values will be included along with the actual and predicted class values. For example, the specification “first–3,10,12–14” would include attributes 1, 2, 3, 10, 12, 13, 14.

<-- 3.11 Video -->
Pruning decision trees
Decision trees run the risk of overfitting the training data. One simple counter-measure is to stop splitting when the nodes get small. Another is to construct a tree and then prune it back, starting at the leaves. For this, J48 uses a statistical test which is rather unprincipled but works well. For example, on the breast-cancer dataset it generates a tree with 4 leaves (6 nodes in total) that gets an accuracy of 75.5%. With pruning switched off, the tree has 152 leaves (179 nodes) whose accuracy is only 69.6%.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! In the last class, we looked at a bare-bones algorithm for constructing decision trees. To get an industrial strength decision tree induction algorithm, we need to add some more complicated stuff, notably pruning. We're going to talk in this [class] about pruning decision trees. Here's a guy pruning a tree, and that's a good image to have in your mind when we're talking about decision trees. We're looking at those little twigs and little branches around the edge of the tree, seeing if their worthwhile, and snipping them off if they're not contributing. That way, we'll get a decision tree that might perform worse on the training data, but perhaps generalizes better to independent test data. That's what we want.
Here's the weather data again. I'm sorry to keep harking back to the weather data, but it's just a nice simple example that we all know now. I've added here a new attribute. I call it an ID code attribute, which is different for each instance.
I've just given them an identification code: a, b, c, and so on. Let's just think from the last lesson, what's going to happen when we consider which is the best attribute to split on at the root, the first decision. We're going to be looking for the information gain from each of our attributes separately. We're going to gain a lot of information by choosing the ID code. Actually, if you split on the ID code, that tells you everything about the instance we're looking at. That's going to be a maximal amount of information gain, and clearly we're going to split on that attribute at the root node of the decision tree.
But that's not going to generalize at all to new weather instances. To get around this problem, having constructed a decision tree, decision tree algorithms then automatically prune it back. You don't see any of this, it just happens when you start the algorithm in Weka. How do we prune? There are some simple techniques for pruning, and some more complicated techniques for pruning. A very simple technique is to not continue splitting if the nodes get very small. I said in the last lesson that we're going to keep splitting until each node has just one class associated with it. Perhaps that's not such a good idea.
If we have a very small node with a couple instances, it's probably not worth splitting that node. That's actually a parameter in J48. I've got Weka going here. I'm going to choose J48 and look at the parameters.
There's a parameter called minNumObj. If I mouse over that parameter, it says "The minimum number of instances per leaf". The default value for that is 2. The second thing we do is to build a full tree and then work back from the leaves. It turns out to be better to build a full tree and prune back rather than trying to do forward pruning as you're building the tree. We apply a statistical test at each stage. That's the confidenceFactor parameter. It's here. The default value is 0.25. "The confidence factor used for pruning [smaller values incur more pruning]." Then, sometimes it's good to prune an interior node, and to raise the subtree beneath that interior node up one level. That's called subtreeRaising.
That's this parameter here. We can switch it on or switch it off. "Whether to consider the subtree raising operation during pruning." Subtree raising actually increases the complexity of the algorithm, so it would work faster if you turned off subtree raising on a large problem. I'm not going to talk about the details of these methods. Pruning is a messy and complicated subject, and it's not particularly illuminating. Actually, I don't really recommend playing around with these parameters here. The default values on J48 tend to do a pretty good job. Of course, it's become apparent to you now that the need to prune is really a result of the original unpruned tree overfitting the training dataset. This is another instance of overfitting.
Sometimes simplifying a decision tree gives better results, not just a smaller, more manageable tree, but actually better results. I'm going to open the diabetes data. I'm going to choose J48, and I'm just going to run it with the default parameters. I get an accuracy of 73.8%, evaluated using cross-validation. The size of the tree is 20 leaves, and a total of 39 nodes. That's 19 interior nodes and 20 leaf nodes. Let's switch off pruning. J48 prunes by default. We're going to switch off pruning. We've got an unpruned option here, which is false, which means it's pruning. I'm going to change that to true -- which means it's not pruning any more -- and run it again.
Now we get a slightly worse result, 72.7%, probably not significantly worse. We get a slightly larger tree -- 22 leaves and 43 nodes. That's a double whammy, really. We've got a bigger tree, which is harder to understand, and we've got a slightly worse prediction result. We would prefer the pruned tree in this example on this dataset. I'm going to show you a more extreme example with the breast cancer data. I don't think we've looked at the breast cancer data before. The class is no-recurrence-events versus recurrence-events, and there are attributes like age, menopause, tumor size, and so on. I'm going to go classify this with J48 in the default configuration.
I need to switch on pruning -- that is, make unpruned false -- and then run it. I get an accuracy of 75.5%, and I get a fairly small tree with 4 leaves and 2 internal nodes. I can look at that tree here, or I can visualize the tree.
We get this nice, simple little decision structure here, which is quite comprehensible and performs pretty well, 75% accuracy. I'm going to switch off pruning. Make unpruned true, and run it again. First of all, I get a much worse result, 69.6% -- probably signficantly worse than the 75.5% I had before. More importantly, I get a huge tree, with 152 leaves and 179 total nodes. It's massive. If I try to visualize that, I probably won't be able to see very much. I can try to fit that to my screen, and it's still impossible to see what's going on here. In fact, if I look at the textual description of the tree, it's just extremely complicated. That's a bad thing.
Here, an unpruned tree is a very bad idea. We get a huge tree which does quite a bit worse than a much simpler decision structure. J48 does pruning by default and, in general, you should let it do pruning according to the default parameters. That would be my recommendation. We've talked about J48, or, in other words, C4.5. We talked about the progression from C4.5 by Ross Quinlan. Here is a picture of Ross Quinlan, an Australian computer scientist, at the bottom of the screen. The progression from C4.5 from Ross to J48, which is the Java implementation essentially equivalent to C4.5. It's a very popular method. It's a simple method and easy to use.
Decision trees are very attractive because you can look at them and see what the structure of the decision is, see what's important about your data. There are many different pruning methods, and their main effect is to change the size of the tree. They have a small effect on the accuracy, and it often makes the accuracy worse. They often have a huge effect on the size of the tree, as we just saw with the breast cancer data. Pruning is actually a general technique to guard against overfitting, and it can be applied to structures other than trees, like decision rules. There's a lot more we could say about decision trees.
For example, we've been talking about univariate decision trees -- that is, ones that have a single test at each node. You can imagine a multivariate tree, where there is a compound test. The test of the node might be 'if this attribute is that AND that attribute is something else'. You can imagine more complex decision trees produced by more complex decision tree algorithms. In general, C4.5/J48 is a popular and useful workhorse algorithm for data mining.
<End Transcript>

<-- 3.12 Quiz -->
The effect of pruning
Question 1
The dataset was created by the Oncology Institute in Ljubljana. For which other domain did they contribute?
computer tomography
lymphography
steganography
---
Correct answer(s):
lymphography
---
Feedback correct:
You will find this in Section 4 of the ARFF file, entitled Relevant information.

<-- 3.12 Quiz -->
The effect of pruning
Question 2
By looking at the comments in the ARFF file, determine how many possible values there are for the age attribute, and how many of these values are used in the dataset.
3 values, all of which are used
8 values, 4 of which are used
9 values, 6 of which are used
9 values, all of which are used
100 values, 90 of which are used
100 values, all of which are used
---
Correct answer(s):
9 values, 6 of which are used
---
Feedback correct:
In Secton 7 of the comments you can see that age is divided into 9 different ranges, and Section 9 says that 6 of these are distinct.

<-- 3.12 Quiz -->
The effect of pruning
Question 3
Now open the breast-cancer.arff dataset in the Explorer, go to the Classify tab, and select J48.
One simple way of pruning a decision tree is to impose a minimum on the number of training examples that reach a leaf. This is done by J48’s minNumObj parameter (default value 2) with the unpruned switch set to True. (The terminology is a little confusing: we are talking about pruning even though unpruned is selected. If we were to deselect unpruned, J48’s other pruning mechanisms would come into play.)
Experiment with the following values of minNumObj, recording the number of leaves and size of the tree in each case:
1,  2,  3,  5,  10,  20,  50,  100
Make a rough pencil-and-paper plot of the number of leaves in the tree (on the vertical axis) against minNumObj (on the horizontal axis).
Which of these shapes do you get?
(a)
(b)
(c)
---
Correct answer(s):
(a)
---
Feedback correct:
The number of leaves in the tree decreases very rapidly as the size of each leaf is allowed to grow.

<-- 3.12 Quiz -->
The effect of pruning
Question 4
Which of these shapes do you get when you plot the total size of the tree (in nodes) against minNumObj?
(a)
(b)
(c)
---
Correct answer(s):
(b)
---
Feedback correct:
The tree size follows the same trajectory as the number of leaves: it decreases very rapidly as the leaf size grows.

<-- 3.12 Quiz -->
The effect of pruning
Question 5
One of the following values for minNumObj produces the same tree as the version of J48 with default parameters (i.e., unpruned = false, minNumObj = 2). Which is it?
minNumObj = 1
minNumObj = 2
minNumObj = 5
minNumObj = 10
minNumObj = 20
minNumObj = 50
minNumObj = 100
---
Correct answer(s):
minNumObj = 20
---
Feedback correct:
Interestingly, although the same tree is generated for the full dataset, the cross-validation accuracies differ: 76% (default parameters) vs. 74%. How can this be?

<-- 3.12 Quiz -->
The effect of pruning
Question 6
In general, J48’s confidenceFactor parameter is best left alone (unless it is tuned automatically using a parameter tuning method). But it is interesting to see its effect. With default values for the other parameters, experiment with the following values of confidenceFactor, recording the performance in each case (evaluated using 10-fold cross-validation):
0.005,  0.05,  0.1,  0.25,  0.5
Which value or values produce the greatest accuracy?
0.005
0.05
0.1
0.05, 0.1. and 0.25 (a tie)
0.1 and 0.25 (a tie)
0.25
0.5
---
Correct answer(s):

<-- 3.12 Quiz -->
The effect of pruning
Question 6
In general, J48’s confidenceFactor parameter is best left alone (unless it is tuned automatically using a parameter tuning method). But it is interesting to see its effect. With default values for the other parameters, experiment with the following values of confidenceFactor, recording the performance in each case (evaluated using 10-fold cross-validation):
0.005,  0.05,  0.1,  0.25,  0.5
Which value or values produce the greatest accuracy?
0.005
0.05
0.1
0.05, 0.1. and 0.25 (a tie)
0.1 and 0.25 (a tie)
0.25
0.5
---
Correct answer(s):
0.05, 0.1. and 0.25 (a tie)
---
Feedback correct:
These all produce a percentage accuracy of 75.5%.

<-- 3.13 Video -->
Nearest neighbor
“Nearest neighbor” (equivalently, “instance based”) is a classic method that often yields good results. Just stash away the training instances. Be lazy! – do  nothing until you have to make a prediction. Then, to classify a test instance, search the training set for the closest training instance and use its class. A similarity function is needed to measure “closeness”; you might want to normalize the attributes first; and you might want to use several neighbors and let them vote on the result.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! I'm sitting here in New Zealand. It's on the globe behind me. That's New Zealand, at the top of the world, surrounded by water. But that's not where I'm from originally. I moved here about 20 years ago. Here on this map, of course, this is New Zealand -- Google puts things with the north at the top, which is probably what you're used to. I came here from the University of Calgary in Canada, where I was for many years. I used to be head of computer science for the University of Calgary. But, originally, I'm from Belfast, Northern Ireland, which is here in the United Kingdom. So, my accent actually is Northern Irish, not New Zealand.
This is not a New Zealand accent. We're going to talk about another machine learning method called the nearest neighbor, or instance-based, machine learning method. When people talk about rote learning, they just talk about remember stuff without really thinking about it. It's the simplest kind of learning. Nearest neighbor implements rote learning. It just remembers the training instances, and then, to classify a new instance, it searches the training set for one that is most like the new instance. The representation of the knowledge here is just the set of instances. It's a kind of lazy learning. The learner does nothing until it has to do some predictions. Confusingly, it's also called instance-based learning. Nearest neighbor learning and instance-based learning are the same thing.
Here is just a little picture of 2-dimensional instance space. The blue points and the white points are two different classes -- yes and no, for example. Then we've got an unknown instance, the red one. We want to know which class it's in. So, we simply find the closest instance in each of the classes and see which is closest. In this case, it's the blue class. So, we would classify that red point as though it belonged to the blue class. If you think about this, that's implicitly drawing a line between the two clouds of points. It's a straight line here, the perpendicular bisector of the line that joins the two closest points. The nearest neighbor method produces a linear decision boundary.
Actually, it's a little bit more complicated than that. It produces a piece-wise linear decision boundary with sometimes a bunch of little linear pieces of the decision boundary. Of course, the trick is what do we mean by "most like". We need a similarity function, and conventionally, people use the regular distance function, the Euclidean distance, which is the sum of the squares of the differences between the attributes. It's the square root of the sum of the squares, but since we're just comparing two instances, we don't need to take the square root. Or, you might use the Manhattan or city block distance, which is the sum of the absolute differences between the attribute values. Of course, I've been talking about numeric attributes here.
If attributes are nominal, we need the difference between different attribute values. Conventionally, people just say the distance is 1 if the attribute values are different and 0 if they are the same. It might be a good idea with nearest neighbor learning to normalize the attributes so that they all lie between 0 and 1, so the distance isn't skewed by some attribute that happens to be on some gigantic scale. What about noisy instances. If we have a noisy dataset, then by accident we might find an incorrectly classified training instance as the nearest one to our test instance. You can guard against that by using the k-nearest-neighbors.
k might be 3 or 5, and you look for the 3 or the 5 nearest neighbors and choose the majority class amongst those when classifying an unknown point. That's the k-nearest-neighbor method. In Weka, it's called IBk (instance-based learning with parameter k), and it's in the lazy class. Let's open the glass dataset. Go to Classify and choose the lazy classifier IBk.
Let's just run it. We get an accuracy of 70.6%. The model is not really printed here, because there is no model. It's just the set of training instances. We're using 10-fold cross-validation, of course. Let's change the value of k, this kNN is the k value. It's set by default to 1. (The number of neighbors to use.) We'll change that to, say, 5 and run that. In this case, we get a slightly worse result, 67.8% with k as 5. This is not such a noisy dataset, I guess. If we change it to 20 and run it again. We get 65% accuracy, slightly worse again.
If we had a noisy dataset, we might find that the accuracy figures improved as k got little bit larger. Then, it would always start to decrease again. If we set k to be an extreme value, close to the size of the whole dataset, then we're taking the distance of the test instance to all of the points in the dataset and averaging those, which will probably give us something close to the baseline accuracy. Here, if I set k to be a ridiculous value like 100. I'm going to take the 100 nearest instances and average their classes. We get an accuracy of 35%, which, I think is pretty close to the baseline accuracy for this dataset.
Let me just find that out with ZeroR, the baseline accuracy is indeed 35%. Nearest neighbor is a really good method. It's often very accurate. It can be slow. A simple implementation would involve scanning the entire training dataset to make each prediction, because we've got to calculate the distance of the unknown test instance from all of the training instances to see which is closest. There are more sophisticated data structures that can make this faster, so you don't need to scan the whole dataset every time. It assumes all attributes are equally important. If that wasn't the case, you might want to look at schemes for selecting or weighting attributes depending on their importance.
If we've got noisy instances, than we can use a majority vote over the k nearest neighbors, or we might weight instances according to their prediction accuracy. Or, we might try to identify reliable prototypes, one for each of the classes. This is a very old method. Statisticians have used k-nearest-neighbor since the 1950's. There's an interesting theoretical result. If the number (n) of training instances approaches infinity, and k also gets larger in such a way that k/n approaches 0, but k also approaches infinity, the error of the k-nearest-neighbor method approaches the theoretical minimum error for that dataset. There is a theoretical guarantee that with a huge dataset and large values of k, you're going to get good results from nearest neighbor learning.
<End Transcript>

<-- 3.14 Quiz -->
Instance-based learning
Question 1
Select the IBk classifier. What is its accuracy, evaluated using 10-fold cross-validation?
71%
72%
74%
97%
---
Correct answer(s):
72%
---
Feedback correct:
On the breast-cancer dataset, IBk gives an accuracy of 72% (not rounded: 72.3776%) for 10-fold cross-validation.
---
Feedback incorrect:
Make sure you are using the default values.
---
Feedback incorrect:
Are you using cross-validation?

<-- 3.14 Quiz -->
Instance-based learning
Question 2
IBk’s KNN parameter specifies the number of nearest neighbors to use when classifying a test instance, and the outcome is determined by majority vote. The default value is 1. Evaluate KNN’s performance with 2, 3, and 5 nearest neighbors. What accuracies do you obtain?
72%, 74%, 73%
72%, 73%, 74%
73%, 74%, 73%
73%, 74%, 74%
---
Correct answer(s):
73%, 74%, 73%
---
Feedback correct:
For 2 neighbors you get 73%, for 3 neighbors 74%, and for 5 neighbors 73% accuracy when cross-validating IBk using 10 folds.

<-- 3.14 Quiz -->
Instance-based learning
Question 3
Do you think these differences are significant? Support your answer by running IBk with the default value of 1 for KNN using the following random number seeds, recording the accuracies:
1, 2, 3, 4, 5
Are the differences likely to be significant?
yes
no
---
Correct answer(s):
no
---
Feedback correct:
These seed values give accuracies of 72%, 74%, 71%, 74%, 73%, which are sufficiently close together that the differences are probably not significant.

<-- 3.14 Quiz -->
Instance-based learning
Question 4
An obvious issue with IBk is how to choose a suitable value for the number of nearest neighbors used. If it’s too small, the method is susceptible to noise in the data. If it’s too large, the decision is smeared out, covering too great an area of instance space. Weka’s IBk implementation has an option that can help by choosing the best value automatically. Check the More button information to find out what it is.
KNN
crossValidate
debug
distanceWeighting
meanSquared
nearestNeighbourSearchAlgorithm
windowSize
---
Correct answer(s):
crossValidate
---
Feedback correct:
If this is set, IBk uses cross-validation to select the best value for KNN (which is the same as k).
---
Feedback incorrect:
You should already know that the KNN parameter specifies the number of nearest neighbors to use when classifying a test instance.

<-- 3.14 Quiz -->
Instance-based learning
Question 5
Let’s artificially add noise to a dataset, determine the best value for KNN using the option you have just discovered, and watch it change with the level of noise.
Open the glass.arff dataset. Select the unsupervised attribute filter addNoise. Observe from its configuration panel that by default it adds 10% noise to the last attribute (the class). Change this to 30% and Apply the filter.
In the Classify panel, select IBk and configure it to determine the best number of neighbors automatically. On the face of it, the KNN parameter is now redundant, but in fact it’s not. Figure out what it does by experimenting with values 1, 10, 20 and checking how many neighbors are used. When you run IBk this information appears in the Classifier model section of the output.
What is the best number of neighbors (as determined by Weka) when the amount of noise added is 0%, 10%, 20%, and 30%? (20 is a safe value to use for KNN in these experiments.) Don’t forget to Undo the effect of the addNoise filter (or reload the dataset) after each experiment!
1, 1, 1, 1
1, 2, 3, 4
1, 2, 3, 12
1, 4, 4, 12
1, 2, 4, 20
1, 4, 12, 20
---
Correct answer(s):
1, 4, 4, 12
---
Feedback correct:
Using the crossValidate mode that you discovered in the previous question, the KNN parameter sets the maximum number of neighbors that will be investigated. Leave this at anything greater than 12 to get these results.

<-- 3.14 Quiz -->
Instance-based learning
Question 5
Let’s artificially add noise to a dataset, determine the best value for KNN using the option you have just discovered, and watch it change with the level of noise.
Open the glass.arff dataset. Select the unsupervised attribute filter addNoise. Observe from its configuration panel that by default it adds 10% noise to the last attribute (the class). Change this to 30% and Apply the filter.
In the Classify panel, select IBk and configure it to determine the best number of neighbors automatically. On the face of it, the KNN parameter is now redundant, but in fact it’s not. Figure out what it does by experimenting with values 1, 10, 20 and checking how many neighbors are used. When you run IBk this information appears in the Classifier model section of the output.
What is the best number of neighbors (as determined by Weka) when the amount of noise added is 0%, 10%, 20%, and 30%? (20 is a safe value to use for KNN in these experiments.) Don’t forget to Undo the effect of the addNoise filter (or reload the dataset) after each experiment!
1, 1, 1, 1
1, 2, 3, 4
1, 2, 3, 12
1, 4, 4, 12
1, 2, 4, 20
1, 4, 12, 20
---
Correct answer(s):

<-- 3.15 Discussion -->
Reflect on this week's Big Question
The Big Question of the week is, “How do simple classification methods work?”
We promised that by the end of the week you would be able to choose learning methods based on several different “flavors” of simplicity. You would be able to explain how they work, apply them to a dataset of your choice, and interpret the output they produce.
As you know, simplicity comes in many different flavors. You could choose a single attribute and base the decision on that, ignoring all others (OneR). Or you could assume that each attribute contributes equally and independently to the final decision (NaiveBayes). Or you could build a tree that tests a few attributes sequentially (J48). Or you could store the training data and give new instances the same classification as their nearest neighbor – or take into account several neighbors (IBk*).
So … how do simple classification methods work?
I bet you’d have no trouble explaining (to your partner, siblings, parents or kids) how these methods work. Try it! Tell your fellow learners how you get on.
And by the way, “there is no better way to learn than to teach”.
(Benjamin Whichcote, Moral and Religious Aphorisms)

<-- 3.16 Article -->
Index
(A full index to the course appears at the end of Week 1.)
      Topic
      Step
      Datasets
      Breast-cancer
      3.11, 3.12, 3.14
      Diabetes
      3.4
      Glass
      3.5, 3.13, 3.14
      Iris
      3.3
      Weather
      3.2, 3.4, 3.5, 3.6, 3.9, 3.11
      Classifiers
      IBk
      3.13, 3.14
      J48
      3.9, 3.10, 3.11, 3.12
      NaiveBayes
      3.6, 3.7
      OneR
      3.2, 3.3, 3.4, 3.5
      Filters
      AddNoise
      3.14
      Copy
      3.7
      Plus …
      Baseline accuracy
      3.3
      Bayes theorem
      3.6
      Classify panel “More options”
      3.10
      Decision trees
      3.9, 3.10, 3.11
      Information gain
      3.9
      Output model
      3.10
      Output predictions
      3.10
      Overfitting
      3.4, 3.5
      Visualize predictions
      3.10

<-- 4.0 Todo -->
More classifiers
What about real-life classification methods?
This week's Big Question!
4.1
What about real-life classification methods?
article
Classification boundaries 
Different classifiers are biased towards different kinds of decision, which you can explore by visualizing the classification boundaries. We look at classification boundaries for OneR, IBk, NaiveBayes, and J48.
4.2
Classification boundaries
video (10:04)
4.3
Exploring classification boundaries
quiz
Linear regression
"Regression" problems are where the class is numeric, and "linear regression" is a standard mathematical technique for predicting numeric classes. In addition, there are non-linear methods that build trees of linear models.
4.4
Linear regression
video (07:56)
4.5
Regression with nominal attributes
quiz
Classification by regression
Linear regression can be used for classification as well. For two-valued nominal classes, just convert them to 0 and 1. For more class labels, either "multi-response linear regression" or "pairwise linear regression" can be used.
4.6
Classification by regression
video (09:59)
4.7
Multi-response linear regression
quiz
Logistic regression
Sometimes it’s best to predict class probabilities instead of predicting the classes themselves. Linear regression can be made to work with probabilities, resulting in logistic regression, a popular classification technique.
4.8
Logistic regression
video (08:59)
4.9
Try out some classifiers
quiz
4.10
Yikes! The math! It’s too much!
discussion
Support vector machines
Support vector machines separate the classes using the "maximum margin hyperplane." This is defined by a few instances, called "support vectors," from each class. The boundary depends on a few points, which reduces overfitting.
4.11
Support vector machines
video (07:20)
4.12
Decision boundaries and overfitting
quiz
Ensemble learning
Many of us dislike committees, but nevertheless they often make good, unbiased, decisions. Several machine learning methods use "committees" of different classifier algorithms: Bagging, Random forests, Boosting, and Stacking.
4.13
Ensemble learning
video (08:31)
4.14
Boosting
quiz
4.15
Reflect on this week's Big Question
discussion
4.16
Index
article

<-- 4.1 Article -->
What about real-life classification methods?
This week we encounter some of the most important classification methods.
Although the principles are not difficult, actual, working, industrial-strength implementations often involve many nit-picky little details, and you’ll probably never completely comprehend the full complexity. What this course aims to convey is the gist of modern machine learning methods, not the gory details. What’s important is that you can use them and understand the principles behind how they work – just as you can use a car, or a computer, without being an automotive engineer or hardware/software specialist.
It’s never a good idea to blindly use methods without any understanding of what’s behind them – and this is particularly true in data mining, where it’s embarrassingly easy to fool yourself about how well your system is working.
We’ll study some pretty cool machine learning methods. First up is a venerable, tried-and-true statistical technique called “linear regression. Then we learn how this can be used in non-standard ways: classification by regression, and logistic regression. We end with the relatively recent techniques of support vector machines and ensemble learning. These are contemporary, state-of-the-art machine learning methods.
By the week’s end you will be able to use modern machine learning methods in Weka, describe – at a high level – how they work, apply them to a dataset of your choice, and interpret the output that they produce.

<-- 4.2 Video -->
Classification boundaries
Different classifiers are biased towards different kinds of decision. You can see this by examining classification boundaries for various machine learning methods trained on a 2D dataset with numeric attributes. Here we use Weka’s Boundary Visualizer to plot boundaries for some example classifiers: OneR, IBk, Naive Bayes, and J48. Characteristic patterns appear.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
In this lesson, we're going to start by looking at classification boundaries for different machine learning methods. We're going to use Weka's Boundary Visualizer, which is another Weka tool that we haven't encountered yet. I'm going to use a 2-dimensional dataset. I've prepared iris.2d.arff.
It's a 2-dimensional version of the iris dataset. I took the regular iris dataset and deleted a couple of attributes -- sepallength and sepalwidth -- leaving me with this 2D dataset, and the class. We're going to look at that using the Boundary Visualizer. You get that from this Visualization menu on the Weka Chooser. There are a lot of tools in Weka, and we're just going to look at this one here, the Boundary Visualizer. I'm going to open the same file in the Boundary Visualizer, the 2-dimensional iris dataset. Here we've got a plot of the data. You can see that we're plotting petalwidth on the y-axis against petallength on the x-axis.
This is a picture of the dataset with the 3 classes setosa in red, versicolor in green, and virginica in blue. I'm going to choose a classifier. Let's begin with the OneR classifier, which is in rules.
I'm going to "plot training data" and just going to let it rip. The color diagram shows the decision boundaries, with the training data superimposed on it. Let's look at what OneR does to this dataset in the Explorer.
OneR has chosen to split on petalwidth. If it's less than a certain amount, we get a setosa; if it's intermediate, we get a versicolor; and if it's greater than the upper boundary, we get a viriginica. It's the same as what's being shown here. We're splitting on petalwidth. If it's less than a certain amount, we get a setosa; in the middle, a versicolor; and at the top, a virginica. This is a spatial representation of the decision boundary that OneR creates on this dataset. That's what the Boundary Visualizer does; it draws decision boundaries. It shows here that OneR chooses an attribute -- in this case petalwidth -- to split on.
It might have chosen petallength, in which case we'd have vertical decision boundaries. Either way, we're going to get stripes from OneR. I'm going to go ahead and look at some boundaries for other schemes. Let's look at IBk, which is a "lazy" classifier. That's the instance-based learner we looked at in the last class. I'm going to run that. Here we get a different kind of pattern. I'd like to plot the training data. We've got diagonal lines. Down here are the setosas underneath this diagonal line; the versicolors in the intermediate region; and the virginicas, by and large, in the top right-hand corner. Remember what [IBk] does. It takes a test instance.
Let's say we had an instance here, just on this side of the boundary, in the red. Then it chooses the nearest instance to that. That would be this one, I guess. That's kind of the nearer than this one here. This is a red point. If I were to cross over the boundary here, it would choose a green class, because this would be the nearest instance then. If you think about it, this boundary goes halfway between this nearest red point and this nearest green point. Similarly, if I take a point up here, I guess the two nearest instances are this blue one and this green one. This blue one is closer.
In this case, the boundary goes along this straight line here.
You can see that it's not just a single line: this is a piecewise linear line, so this part of the boundary goes exactly halfway between these two points quite close to it. Down here, the boundary goes exactly halfway between these two points. It's the perpendicular bisector of the line joining these points. So we get a piecewise linear boundary made up of little pieces.
It's kind of interesting to see what happens if we change the parameter: if we look at, say, 5 nearest neighbors instead of just 1. Now we get a slightly blurry picture, because whereas down here in the pure red region the 5 nearest neighbors to a point are all red points, if we look in the intermediate region here, then the nearest neighbors to a point here -- this is going to be in the 5, and this might be another one in the 5, and there might be a couple more down here in the 5. So we get an intermediate color here, and IBk takes a vote.
If we had 3 reds and 2 greens, then we'd be in the red region and that would be depicted as this darker red here. If it had been the other way round with more greens than reds, we'd be in the green region. So we've got a blurring of these boundaries. These are probabilistic descriptions of the boundary. Let me just change k to 20 and see what happens.
Now we get the same shape, but even more blurry boundaries. The Boundary Visualizer reveals the way that machine learning schemes are thinking, if you like. The internal representation of the dataset. They help you think about the sorts of things that machine learning methods do. Let's choose another scheme. I'm going to choose NaiveBayes. When we talked about NaiveBayes, we only talked about discrete attributes. With continuous attributes, I'm going to choose a supervised discretization method. Don't worry about this detail, it's the most common way of using NaiveBayes with numeric attributes.
Let's look at that picture. This is interesting. When you think about NaiveBayes, it treats each of the two attributes as contributing equally and independently to the decision. It sort of decides what it should be along this dimension and decides what it should be along this dimension and multiples the two together. Remember the multiplication that went on in NaiveBayes. When you multiple these things together, you get a checkerboard pattern of probabilities, multiplying up the probabilities. That's because the attributes are being treated independently. That's a very different kind of decision boundary from what we saw with instance-based learning.
That's what's so good about the Boundary Visualizer: it helps you think about how things are working inside. I'm going to do one more example. I'm going to do J48, which is in trees.
Here we get this kind of structure. Let's take a look at what happens in the Explorer if we choose J48.
We get this little decision tree: split first on petalwidth; if it's less than 0.6 it's a setosa for sure. Then split again on petalwidth; if it's greater than 1.7, it's a virginica for sure. Then, in between, split on petallength and then again on petalwidth, getting a mixture of versicolors and viriginicas. We split first on petalwidth; that's this split here. Remember the vertical axis is the petalwidth axis. If it's less than a certain amount, it's a setosa for sure. Then we split again on the same axis. If it's greater than a certain amount, it's a virginica for sure. If it's in the intermediate region, we split on the other axis, which is petallength.
Down here, it's a versicolor for sure, and here we're going to split again on the petalwidth attribute. Let's change the minNumObj parameter, which controls the minimum size of the leaves. If we increase that, we're going to get a simpler tree. We discussed this parameter in one of the lessons of Class 3. If we run now, then we get a simpler version, corresponding to the simpler rules we get with this parameter set. Or we can set the parameter to a higher value, say 10, and run it again. We get even simpler rules, very similar to the rules produced by OneR. We've looked at classification boundaries.
Classifiers create boundaries in instance space and different classifiers have different capabilities for carving up instance space. That's called the "bias" of the classifier -- the way in which it's capable of carving up the instance space. We looked at OneR, IBk, NaiveBayes, and J48, and found completely different biases, completely different ways they carve up the instance space. Of course, this kind of visualization is restricted to numeric attributes and 2-dimensional plots, so it's not a very general tool, but it certainly helps you think about these different classifiers.
<End Transcript>

<-- 4.3 Quiz -->
Exploring classification boundaries
Question 1
Choose the IBk classifier with default options and start the boundary visualization. You will notice a small, faint region of mixed color (green and blue). How can you get mixed colors when only one nearest neighbor is being used?
There are instances with different classes at the same point of the graph
IBk has a bug
IBk defaults to two nearest neighbors
IBk defaults to more than two nearest neighbors
---
Correct answer(s):
There are instances with different classes at the same point of the graph
---
Feedback correct:
Examine the points in this small region using the Explorer’s Visualize panel.
Instances 71, 127 and 139 have the same attribute values, but their classes are versicolor, virginica and virginica respectively. The colors on the plot are mixed in the corresponding proportions.
---
Feedback incorrect:
Examine the points in this small region using the Explorer’s Visualize panel.

<-- 4.3 Quiz -->
Exploring classification boundaries
Question 2
Look at the decision boundary created by the Logistic regression method, which we will learn about later this week. You will find it at functions > Logistic.
What can you say about the boundary shapes?
The boundaries are non-linear
Crisp boundaries, changing abruptly from one color to another
Gradual transition from one color to another
Checkered pattern, crisp boundaries, no mixed colors
---
Correct answer(s):
Gradual transition from one color to another
---
Feedback correct:
We will discover why this happens in the Logistic regression activity.

<-- 4.3 Quiz -->
Exploring classification boundaries
Question 3
Look at the decision boundary created by the Support Vector Machine method, which we will also learn about later this week. You will find it at functions > SMO.
What can you say about the resulting plot?
The boundaries are non-linear
There are no areas of pure color
Checkered pattern with crisp boundaries
Checkered pattern with slightly fuzzy boundaries
---
Correct answer(s):
There are no areas of pure color
---
Feedback correct:
This is most striking for red, but the greenish and bluish colors are not pure either. The system is never completely certain about the classification of any point!

<-- 4.3 Quiz -->
Exploring classification boundaries
Question 4
Look at the decision boundary created by the Random Forest method, which we will learn about in the Ensemble learning activity at the end of this week. You will find it at trees > RandomForest.
What can you say about the boundary shapes?
Crisp boundaries, changing abruptly from one color to another
There are no areas of pure color
Gradual transition from one color to another
Checkered pattern with slightly fuzzy boundaries
---
Correct answer(s):
Checkered pattern with slightly fuzzy boundaries
---
Feedback correct:
As we will learn in the Ensemble learning activity, random forests are collections of trees, all slightly different.
---
Feedback incorrect:
There are areas of pure red, pure green, and pure blue.

<-- 4.3 Quiz -->
Exploring classification boundaries
Question 4
Look at the decision boundary created by the Random Forest method, which we will learn about in the Ensemble learning activity at the end of this week. You will find it at trees > RandomForest.
What can you say about the boundary shapes?
Crisp boundaries, changing abruptly from one color to another
There are no areas of pure color
Gradual transition from one color to another
Checkered pattern with slightly fuzzy boundaries
---
Correct answer(s):

<-- 4.4 Video -->
Linear regression
Classification involves a nominal class value, whereas regression involves a numeric class. Linear regression is a classical statistical method that computes the coefficients or “weights” of a linear expression, and the predicted (“class”) value is the sum of each attribute value multiplied by its weight. A model tree is a tree where each leaf is a linear regression model; it’s like a patchwork of linear models. Linear regression and model trees are both easy to do in Weka.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
A classification problem is when what you're trying to predict is a nominal value, whereas in a regression problem what you're trying to predict is a numeric value. We've seen examples of datasets with nominal and numeric attributes before, but we've never looked at the problem of regression, of trying to predict a numeric value as the output of a machine learning scheme. This is a classical statistical method, dating back more than 2 centuries. This is the kind of picture you see. You have a cloud of data points in 2 dimensions, and we're trying to fit a straight line to this cloud of data points and looking for the best straight-line fit.
Only in our case we might have more than 2 dimensions, there might be multiple dimensions. It's still a standard problem. Let's just look at the 2-dimensional case here. You can write a straight line equation in this form, with weights w0 plus w1a1 plus w2a2, and so on. Just think about this in one dimension where there's only one "a". Forget about all the things at the end here, just consider w0 plus w1a1. That's the equation of this line -- it's the equation of a straight line -- where w0 and w1 are two constants to be determined from the data. This, of course, is going to work most naturally with numeric attributes, because we're multiplying these attribute values by weights.
We'll worry about nominal attributes in just a minute. We're going to calculate these weights from the training data -- w0, w1, and w2. Those are what we're going to calculate from the training data. Then, once we've calculated the weights, we're going to predict the value for the first training instance, a1. The notation gets really horrendous here. I know it looks pretty scary, but it's pretty simple. We're using this linear sum with these weights that we've calculated, using the attribute values of the first [training] instance in order to get the predicted value for that. We're going to get predicted values for the training instances using this rather horrendous formula here.
I know it looks pretty scary, but it's actually not so scary. These w's are just numbers that we've calculated from the training data, and then these things here are the attribute values of the first training instance a1 -- that 1 at the top here means it's the first training instance. This 1, 2, 3 means it's the first, second, and third attribute. We can write this in this neat little sum form here, which looks a little bit better. Notice, by the way, that we're defining a0 -- the zeroth attribute value -- to be 1. That just makes this formula work.
For the first training instance, that gives us this number x, the predicted value for the first training instance and this particular value of a1. Then we're choosing the weights to minimize the squared error on the training data. This is the actual x value for this i'th training instance. This is the predicted value for the i'th training instance. We're going to take the difference between the actual and the predicted value, square them up, and add them all together. And that's what we're trying to minimize. We get the weights by minimizing this sum of squared errors. That's a mathematical job; we don't need to worry about the mechanics of doing that. It's a standard matrix problem.
It works fine if there are more instances than attributes. You couldn't expect this to work if you had a huge number of attributes and not very many instances. But providing there are more instances than attributes -- and usually there are, of course -- that's going to work ok. If we did have nominal values, if we just have a 2-valued/binary-valued, we could just convert it to 0 and 1 and use those numbers. If we have multi-valued nominal attributes, you'll have a look at that in the activity at the end of this lesson.
We're going to open a regression dataset and see what it does: cpu.arff. This is a regular kind of dataset. It's got numeric attributes, and the most important thing here is that it's got a numeric class -- we're trying to predict a numeric value. We can run LinearRegression; it's in the functions category. We just run it, and this is the output. We've got the model here. The class has been predicted as a linear sum. These are the weights I was talking about. It's this weight times this attribute value plus this weight times this attribute value, and so on. Minus -- and this is w0, the constant weight, not modified by an attribute. This is a formula for computing the class.
When you use that formula, you can look at the success of it in terms of the training data. The correlation coefficient, which is a standard statistical measure, is 0.9. That's pretty good. Then there are various other error figures here that are printed. On the slide, you can see the interpretation of these error figures. It's really hard to know which one to use. They all tend to produce the same sort of picture, but I guess the exact one you should use depends on the application. There's the mean absolute error and the root mean squared error, which is the standard metric to use. That's linear regression. I'm actually going to look at nonlinear regression here.
A "model tree" is a tree where each leaf has one of these linear regression models. We create a tree like this, and then at each leaf we have a linear model, which has got those coefficients. It's like a patchwork of linear models, and this set of 6 linear patches approximates a continuous function. There's a method under "trees" with the rather mysterious name of M5P. If we just run that, that produces a model tree. Maybe I should just visualize the tree. Now I can see the model tree, which is similar to the one on the slide. You can see that each of these -- in this case 5 -- leaves has a linear model -- LM1,
LM2, LM3, … And if we look back here, the linear models are defined like this: LM1 has this linear formula; this linear formula for LM2; and so on. We chose trees > M5P, we ran it, and we looked at the output. We could compare these performance figures -- 92-93% correlation, mean absolute error of 30, and so on -- with the ones for regular linear regression, which got a slightly lower correlation, and a slightly higher absolute error -- in fact, I think all these error figures are slightly higher. That's something we'll be asking you to do in the activity associated with this lesson. Linear regression is a well-founded, venerable mathematical technique. Practical problems often require non-linear solutions.
The M5P method builds trees of regression models, with linear models at each leaf of the tree.
<End Transcript>

<-- 4.5 Quiz -->
Regression with nominal attributes
Question 1
How many different values does that nominal attribute have?
4
8
30
32
209
---
Correct answer(s):
30
---
Feedback correct:
The nominal attribute is vendor. These were all computer sales companies, back in the old days.

<-- 4.5 Quiz -->
Regression with nominal attributes
Question 2
Run the LinearRegression classifier (with default options) on the cpu.with.vendor dataset, using 10-fold cross-validation. Several different measures are printed. Linear regression optimizes the root mean squared error; small is good. Instead we will look at the correlation coefficient. Large correlation is good, and the value cannot be greater than 1. What is the correlation coefficient in this case?
0.92
0.93
0.95
0.96
---
Correct answer(s):
0.93
---
Feedback correct:
Weka gives the result as 0.9257
---
Feedback incorrect:
Are you using 10-fold cross validation as the evaluation method? This is the result you get when you evaluate using Percentage split.
---
Feedback incorrect:
Are you using 10-fold cross validation as the evaluation method? This is the result you get when you evaluate on the training set.

<-- 4.5 Quiz -->
Regression with nominal attributes
Question 3
Now find the M5P tree-based algorithm and run it (with default options), again using 10-fold cross-validation. What is the correlation coefficient now?
0.93
0.97
0.98
0.99
---
Correct answer(s):
0.98
---
Feedback correct:
Weka gives the result as 0.9766. M5P performs quite a lot better than LinearRegression.
---
Feedback incorrect:
Are you using M5P as the classifier? This is the result you get with LinearRegression.
---
Feedback incorrect:
Are you using 10-fold cross validation as the evaluation method? This is the result you get when evaluating on the training set.

<-- 4.5 Quiz -->
Regression with nominal attributes
Question 4
M5P produces a tree with linear models at the leaves. How many linear models are there in this case?
1
2
4
10
209
---
Correct answer(s):
2
---
Feedback correct:
You can see them in the classifier output, and also in the tree visualizer accessible from the right-click menu. The models are called LM 1 and LM 2.

<-- 4.5 Quiz -->
Regression with nominal attributes
Question 5
The single nominal attribute in this dataset is called vendor. Both LinearRegression and M5P convert it into several binary attributes, with values 0 and 1, that are used in the weighted summation produced by the regression method.
How many of these binary attributes are used?
Select all the answers you think are correct.
1 for LinearRegression
2 for LinearRegression
3 for LinearRegression
11 for LinearRegression
1 for M5P
2 for M5P
3 for M5P
11 for M5P
---
Correct answer(s):
11 for LinearRegression
3 for M5P
---
Feedback correct:
The model produced by LinearRegression contains 11 attributes of the form vendor = …
Both linear models produced by M5P contain 3 attributes of the form vendor = …
---
Feedback incorrect:
The model produced by LinearRegression contains 11 attributes of the form vendor = …
---
Feedback incorrect:
Both linear models produced by M5P contain 3 attributes of the form vendor = …

<-- 4.5 Quiz -->
Regression with nominal attributes
Question 6
Weka has a supervised attribute filter that converts a nominal attribute into the same set of binary attributes used by LinearRegression and M5P. What is it called?
AddClassification
AttributeSelection
ClassOrder
Discretize
NominalToBinary
PLSFilter
---
Correct answer(s):
NominalToBinary
---
Feedback correct:
Try it! Note: be sure to select the supervised version of this filter, not the unsupervised one.

<-- 4.5 Quiz -->
Regression with nominal attributes
Question 7
There is also an unsupervised attribute filter with the same name, NominalToBinary. This also converts a nominal attribute into a set of binary attributes, but in a different way. What does it do?
Produces a smaller number of binary attributes
Produces a larger number of binary attributes
Creates one binary attribute for each value of the nominal attribute
Uses larger value sets for each binary attribute
Uses different value sets for each binary attribute
---
Correct answer(s):

<-- 4.5 Quiz -->
Regression with nominal attributes
Question 7
There is also an unsupervised attribute filter with the same name, NominalToBinary. This also converts a nominal attribute into a set of binary attributes, but in a different way. What does it do?
Produces a smaller number of binary attributes
Produces a larger number of binary attributes
Creates one binary attribute for each value of the nominal attribute
Uses larger value sets for each binary attribute
Uses different value sets for each binary attribute
---
Correct answer(s):
Creates one binary attribute for each value of the nominal attribute

<-- 4.6 Video -->
Classification by regression
Linear regression can be used for classification too. On the diabetes data, use the NominalToBinary filter to convert the two classes, which are nominal, to the numeric values 0 and 1, and apply linear regression. The result is a predicted number between 0 and 1 for each instance. The addClassification filter is used to add that number as a new attribute; then OneR is applied to choose a good split point on that attribute to predict the original two classes. The procedure is a bit cumbersome, but the result works quite well as a classifier.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! Welcome back! In the last lesson, we looked at linear regression -- the problem of predicting, not a nominal class value, but a numeric class value. The regression problem. In this lesson, we're going to look at how to use regression techniques for classification. It sounds a bit weird, but regression techniques can be really good under certain circumstances, and we're going to see if we can apply them to ordinary classification problems. In a 2-class problem, it's quite easy really.
We're going to call the 2 classes 0 and 1 and just use those as numbers, and then come up with a regression line that, presumably for most 0 instances has a pretty low value, and for most 1 instances has a larger value, and then come up with a threshold for determining whether, if it's less than that threshold, we're going to predict class 0; if it's greater, we're going to predict class 1. If we want to generalize that to more than 2 classes, we can use a separate regression for each class. We set the output to 1 for instances that belong to the class, and 0 for instances that don't.
Then come up with a separate regression line for each class, and given an unknown test example, we're going to choose a class with the largest output. That would give us n regressions for a problem where there are n different classes.
We could alternatively use pairwise regression: take every pair of classes -- that's n squared over 2 -- and have a linear regression line for each pair of classes, discriminating an instance in one class of that pair from the other class of that pair. We're going to work with a 2-class problem, and we're going to investigate 2-class classification by regression. I'm going to open diabetes.arff. Then I'm going to convert the class. Actually, let's just try to apply regression to this. I'm going to try LinearRegression. You see it's grayed out here. That means it's not applicable. I can select it, but I can't start it.
It's not applicable because linear regression applies to a dataset where the class is numeric, and we've got a dataset where the class is nominal. We need to fix that. We're going to change this from these 2 labels to 0 and 1, respectively. We'll do that with a filter. We want to change an attribute. It's unsupervised. We want to change a nominal to a binary attribute, so that's the NominalToBinary filter. We want to apply that to the 9th attribute. The default will apply it to all the attributes, but we just want to apply it to the 9th attribute. I'm hoping it will change this attribute from nominal to binary. Unfortunately, it doesn't.
It doesn't have any effect, and the reason it doesn't have any effect is because these attribute filters don't work on the class value. I can change the class value; we're going to give this "No class", so now this is not the class value for the dataset. Run the filter again.
Now I've got what I want: this attribute "class" is either 0 or 1. In fact, this is the histogram -- there are this number of 0's and this number of 1's, which correspond to the two different values in the original dataset. Now, we've got our LinearRegression, and we can just run it.
This is the regression line. It's a line, 0.02 times the "pregnancy" attribute, plus this times the "plas" attribute, and so on, plus this times the "age" attribute, plus this number. That will give us a number for any given instance. We can see that number if we select "Output predictions" and run it again. Here is a table of predictions for each instance in the dataset. This is the instance number; this is the actual class of the instance, which is 0 or 1; this is the predicted class, which is a number -- sometimes it's less than 0. We would hope that these numbers are generally fairly small for 0's and generally larger for 1's.
They sort of are, although it's not really easy to tell. This is the error value here in the fourth column. I'm going to do more extensive investigation, and you might ask why are we bothering to do this? First of all, it's an interesting idea that I want to explore. It will lead to quite good performance for classification by regression, and it will lead into the next lesson on logistic regression, which is an excellent classification technique. Perhaps most importantly, we'll learn how to do some cool things with the Weka interface. My strategy is to add a new attribute called "classification" that gives this predicted number, and then we're going to use OneR to optimize a split point for the two classes.
We'll have to restore the class back to its original nominal value, because, remember, I just converted it to numeric. Here it is in detail. We're going to use a supervised attribute filter [AddClassification]. This is actually pretty cool, I think.
We're going to add a new attribute called "classification". We're going to choose a classifier for that -- LinearRegression.
We need to set "outputClassification" to "True". If we just run this, it will add a new attribute to the dataset. It's called "classification", and it's got these numeric values, which correspond exactly to the numeric values that were predicted here by the linear regression scheme. Now, we've got this "classification" attribute, and what I'd like to do now is to convert the class attribute back to nominal from numeric. I want to use ZeroR now, and ZeroR will only work with a nominal class. Let me convert that. I want NumericToNominal.
I want to run that on attribute number 9.
Let me apply that, and now, sure enough, I've got the two labels 0 and 1. This is a nominal attribute with these two labels. I'll be sure to make that one the class attribute.
Then I get the colors back -- 2 colors for the 2 classes. Really, I want to predict this "class" based on the value of "classification", that numeric value. I'm going to delete all the other attributes.
I'm going to go to my Classify panel here. I'm going to predict "class" -- this nominal value "class" -- and I'm going to use OneR.
I think I'll stop outputting the predictions because they just get in the way; and run that.
It's 72-73%, and that's a bit disappointing. But actually, when you look at this, OneR has produced this really overfitted rule. We want a single split point. If it's less than this than predict 0, otherwise predict 1. We can get around that by changing this "b" parameter, the minBucketSize parameter, to be something much larger. I'm going to change it to 100 and run it again. Now I've got much better performance, 77% accuracy, and this is the kind of split I've
got: if the classification -- that is the regression value -- is less than 0.47 I'm going to call it a 0; otherwise I'm going to call it a 1. So I've got what I wanted, classification by regression. We've extended linear regression to classification. This performance of 76.8% is actually quite good for this problem. It was easy to do with 2 classes, 0 and 1; otherwise you need to have a regression for each class -- multi-response linear regression -- or else for each pair of classes -- pairwise linear regression. We learnt quite a few things about Weka. We learned about unsupervised attribute filters to convert nominal attributes to binary, and numeric attributes back to nominal.
We learned about this cool filter AddClassification, which adds the classification according to a machine learning scheme as an attribute in the dataset. We learned about setting and unsetting the class of the dataset, and we learned about the minimum bucket size parameter to prevent OneR from overfitting. That's classification by regression. In the next lesson, we're going to do better. We're going to look at logistic regression, an advanced technique which effectively does classification by regression in an even more effective way. We'll see you soon. Bye!
<End Transcript>

<-- 4.7 Quiz -->
Multi-response linear regression
Question 1
In the Classify panel, check the LinearRegression classifier. You will find that it is grayed out. Why?
The attributes in the dataset are all numeric
There are not enough attributes in the dataset
Linear regression only works in New Zealand
The class is not numeric
---
Correct answer(s):
The class is not numeric
---
Feedback correct:
LinearRegression works with numeric classes, not with nominal ones.

<-- 4.7 Quiz -->
Multi-response linear regression
Question 2
Apply the MakeIndicator filter (which is unsupervised), with default parameters. How does it affect the class value (check 3 boxes)?
Select all the answers you think are correct.
Makes it 0 for Iris-setosas
Makes it 1 for Iris-setosas
Makes it 0 for Iris-versicolors
Makes it 1 for Iris-versicolors
Makes it 0 for Iris-virginicas
Makes it 1 for Iris-virginicas
---
Correct answer(s):
Makes it 0 for Iris-setosas
Makes it 0 for Iris-versicolors
Makes it 1 for Iris-virginicas
---
Feedback correct:
The Iris-virginicas are singled out, but it’s a bit hard to determine that. Use the Edit button to check the dataset before and after applying the filter – the instance numbers don’t change.
---
Feedback incorrect:
The Iris-virginicas are singled out, but it’s a bit hard to determine that. Use the Edit button to check the dataset before and after applying the filter – the instance numbers don’t change.

<-- 4.7 Quiz -->
Multi-response linear regression
Question 3
Run the LinearRegression classifier on the filtered dataset, using 10-fold cross-validation. But first, be sure to click Output predictions in the More options menu and output the predictions as PlainText. You will need these later on.
What is the correlation coefficient?
0.25
0.30
0.77
0.89
---
Correct answer(s):
0.77
---
Feedback correct:
Weka gives the result as 0.7676.
---
Feedback incorrect:
Are you looking at the correlation coefficient?

<-- 4.7 Quiz -->
Multi-response linear regression
Question 4
Which attribute or attributes are used in the model that Weka outputs?
Select all the answers you think are correct.
sepallength
sepalwidth
petallength
petalwidth
class
---
Correct answer(s):
sepalwidth
petalwidth
---
Feedback correct:
Weka outputs the model
 0.1791 * sepalwidth  +  0.513 * petalwidth  –  0.8286
Weka outputs the model
 0.1791 * sepalwidth  +  0.513 * petalwidth  –  0.8286
---
Feedback incorrect:
Weka outputs the model
 0.1791 * sepalwidth  +  0.513 * petalwidth  –  0.8286

<-- 4.7 Quiz -->
Multi-response linear regression
Question 5
Undo your changes in the Preprocess panel, and investigate the effect of the filter’s valueIndices parameter. Use it to make a class value that is 1 only for Iris-versicolors. Run LinearRegression again, with the same settings. 
What is the correlation coefficient?
0.45
0.46
0.94
0.95
---
Correct answer(s):
0.46
---
Feedback correct:
(Weka gives the result as 0.458)
valueIndices specifies a range of nominal values to act on. The default is for the filter to set the last attribute (the class) to its last value (3, for Iris-virginica). To change to Iris-versicolor, set valueIndex to 2.

<-- 4.7 Quiz -->
Multi-response linear regression
Question 6
Which attribute is not used in the model that Weka outputs?
sepallength
sepalwidth
petallength
petalwidth
---
Correct answer(s):
sepallength
---
Feedback correct:
Weka outputs the model
   –0.4548*sepalwidth+0.2032*petallength–0.4711*petalwidth+1.5232

<-- 4.7 Quiz -->
Multi-response linear regression
Question 7
Now repeat the above for the Iris-setosas. What is the correlation coefficient?
0.46
0.77
0.94
0.95
---
Correct answer(s):
0.95
---
Feedback correct:
Weka gives the result as 0.9456

<-- 4.7 Quiz -->
Multi-response linear regression
Question 8
The Multi-response linear regression method chooses the class of an instance according to whichever of the three regression formulae produces the largest output. Judging by the correlations you have observed, which class do you think it will produce most errors for?
Iris-setosa
Iris-versicolor
Iris-virginica
petalwidth
---
Correct answer(s):
Iris-versicolor
---
Feedback correct:
The correlation coefficient for Iris-versicolor (0.46) is much smaller than for the others (0.77 and 0.95), so this class is likely to have the greatest number of errors.
---
Feedback incorrect:
That’s not a class value

<-- 4.7 Quiz -->
Multi-response linear regression
Question 9
Now click in Weka’s Result list to see the predictions that are output by the three models. Check the first four instances only. Weka outputs predictions in the shuffled order that is used by the cross-validation, not in the instances’ original order – which makes it hard to determine their true class. But the information is there. Note that although the shuffling order is random, it depends only on the random seed value and therefore remains the same throughout.
Multiresponse linear regression will make just one error on the first four instances (in shuffled order), of the total of 150 instances used in the 10-fold cross-validation. Which instance is it?
#1
#2
#3
#4
---
Correct answer(s):
#3
---
Feedback correct:
The first 4 instances belong to class Iris-virginica, Iris-virginica, Iris-versicolor, and Iris-setosa respectively. You can tell that from the three sets of Weka output.
The three models you have produced are for Iris-virginica, Iris-versicolor, and Iris-setosa respectively.
For instance#3 (Iris-versicolor), the first model outputs .359, which is greater than the output from the other two models. This means that this instance will be predicted as Iris-virginica. This is an error.
---
Feedback incorrect:
The first 4 instances belong to class Iris-virginica, Iris-virginica, Iris-versicolor, and Iris-setosa respectively. You can tell that from the three sets of Weka output.
The three models you have produced are for Iris-virginica, Iris-versicolor, and Iris-setosa respectively.
For instance#1 (Iris-virginica), the first model predicts .966, which is greater than the output from the other two models. This is not an error.
---
Feedback incorrect:
The first 4 instances belong to class Iris-virginica, Iris-virginica, Iris-versicolor, and Iris-setosa respectively. You can tell that from the three sets of Weka output.
The three models you have produced are for Iris-virginica, Iris-versicolor, and Iris-setosa respectively.
For instance#2 (Iris-virginica), the first model predicts .896, which is greater than the output from the other two models. This is not an error.
---
Feedback incorrect:
The first 4 instances belong to class Iris-virginica, Iris-virginica, Iris-versicolor, and Iris-setosa respectively. You can tell that from the three sets of Weka output.
The three models you have produced are for Iris-virginica, Iris-versicolor, and Iris-setosa respectively.
For instance#4 (Iris-setosa), the third model predicts .827, which is greater than the output from the other two models. This is correct.

<-- 4.7 Quiz -->
Multi-response linear regression
Question 10
As you saw in the last question, Weka outputs predictions in shuffled order. This can be a nuisance. However, a filter can be used to add the original instance numbers alongside the predictions. Which filter?
Add
AddCluster
AddClassification
AddID
---
Correct answer(s):

<-- 4.7 Quiz -->
Multi-response linear regression
Question 10
As you saw in the last question, Weka outputs predictions in shuffled order. This can be a nuisance. However, a filter can be used to add the original instance numbers alongside the predictions. Which filter?
Add
AddCluster
AddClassification
AddID
---
Correct answer(s):
AddID
---
Feedback correct:
When you click the More button, this is described as an instance filter that adds an ID attribute to the dataset. The new attribute contains a unique ID for each instance.

<-- 4.8 Video -->
Logistic regression
Many classification methods produce probabilities rather than black-or-white classifications. Naive Bayes is an obvious example, but other methods do too. The numbers between 0 and 1 produced by linear regression in the previous lesson may look like probabilities, but they’re not. However, a variant called “logistic regression” does produce probabilities. Logistic regression is a powerful classification that predict probabilities directly through something called the “logit transform”.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! Welcome back to Data Mining with Weka. In the last lesson, we looked at classification by regression, how to use linear regression to perform classification tasks. In this lesson we’re going to look at a more powerful way of doing the same kind of thing. It’s called “logistic regression”. It’s fairly mathematical, and we’re not going to go into the dirty details of how it works, but I’d like to give you a flavor of the kinds of things it does and the basic principles that underline logistic regression. Then, of course, you can use it yourself in Weka without any problem. One of the things about data mining is that you can sometimes do better by using prediction probabilities rather than actual classes.
Instead of predicting whether it’s going to be a “yes” or a “no”, you might do better to predict the probability with which you think it’s going to be a “yes” or a “no”. For example, the weather is 95% likely to be rainy tomorrow, or 72% likely to be sunny, instead of saying it’s definitely going to be rainy or it’s definitely going to be sunny. Probabilities are really useful things in data mining. Naïve Bayes produces probabilities; it works in terms of probabilities. We’ve seen that in an earlier lesson. I’m going to open “diabetes” and run Naïve Bayes.
I’m going to use a percentage split with 90%, so that leaves 10% as a test set.
Then I’m going to make sure I output the predictions on those 10%, and run it. I want to look at the predictions that have been output. This is a 2-class dataset, the classes are “tested_negative” and “tested_positive”, and these are the instances – number 1, number 2, number 3, etc. This is the actual class – tested_negative, tested_positive, tested_negative, etc. This is the predicted class – tested_negative, tested_negative, tested_negative, tested_negative, etc. This is a plus under the error column to say where there’s an error, so there’s an error with instance number 2. These are the actual probabilities that come out of NaiveBayes. So for instance 1 we’ve got a 99% probability that it’s negative, and a 1% probability that it’s positive.
So we predict it’s going to be negative; that’s why that’s tested_negative. And in fact we’re correct; it is tested_negative. This instance, which is actually incorrect, we’re predicting 67% percent for negative and 33% for positive, so we decide it’s a negative, and we’re wrong. We might have been better saying that here we’re really sure it’s going to be a negative, and we’re right; here we think it’s going to be a negative, but we’re really not sure, and it turns out that we’re wrong. Sometimes it’s a lot better to think in terms of the output as probabilities, rather than being forced to make a binary, black-or-white classification. Other data mining methods produce probabilities, as well.
If I look at ZeroR, and run that, these are the probabilities – 65% versus 35%. All of them are the same. Of course, it’s ZeroR! – it always produces the same thing. In this case, it always says tested_negative and always has the same probabilities. The reason why the numbers are like that, if you look at the slide here, is that we’ve chosen a 90% training set and a 10% test set, and the training set contains 448 negative instances and 243 positive instances. Remember the “Laplace Correction” in [the Simplicity first video, Week 3]? – we add 1 to each of those counts to get 449 and 244. That gives us a 65% probability for being a negative instance.
That’s where these numbers come from. If we look at J48 and run that, then we get more interesting probabilities here – the negative and positive probabilities, respectively. You can see where the errors are. These probabilities are all different. Internally, J48 uses probabilities in order to do its pruning operations. We talked about that when we discussed J48’s pruning, although I didn’t explain explicitly how the probabilities are derived. The idea of logistic regression is to make linear regression produce probabilities, too. This gets a little bit hairy. Remember, when we use linear regression for classification, we calculate a linear function using regression and then apply a threshold to decide whether it’s a 0 or a 1.
It’s tempting to imagine that you can interpret these numbers as probabilities, instead of thresholding like that, but that’s a mistake. They’re not probabilities. These numbers that come out on the regression line are sometimes negative, and sometimes greater than 1. They can’t be probabilities, because probabilities don’t work like that. In order to get better probability estimates, a slightly more sophisticated technique is used. In linear regression, we have a linear sum. In logistic regression, we have the same linear sum down here – the same kind of linear sum that we saw before – but we embed it in this kind of formula. This is called a “logit transform”. A logit transform – this is multi-dimensional with a lot of different a’s here.
If we’ve got just one dimension, one variable a1, then if this is the input to the logit
transform, the output looks like this: it’s between 0 and 1. It’s sort of an S-shaped curve that applies a softer function. Rather than just 0 and then a step function, it’s soft version of a step function that never gets below 0, never gets above 1, and has a smooth transition in between. When you’re working with a logit transform, instead of minimizing the squared error (remember, when we do linear regression we minimize the squared error), it’s better to choose weights to maximize a probabilistic function called the “log-likelihood function”, which is this pretty scary looking formula down at the bottom. That’s the basis of logistic regression.
We won’t talk about the details any more: let me just do it. We’re going to use the “diabetes” dataset. In the last lesson we got 76.8% with classification by regression. Let me tell you if you do ZeroR, NaiveBayes, and J48, you get these numbers here. I’m going to find the logistic regression scheme. It’s in “functions”, and called “Logistic”. I’m going to use 10-fold cross-validation. I’m not going to output the predictions. I’ll just run it – and I get 77.2% accuracy. That’s the best figure in this column, though it’s not much better than Naïve Bayes, so you might be a bit skeptical about whether it really is better.
I did this 10 times and calculated the means myself, and we get these figures for the mean of 10 runs. ZeroR stays the same, of course, at 65.1%; it produces the same accuracy on each run. NaiveBayes and J48 are different, and here logistic regression gets an average of 77.5%, which is appreciably better than the other figures in this column. You can extend the idea to multiple classes. When we did this in the previous lesson, we performed a regression for each class, a multi-response regression. That actually doesn’t work well with logistic regression, because you need the probabilities to sum to 1 over the various different classes. That introduces more computational complexity and needs to be tackled as a joint optimization problem.
The result is logistic regression, a popular and powerful machine learning method that uses the logit transform to predict probabilities directly. It works internally with probabilities, like Naïve Bayes does. We also learned in this lesson about prediction probabilities that can be obtained from other methods, and how to calculate probabilities from ZeroR.
<End Transcript>

<-- 4.9 Quiz -->
Try out some classifiers
Question 1
Fill in the blanks with the words ZeroR, NaiveBayes, J48 and Logistic to reflect the results that you obtained on the glass.arff dataset.
---
Correct answer(s):

<-- 4.9 Quiz -->
Try out some classifiers
Question 2
Repeat the same four algorithms (ZeroR, NaiveBayes, J48, Logistic) and 10-fold cross-validation on the labor.arff dataset. In each case record the root mean squared error. What is the correct ordering in terms of this measure?
---
Correct answer(s):

<-- 4.9 Quiz -->
Try out some classifiers
Question 3
Repeat the same four algorithms (ZeroR, NaiveBayes, J48, Logistic) and 10-fold cross-validation on the breast-cancer.arff dataset, in each case recording the classification accuracy. What is the correct ordering in terms of accuracy?
---
Correct answer(s):

<-- 4.9 Quiz -->
Try out some classifiers
Question 4
In this question we will examine the prediction probabilities that OneR generates, and learn how they are calculated. These probabilities are always either 0 or 1.
Open the ionosphere.arff dataset, select OneR as classifier and use the training set for testing. OneR generates this rule:
a06:
 < -0.217515   -> b
 < -0.010275   -> g
 < 0.00101     -> b
 < 0.110845    -> g
 < 0.15505     -> b
 < 0.7955      -> g
 >= 0.7955     -> b
Use Weka to help you fill in the following table for the first 6 instances of the training set. To get prediction probabilities, click More options, under Output predictions select PlainText, and then click the box that says PlainText in order to configure it. For outputDistribution, select True.
OneR uses one of the following methods to calculate the probabilities when the above rule is used to make a prediction for an instance. Use the information in your table to determine which method.
Evaluate attribute a06 against all conditions of the rule and predict the class that was chosen most often, with probability 1. Other classes get probability 0.
Check a06 against each condition in turn. The first match determines the class, with a probability of 1. Other classes get 0.
Check a06 against each condition in turn. The first match determines the class, and its probability is calculated as (position of condition)/(number of conditions). The remaining probability is evenly distributed between the other classes.
---
Correct answer(s):

<-- 4.9 Quiz -->
Try out some classifiers
Question 4
In this question we will examine the prediction probabilities that OneR generates, and learn how they are calculated. These probabilities are always either 0 or 1.
Open the ionosphere.arff dataset, select OneR as classifier and use the training set for testing. OneR generates this rule:
a06:
 < -0.217515   -> b
 < -0.010275   -> g
 < 0.00101     -> b
 < 0.110845    -> g
 < 0.15505     -> b
 < 0.7955      -> g
 >= 0.7955     -> b
Use Weka to help you fill in the following table for the first 6 instances of the training set. To get prediction probabilities, click More options, under Output predictions select PlainText, and then click the box that says PlainText in order to configure it. For outputDistribution, select True.
OneR uses one of the following methods to calculate the probabilities when the above rule is used to make a prediction for an instance. Use the information in your table to determine which method.
Evaluate attribute a06 against all conditions of the rule and predict the class that was chosen most often, with probability 1. Other classes get probability 0.
Check a06 against each condition in turn. The first match determines the class, with a probability of 1. Other classes get 0.
Check a06 against each condition in turn. The first match determines the class, and its probability is calculated as (position of condition)/(number of conditions). The remaining probability is evenly distributed between the other classes.
---
Correct answer(s):
Check a06 against each condition in turn. The first match determines the class, with a probability of 1. Other classes get 0.
---
Feedback correct:
Use the Preprocess panel’s Edit button to examine the raw data, and the Classify panel’s Output predictions option to determine the probabilities.
OneR takes the value of the attribute it has chosen, and evaluates each condition in turn. The first one that evaluates to true determines the class, which gets a probability of 1. All others get 0. For example, the first instance has a06 = 0.02306, which matches “a06 < 0.110845”, leading to class label g.
---
Feedback incorrect:
Use the Preprocess panel’s Edit button to examine the raw data, and the Classify panel’s Output predictions option to determine the probabilities.

<-- 4.10 Discussion -->
Yikes! The math! It’s too much!
In the last couple of activities you’ve seen some mathematics that you probably didn’t want to see, and you might have realized that you’ll never completely understand how all these machine learning methods work in detail. I want you to know that what I’m aiming to convey is the gist of modern machine learning methods, not the details. What’s important is that you can use them and that you understand a little bit of the principles behind how they work.
This might be time for a little cathartic discussion. Express how you feel. Let it all out. Know you’re not alone!

<-- 4.11 Video -->
Support vector machines
In essence, support vector machines drive a straight line between two classes, right down the middle of the channel – which you can see using Weka’s boundary visualizer. If the classes cannot be separated by a straight line, a device called the “kernel trick” enables support vector machines to make boundaries of different shapes, not just straight lines. Support vector machines are very resilient to overfitting, because the boundary depends on just a few well-chosen data points, not the entire training set. They are implemented by Weka’s SMO classifier.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello again. In most courses, there comes a point where things start to get a little tough. In the last couple of lessons, you’ve seen some mathematics that you probably didn’t want to see, and you might have realized that you’ll never completely understand how all these machine learning methods work in detail. I want you to know that what I’m trying to convey is the gist of modern machine learning methods, not the details. What’s important is that you can use them and that you understand a little bit of the principles behind how they work. And the math is almost finished. So hang in there; things will start to get easier – and anyway, there’s not far to
go: just a few more lessons. I told you before that I play music. Someone came round to my house last night with a contrabassoon. It’s the deepest, lowest instrument in the orchestra. You don’t often see or hear one. So, here I am, trying to play a contrabassoon for the first time.
I think this has got to be the lowest point of our course, Data Mining with Weka! Today I want to talk about support vector machines, another advanced machine learning technique. We looked at logistic regression in the last lesson, and we found that these produce linear boundaries in the space. In fact, here I’ve used Weka’s Boundary Visualizer to show the boundary produced by a logistic regression machine – this is on the 2D Iris data, plotting “petalwidth” against “petallength”. This black line is the boundary between these classes, the red class and the green class.
It might be more sensible, if we were going to put a boundary between these two classes, to try and drive it through the widest channel between the two classes, the maximum separation from each class. Here’s a picture where the black line now is right down the middle of the channel between the two classes. Actually, mathematically, we can find that line by taking the two critical members, one from each class – they’re called “support vectors”; these are the critical points that define the channel – and take the perpendicular bisector of the line joining those two support vectors. That’s the idea of support vector machines.
We’re going to put a line between the two classes, but not just any old line that separates them. We’re trying to drive the widest channel between the two classes. Here’s another picture. We’ve got two clouds of points, and I’ve drawn a line around the outside of each cloud – the green cloud and the brown cloud. It’s clear that any interior points aren’t going to affect this hyperplane, this plane, this separating line. I call it a line, but in multi dimensions it would be a plane, or a hyperplane in four or more dimensions.
There are just a few of the points in each cloud that define the position of the line: the support vectors. In this case, there are [three] points. Support vectors define the boundary. The thing is that all the other instances in the training data could be deleted without changing the position of the dividing hyperplane. There’s a simple equation – and this is the last equation in this course – a simple equation that gives the formula for the maximum margin hyperplane as a sum over the support vectors. These are a vector product with each of the support vectors, and the sum there. It’s pretty simple to calculate this maximum margin hyperplane once you’ve got the support vectors.
It’s a very easy sum, and, like I say, it only depends on the support vectors. None of the other points play any part in this calculation. Now in real life, you might not be able to drive a straight line between the classes. Classes are called “linearly separable” if there exists a straight line that separates the two classes. In this picture, the two classes are not linearly separable. It might be a little hard to see, but there are some blue points on the green side of the line, and a couple of green points on the blue side of the line. It’s not possible to get a single straight line that divides these points.
That makes support vector machines – the mathematics – a little more complicated. But it’s still possible to define the maximum margin hyperplane under these conditions.
That’s it: support vector machines. It’s a linear decision boundary. Actually, there’s a really clever technique which allows you to get more complex boundaries. It’s called the “Kernel trick”. By using different formulas for the “kernel” – and in Weka you just select from some possible different kernels – you can get different shapes of boundaries, not just straight lines. Support vector machines are fantastic because they’re very resilient to overfitting. The boundary just depends on a very small number of points in the dataset. So it’s not going to overfit the dataset, because it doesn’t depend on almost all of the points in the dataset, just a few of these critical points – the support vectors.
So it’s very resilient to overfitting, even with large numbers of attributes. In Weka, there are a couple of implementations of support vector machines. We could look in the “functions” category for “SMO”. Let me have a look at that over here. If I look in “functions” for “SMO”, that implements an algorithm called “Sequential Minimal Optimization” for training a support vector classifier. There are a few parameters here, including, for example, the different choice of kernels.
You can choose different kernels: you can play around and try out different things. There are a few other parameters. Actually, the SMO algorithm is restricted to two classes, so this will only work with a 2-class dataset. There are other, more comprehensive, implementations of support vector machines in Weka. There’s a library called “LibSVM”, an external library, and Weka has an interface to this library. This is a wrapper class for the LibSVM tools. You need to download these separately from Weka and put them in the right Java classpath. You can see that there are a lot of different parameters here, and, in fact, a lot of information on this support vector machine package.
<End Transcript>

<-- 4.12 Quiz -->
Decision boundaries and overfitting
Question 1
Which of the following machine learning methods produce decision boundaries that are strictly linear?
IBk
J48
Logistic regression
OneR
---
Correct answer(s):
Logistic regression
---
Feedback correct:
Logistic regression is a sophisticated way of choosing a linear decision boundary for classification.
---
Feedback incorrect:
For just two classes nearest-neighbor learning produces linear boundaries, but in general they are more complex.

<-- 4.12 Quiz -->
Decision boundaries and overfitting
Question 2
Which of the following machine learning methods produce decision boundaries that consist of axis-parallel segments?
IBk
J48
Logistic regression
SMO (Support vector machines)
---
Correct answer(s):
J48
---
Feedback correct:
Yep. The decision boundary for a test like “is attribute a < some number” is a line (or hyperplane) that’s perpendicular to the a axis and parallel to all the other axes.

<-- 4.12 Quiz -->
Decision boundaries and overfitting
Question 3
Which of the following machine learning methods produce decision boundaries that are piecewise linear (and not necessarily linear or axis-parallel)?
IBk
J48
Logistic regression
OneR
---
Correct answer(s):
IBk
---
Feedback correct:
Yep. The decision boundaries for IBk are collections of lines (or hyperplanes) that are the perpendicular bisectors of the lines joining every pair of training data points.

<-- 4.12 Quiz -->
Decision boundaries and overfitting
Question 4
Open the credit-g.arff dataset, run the four algorithms IBk, J48, Logistic, and SMO, and record the accuracy using 10-fold cross-validation. What order do the algorithms come in?
---
Correct answer(s):

<-- 4.12 Quiz -->
Decision boundaries and overfitting
Question 5
As you know, performance estimates obtained on the training set are overly optimistic because of overfitting. Just how badly an algorithm overfits can be judged in terms of the apparent (but illusory) performance improvement from cross-validation to training-set evaluation. Given what you know about how they operate, order these four algorithms according to the expected amount of overfitting, from greatest to least. Then confirm your intuition with a Weka experiment using the credit-g.arff dataset.
IBk overfits more than J48, which overfits more than SMO, which is comparable to Logistic
IBk overfits more than Logistic, which overfits more than SMO, which overfits more than J48
Logistic overfits more than IBk, which overfits more than J48, which overfits more than SMO
J48 is comparable to Logistic, which overfits more than SMO, which overfits more than IBk
---
Correct answer(s):

<-- 4.12 Quiz -->
Decision boundaries and overfitting
Question 5
As you know, performance estimates obtained on the training set are overly optimistic because of overfitting. Just how badly an algorithm overfits can be judged in terms of the apparent (but illusory) performance improvement from cross-validation to training-set evaluation. Given what you know about how they operate, order these four algorithms according to the expected amount of overfitting, from greatest to least. Then confirm your intuition with a Weka experiment using the credit-g.arff dataset.
IBk overfits more than J48, which overfits more than SMO, which is comparable to Logistic
IBk overfits more than Logistic, which overfits more than SMO, which overfits more than J48
Logistic overfits more than IBk, which overfits more than J48, which overfits more than SMO
J48 is comparable to Logistic, which overfits more than SMO, which overfits more than IBk
---
Correct answer(s):
IBk overfits more than J48, which overfits more than SMO, which is comparable to Logistic
---
Feedback correct:
IBk overfits dramatically: it’s 100% accurate on the training set (barring duplicate training points with different classes)! J48 can overfit because of the complex decision boundaries it can produce; the effect is ameliorated, but rarely completely eliminated, by its pruning algorithm. With this dataset, J48’s apparent accuracy improves from 70.5% to 85.5%. Logistic regression is a sophisticated way of producing a good linear decision boundary, which is necessarily simple and therefore less likely to overfit; here its apparent performance increases just a little, from 75.5% to 78.6%. SMO also produces a linear boundary, and accuracy increases from 75.1% to 78.4%.

<-- 4.13 Video -->
Ensemble learning
Sometimes committees make better decisions than individuals. An ensemble of different classification methods can be applied to the same problem and vote on the classification of test instances. Bagging, randomization, boosting and stacking are ensemble-based classification methods. It is good to have diverse classifiers in the ensemble, and these methods create diversity in different ways. Instead of voting, stacking combines results from an ensemble of different kinds of learner using another learner.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello again! In real life, when we have important decisions to make, we often choose to make them using a committee. Having different experts sitting down together, with different perspectives on the problem, and letting them vote, is often a very effective and robust way of making good decisions. The same is true in machine learning. We can often improve predictive performance by having a bunch of different machine learning methods, all producing classifiers for the same problem, and then letting them vote when it comes to classifying an unknown test instance. One of the disadvantages is that this produces output that is hard to analyze.
There are actually approaches that try and produce a single comprehensible structure, but we’re not going to be looking at any of those. So the output will be hard to analyze, but you often get very good performance. It’s a fairly recent technique in machine learning. We’re going to look at four methods, called “bagging”, “randomization”, “boosting”, and “stacking”. They’re all implemented in Weka, of course. The idea with bagging, we want to produce several different decision structures. Let’s say we use J48 to produce decision trees, then we want to produce slightly different decision trees. We can do that by having several different training sets of the same size. We can get those by sampling the original training set.
In fact, in bagging, you sample the set “with replacement”, which means that sometimes you might get two of the same [instances] chosen in your sample. We produce several different training sets, and then we build a model for each one – let’s say a decision tree – using the same machine learning scheme, or using some other machine learning scheme. Then we combine the predictions of the different models by voting, or if it’s a regression situation you would average the numeric result rather than voting on it. This is very suitable for learning schemes that are called “unstable”. Unstable learning schemes are ones where a small change in the training data can make a big change in the model.
Decision trees are a really good example of this. You can get a decision tree and just make a tiny little change in the training data and get a completely different kind of decision tree. Whereas with Naïve Bayes, if you think about how Naïve Bayes works, little changes in the training set aren’t going to make much difference to the result of Naïve Bayes, so that’s a “stable” machine learning method. In Weka we have a “Bagging” classifier in the meta set.
I’m going to choose meta > Bagging: here it is. We can choose here the bag size – this is saying a bag size of 100%, which is going to sample the training set to get another set the same size, but it’s going to sample “with replacement”. That means we’re going to get different sets of the same size each time we sample, but each set might contain repeats of the original training [instances]. Here we choose which classifier we want to bag, and we can choose the number of bagging iterations here, and a random-number seed. That’s the bagging method. The next one I want to talk about is “random forests”. Here, instead of randomizing the training data, we randomize the algorithm.
How you randomize the algorithm depends on what the algorithm is. Random forests are when you’re using decision tree algorithms. Remember when we talked about how J48 works? – it selects the best attribute for splitting on each time. You can randomize this procedure by not necessarily selecting the very best, but choosing a few of the best options, and randomly picking amongst them. That gives you different trees every time. Generally, if you bag decision trees, if you randomize them and bag the result, you get better performance. In Weka, we can look under “tree” classifiers for RandomForest.
Again, that’s got a bunch of parameters. The maximum depth of the trees produced – I think 0 would be unlimited depth. The number of features we’re going to use. We might select, say 4 features; we would select from the top 4 features – every time we decide on what decision to put in the tree, we select that from among the top 4 candidates. The number of trees we’re going to produce, and so on. That’s random forests.
Here’s another kind of algorithm: it’s called “boosting”.
It’s iterative: new models are influenced by the performance of previously built models. Basically, the idea is that you create a model, and then you look at the instances that are misclassified by that model. These are the hard instances to classify, the ones it gets wrong. You put extra weight on those instances to make a training set for producing the next model in the iteration. This encourages the new model to become an “expert” for instances that were misclassified by all the earlier models. The intuitive justification for this is that in a real life committee, committee members should complement each other’s expertise by focusing on different aspects of the problem.
In the end, to combine them we use voting, but we actually weight models according to their performance. There’s a very good scheme called AdaBoostM1, which is in Weka and is a standard and very good boosting implementation – it often produces excellent results. There are few parameters to this as well; in particular the number of iterations. The final ensemble learning method is called “stacking”. Here we’re going to have base learners, just like the learners we talked about previously. We’re going to combine them not with voting, but by using a meta-learner, another learner scheme that combines the output of the base learners. We’re going to call the base learners level-0 models, and the meta-learner is a level-1 model.
The predictions of the base learners are input to the meta-learner. Typically you use different machine learning schemes as the base learners to get different experts that are good at different things. You need to be a little bit careful in the way you generate data to train the level-1
model: this involves quite a lot of cross-validation, I won’t go into that. In Weka, there’s a meta classifier called “Stacking”, as well as “StackingC” – which is a more efficient version of Stacking. Here is Stacking; you can choose different meta-classifiers here, and the number of stacking folds. We can choose different classifiers; different level-0 classifiers, and a different meta-classifier. In order to create multiple level-0 models, you need to specify a meta-classifier as the level-0 model. It gets a little bit complicated; you need to fiddle around with Weka to get that working. That’s it then. We’ve been talking about combining multiple models into ensembles to produce an ensemble for learning, and the analogy is with committees of humans.
Diversity helps, especially when learners are unstable. And we can create diversity in different ways. In bagging, we create diversity by resampling the training set. In random forests, we create diversity by choosing alternative branches to put in our decision trees. In boosting, we create diversity by focusing on where the existing model makes errors; and in stacking, we combine results from a bunch of different kinds of learner using another learner, instead of just voting.
<End Transcript>

<-- 4.14 Quiz -->
Boosting
Question 1
For classification, what measure does DecisionStump use?
Information gain
Entropy
Gini impurity
---
Correct answer(s):
Entropy
---
Feedback correct:
The More button in DecisionStump’s configuration panel states that for classification, the algorithm is based on entropy.
---
Feedback incorrect:
J48 is an example of a classifier that uses Information gain, but DecisionStump doesn’t.
---
Feedback incorrect:
The Gini impurity measure is used by some machine learning algorithms, but not by DecisionStump.

<-- 4.14 Quiz -->
Boosting
Question 2
Now perform 10-fold cross-validation on the diabetes dataset with DecisionStump. How many leaves does the tree that is generated for the full dataset have?
1
2
3
4
---
Correct answer(s):
3
---
Feedback correct:
DecisionStump makes a binary split on one of the attributes. As well as two leaves for the split criterion, it makes a third for when the value of that attribute is missing. (Missing values will be discussed next week).

<-- 4.14 Quiz -->
Boosting
Question 3
In the previous experiment, which attribute was used to split the data on?
preg
plas
pres
skin
insu
mass
pedi
age
---
Correct answer(s):
plas
---
Feedback correct:
The attribute name used for splitting is shown at the start of the rules.

<-- 4.14 Quiz -->
Boosting
Question 4
Does DecisionStump perform better than ZeroR on this dataset?
yes
no
---
Correct answer(s):
---
Feedback correct:
Though considered a “weak learner” because it can only produce a tree with one level, DecisionStump (at 72%) outperforms the ZeroR baseline (65%).

<-- 4.14 Quiz -->
Boosting
Question 5
Now select AdaBoostM1, a boosting algorithm that by default utilizes DecisionStump as its base learner, and familiarize yourself with its options.
Apply AdaBoostM1 to the same dataset using various values for the numIterations option, and make a rough paper-and-pencil plot of the accuracies obtained (with 10-fold cross-validation). What does the graph look like?
(a)
(b)
(c)
(d)
---
Correct answer(s):
(a)
---
Feedback correct:
With boosting, the accuracy generally improves as the number of iterations increases, and then flattens out. For this dataset it improves only sightly, and only at the beginning of the graph.

<-- 4.14 Quiz -->
Boosting
Question 6
Which of the shapes would you expect if you boosted the ZeroR algorithm instead of DecisionStump?
(a)
(b)
(c)
(d)
---
Correct answer(s):
(d)
---
Feedback correct:
ZeroR will produce the same classifier for practically any subset of the data (provided only that statistical sampling leaves the majority class intact). Combining these identical classifiers would give the same result as ZeroR by itself. However, if you try this with Weka it will simply refuse to boost the ZeroR algorithm.

<-- 4.14 Quiz -->
Boosting
Question 7
Apply AdaBoostM1 to the breast-cancer.arff dataset for the default number of iterations (10), boosting OneR instead of DecisionStump. Ten models are produced, each testing a single attribute. Which attributes are used in at least one model?
Select all the answers you think are correct.
age
menopause
tumor-size
inv_nodes
node-caps
deg-malig
breast
breast-quad
irradiat
---
Correct answer(s):

<-- 4.14 Quiz -->
Boosting
Question 7
Apply AdaBoostM1 to the breast-cancer.arff dataset for the default number of iterations (10), boosting OneR instead of DecisionStump. Ten models are produced, each testing a single attribute. Which attributes are used in at least one model?
Select all the answers you think are correct.
age
menopause
tumor-size
inv_nodes
node-caps
deg-malig
breast
breast-quad
irradiat
---
Correct answer(s):
age
menopause
tumor-size
inv_nodes
deg-malig

<-- 4.15 Discussion -->
Reflect on this week's Big Question
The Big Question this week is, “What about real-life classification methods?”
We promised that by the end of the week you would be able to use modern machine learning methods, describe – at a high level – how they work, apply them to a dataset of your choice, and interpret the output that they produce. We are not aiming to convey the gory details, but rather the principles behind how they work.
So: can you?
This might be a bit of a challenge: choose one of this week’s methods and explain how it works to your partner, siblings, parents or kids. But try it! Teaching is the best form of learning. (I know.) Tell your fellow learners how you get on.

<-- 4.16 Article -->
Index
(A full index to the course appears at the end of Week 1.)
      TOPIC
      Step
      Datasets
      Breast-cancer
      4.9, 4.14
      Cpu
      4.4, 4.5
      Cpu.with.vendor
      4.5
      Credit-g
      4.12
      Diabetes
      4.6, 4.8, 4.14
      Glass
      4.9
      Ionosphere
      4.9
      Iris
      4.2, 4.3, 4.7, 4.11
      Labor
      4.9
      Classifiers
      DecisionStump
      4.14
      IBk
      4.2, 4.3, 4.12
      J48
      4.2, 4.8, 4.9, 4.12
      LinearRegression
      4.4, 4.5, 4.6, 4.7
      Logistic
      4.3, 4.8, 4.9, 4.11, 4.12
      M5P
      4.4, 4.5
      NaiveBayes
      4.2, 4.8, 4.9
      OneR
      4.2, 4.6, 4.9, 4.14
      SMO
      4.3, 4.11, 4.12
      ZeroR
      4.6, 4.8, 4.9, 4.14
      Metalearners
      AdaBoostM1
      4.13, 4.14
      Bagging
      4.13
      RandomForest
      4.3
      Stacking, StackingC
      4.13
      Filters
      AddClassification
      4.6
      AddID
      4.7
      MakeIndicator
      4.7
      NominalToBinary
      4.5, 4.6
      NumericToNominal
      4.6
      Packages
      LibSVM
      4.11
      Plus …
      Boundary visualizer
      4.11
      Contrabassoon
      4.11
      Output predictions
      4.7, 4.9
      Visualize classification boundaries
      4.2, 4.3

<-- 5.0 Todo -->
Putting it all together
What else is there to know?
This week's Big Question!
5.1
What else is there to know?
article
The data mining process
Producing classifiers is just a small part of the overall data mining process – perhaps the easiest part! Other parts involve formulating the question, gathering data, cleaning it, defining new features, and deploying the result.
5.2
The data mining process
video (06:22)
5.3
Outliers
quiz
Pitfalls and pratfalls
Be skeptical, and particularly wary of overfitting. Missing values can signify various things; classifiers treat them differently. There’s no single "best learner"; all methods have biases. Data mining is an experimental science!
5.4
Pitfalls and pratfalls
video (09:13)
5.5
Missing values
quiz
Data mining and ethics
It’s far harder to anonymize data than you think! The purpose of data mining is to discriminate, but some kinds of discrimination are unethical, and illegal. Data mining discovers correlations, but these do not imply causation.
5.6
Data mining and ethics
video (06:40)
5.7
Correlation, causation, and reidentification
quiz
There's no magic in data mining
There’s no magic in data mining! – in fact, perhaps Weka makes things too easy. You’ve learned lots, but don’t be smug: this course has missed out plenty. And you've learned a powerful technology: please use it wisely.
5.8
Summary
video (06:04)
5.9
What classifiers do
quiz
5.10
Reflect on this week's Big Question
discussion
Farewell
It's time to say goodbye. 
5.11
Post-course assessment
test
This is a test step, it helps you verify your understanding. If you want to earn a Certificate of Achievement on this course you need to complete this test and any others, scoring an average of 70% or above.
To take tests you need to upgrade this course.  It costs $94, which also gets you:
Unlimited access to the course for as long as it exists on FutureLearn, so you can learn at your own pace
A Certificate of Achievement when you’re eligible, to prove what you’ve learned
Upgrade
Find out more
5.12
Farewell
article
5.13
Index
article

<-- 5.1 Article -->
What else is there to know?
You’ve learned lots in this course about machine learning and its use in data mining. Most importantly, you’ve learned that there’s no magic in data mining, just a bunch of fairly simple techniques for analyzing data to produce models, and evaluating the performance of the predictions. You now know that interpreting the output, and understanding what is being done, is a key to successful application of this technology.
Producing classifiers is just a small part of the overall data mining process – perhaps the easiest part! What else needs doing? When interpreting the results you need to be skeptical – what can possibly go wrong? (lots!). When working with data you need to be sensitive – why? Data mining is essentially about discrimination, which is sometimes unethical, even illegal(!) – when? What techniques have you learned, and what have we missed?
At the end of the week you should be able to address these issues in an informed manner.

<-- 5.2 Video -->
The data mining process
If your vision of data mining is to get some data, apply Weka, get a cool result, and everyone’s happy – think again! Before you even begin to apply a classifier you’re going to have to ask the right question, find suitable data, clean it, devise new features … Weka is only part of the entire data mining process – the easiest part. Other aspects, including political problems in
getting hold of the data and deploying the result, are often more onerous in the overall data mining process.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
This might be your vision of the data mining process. You’ve got some data or someone gives you some data. You’ve got Weka. You apply Weka to the data, you get some kind of cool result from that, and everyone’s happy. If so, I’ve got bad news for you. It’s not going to be like that at all. Really, this would be a better way to think about it. You’re going to have a circle; you’re going to go round and round the circle. It’s true that Weka is important – it’s in the very middle of the circle here. It’s going to be crucial, but it’s only a small part of what you have to do.
Perhaps the biggest problem is going to be to ask the right kind of question. You need to be answering a question, not just vaguely exploring a collection of data. Then you need to get together the data that you can get hold of that gives you a chance of answering this question using data mining techniques. It’s hard to collect the data. You’re probably going to have an initial dataset, but you might need to add some demographic data, or some weather data, or some data about other stuff. You’re going to have to go to the web and find more information to augment your dataset.
Then you’ll merge all that together: do some database hacking to get a dataset that contains all the attributes that you think you might need – or that you think Weka might need. Then you’re going to have to clean the data. The bad news is that real world data is always very messy. That’s a long and painstaking process of looking around, looking at the data, trying to understand it, trying to figure out what the anomalies are and whether it’s good to delete them or not. That’s going to take a while. Then you’re going to need to define some new features, probably. This is the feature engineering process, and it’s the key to successful data mining.
Then, finally, you’re going to use Weka, of course. You might go around this circle a few times to get a nice algorithm for classification, and then you’re going to need to deploy the algorithm in the real world. Each of these processes is difficult. You need to think about the question that you want to answer. “Tell me something cool about this data” is not a good enough question. You need to know what you want to know from the data. Then you need to gather it. There’s a lot of data around, like I said at the very beginning, but the trouble is that we need classified data to use classification techniques in data mining.
We need expert judgements on the data, expert classifications, and there’s not so much data around that includes expert classifications, or correct results. They say that more data beats a clever algorithm. So rather than spending time trying to optimize the exact algorithm you’re going to use in Weka, you might be better off employed in getting more and more data. Then you’ve got to clean it, and like I said before, real data is very mucky. That’s going to be a painstaking matter of looking through it and looking for anomalies. Feature engineering, the next step, is the key to data mining. We’ll talk about how Weka can help you a little bit in a minute. Then you’ve got to deploy the result.
Implementing it – well, that’s the easy part. The difficult part is to convince your boss to use this result from this data mining process that he probably finds very mysterious and perhaps doesn’t trust very much. Getting anything actually deployed in the real world is a pretty tough call. The key technical part of all this is feature engineering, and Weka has a lot of [filters] that will help with this. Here are just a few of them. It might be worthwhile defining a new feature, a new attribute, that’s a mathematical expression involving existing attributes. Or you might want to modify an existing attribute. With AddExpression, you can use any kind of mathematical formula to create a new attribute from existing ones.
You might want to normalize or center your data, or standardize it statistically. Transform a numeric attribute to have a zero mean – that’s “center”. Or transform it into a given numeric range – that’s “normalize”. Or give it a zero mean and unit variance, that’s a statistical operation called “standardization”. You might want to take those numeric attributes and discretize them into nominal values. Weka has both supervised and unsupervised attribute discretization filters. There are a lot of other transformations. For example, the PrincipalComponents transformation involves a matrix analysis of the data to select the principal components in a linear space. That’s mathematical, and Weka contains a good implementation. RemoveUseless will remove attributes that don’t vary at all, or vary too much.
Then there are a couple of filters that help you deal with time series, when your instances represent a series over time. You probably want to take the difference between one instance and the next, or a difference with some kind of lag – one instance and the one 5 before it, or 10 before it. These are just a few of the filters that Weka contains to help you with your feature engineering. The message of this lesson is that Weka is only a small part of the entire data mining process, and it’s the easiest part. In this course, we’ve chosen to tell you about the easiest part of the process! I’m sorry about that.
The other bits are, in practice, much more difficult.
There’s an old programmer’s blessing: “May all your problems be technical ones”. It’s the other problems – the political problems in getting hold of the data, and deploying the result – those are the ones that tend to be much more onerous in the overall data mining process. So good luck!
<End Transcript>

<-- 5.3 Quiz -->
Outliers
Question 1
View the data in Weka’s Visualize panel. What does it look like?
(a)
(b)
(c)
---
Correct answer(s):
(c)
---
Feedback correct:
This data plots phone calls against year. In the 1960s it exhibits a large, surprising, sustained, spike.

<-- 5.3 Quiz -->
Outliers
Question 2
The class (phone calls) is numeric, so LinearRegression seems appropriate. Try it. What is the correlation coefficient, when evaluated with 10-fold cross-validation?
0.45
0.54
0.95
0.99
1.00
---
Correct answer(s):
0.45
---
Feedback incorrect:
That’s what you get when you evaluate it on the training set.

<-- 5.3 Quiz -->
Outliers
Question 3
In order to visualize the result, add the classifier’s predictions to the dataset using the supervised attribute filter AddClassification, and go to the Visualize panel. (As you will discover, this filter does nothing unless you turn on the outputClassification option.) What does the plot look like?
(a)
(b)
(c)
---
Correct answer(s):
(b)
---
Feedback correct:
Linear regression fits a straight line to the data, which in this case is increasing. But it’s not a very good fit. Imagine the regression line superimposed on the plot of phone calls against year. (Unfortunately you will have to imagine it, because Weka’s built-in visualization facilities do not allow two graphs to be plotted at once.)

<-- 5.3 Quiz -->
Outliers
Question 4
Run LinearRegression in the Classify panel. Make sure you are predicting the right thing (phone calls, not classification)!
What is the model that is produced?
---
Correct answer(s):
0.5041
26.0059

<-- 5.3 Quiz -->
Outliers
Question 5
First, use Weka’s Package Manager to install the leastMedSquared package.
Now change the classifier to LeastMedSq, another regression-style classifier. Run it in the Classify panel.
Which of these models does it produce (numbers have been rounded)?
0.5 * year – 26
0.1 * year – 5
0.05 * year + 0.1 * classification – 2
---
Correct answer(s):
0.1 * year – 5
---
Feedback correct:
This is a straight line with slope 0.1, which is much less than the 0.5 generated by LinearRegression (first answer above)
---
Feedback incorrect:
That’s the model produced by LinearRegression
---
Feedback incorrect:
classification is not an attribute of the original dataset! Remove it before applying LeastMedSq.

<-- 5.3 Quiz -->
Outliers
Question 6
Do the LinearRegression and LeastMedSq methods yield similar regression lines?
yes
no
---
Correct answer(s):
no
---
Feedback correct:
LeastMedSq gives a line with slope 0.11, whereas LinearRegression gives a slope of 0.5. To see the difference graphically, sketch them on a piece of paper, and imagine them superimposed on the plot of phone calls against year, as you did earlier for LinearRegression.
---
Feedback incorrect:
They should be very different. Be sure that you are using the correct attribute, phone calls, as the class.

<-- 5.3 Quiz -->
Outliers
Question 7
LeastMedSq gives an accurate regression line even when there are outliers. However, it is computationally very expensive. In practical situations it is common to delete outliers manually and then use linear regression. Identify the outliers using the Visualize panel and remove them manually (there are six obvious ones, plus two more that are not quite so striking). Apply LinearRegression again. What is the correlation coefficient, when evaluated with 10-fold cross-validation?
0.45
0.88
0.98
0.99
1.00
---
Correct answer(s):
0.99
---
Feedback correct:
The correlation of the predictions with the data has improved enormously, from 0.45 to 0.99 – near-perfect correlation.
---
Feedback incorrect:
This (0.4452) is the result you get for the full data set
---
Feedback incorrect:
This (0.8759) is the result when you delete the 6 most obvious outliers. Delete a further 2, one on either side of that range.
---
Feedback incorrect:
This (0.9803) is the result when you delete the 6 most obvious outliers plus one more. You need to identify and delete a further outlier.

<-- 5.3 Quiz -->
Outliers
Question 8
There’s something fishy about this data. Do an internet search for “International Phone Calls from Belgium 1950-1973” and find out what went wrong with the dataset. What is the reason for the outliers?
Human error
Different measurement used
Flat rate for calls introduced
---
Correct answer(s):

<-- 5.3 Quiz -->
Outliers
Question 8
There’s something fishy about this data. Do an internet search for “International Phone Calls from Belgium 1950-1973” and find out what went wrong with the dataset. What is the reason for the outliers?
Human error
Different measurement used
Flat rate for calls introduced
---
Correct answer(s):
Different measurement used
---
Feedback correct:
Instead of the number of phone calls, between 1964 and 1970 the total duration of the calls (in minutes) was recorded. Such errors are quite common in practice.

<-- 5.4 Video -->
Pitfalls and pratfalls
Be skeptical, and wary of overfitting. Always use fresh data for evaluation. Datasets often have missing values, which can mean different things – and different classifiers treat them in different ways. Finally, there’s no free lunch, no “best learner”. In order to generalize, any learner must embody some knowledge or assumptions beyond the data it’s given. All methods have biases. Data mining is an experimental science!
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! Welcome back for another few minutes in New Zealand. In the last lesson, we learned that Weka only helps you with a small part of the overall data mining process, the technical part, which is perhaps the easy part. In this lesson, we’re going to learn that there are many pitfalls and pratfalls even in that part. Let me just define these for you. A “pitfall” is a hidden or unsuspected danger or difficulty, and there are plenty of those in the field of machine learning. A “pratfall” is a stupid and humiliating action, which is very easy to do when you’re working with data. The first lesson is that you should be skeptical. In data mining it’s very easy to cheat.
Whether you’re cheating consciously or unconsciously, it’s easy to mislead yourself or mislead others about the significance of your results. For a reliable test, you should use a completely fresh sample of data that has never been seen before. You should save something for the very end, that you don’t use until you’ve selected your algorithm, decided how you’re going to apply it, and the filters, and so on. At the very, very end, having done all that, run it on some fresh data to get an estimate of how it will perform. Don’t be tempted to then change it to improve it so that you get better results on that data. Always do your final run on fresh data.
We’ve talked a lot about overfitting, and this is basically the same kind of problem. Of course, you know not to test on the training set. We’ve talked about that endlessly throughout this course. Data that’s been used for development in any way is tainted. Any time you use some data to help you make a choice of the filter, or the classifier, or how you’re going to treat your problem, then that data is tainted. You should be using completely fresh data to get evaluation results. Leave some evaluation data aside for the very end of the process. That’s the first piece of advice. Another thing I haven’t told you about in this course so far is missing values.
In real datasets, it’s very common that some of the data values are missing. They haven’t been recorded. They might be unknown; we might have forgotten to record them; they might be irrelevant. There are two basic strategies for dealing with missing values in a dataset. You can omit instances where the attribute value is missing, or somehow find a way of omitting that particular attribute in that instance. Or you can treat “missing” as a separate possible value. You need to ask yourself, is there significance in the fact that a value is missing? They say that if you’ve got something wrong with you and go to the doctor, and he does
some tests on you: if you just record the tests that he does – not the results of the test, but just the ones he chooses to do – there’s a very good chance that you can work out what’s wrong with you just from the existence of the tests, not from their results. That’s because the doctor chooses tests intelligently. The fact that he doesn’t choose a test doesn’t mean that that value is missing, or accidentally not there. There’s huge significance in the fact that he’s chosen not to do certain tests. This is a situation where “missing” should be treated as a separate possible value. There’s significance in the fact that a value is missing.
But in other situations, a value might be missing simply because a piece of equipment malfunctioned, or for some other reason – maybe someone forgot something. Then there’s no significance in the fact that it’s missing. Pretty well all machine learning algorithms deal with missing values. In an ARFF file, if you put a question mark as a data value, that’s treated as a missing value. All methods in Weka can deal with missing values. But they make different assumptions about them. If you don’t appreciate this, it’s easy to get misled. Let me just take two simple and well known (to us) examples – OneR and J48. They deal with missing values in different ways.
I’m going to load the nominal weather data and run OneR on it: I get 43%. Let me run J48 on it, to get 50%. I’m going to edit this dataset by changing the value of “outlook” for the first four “no” instances to “missing”. That’s how we do it here in this editor. If we were to write this file out in ARFF format, we’d find that these values are written into the file as question marks.
Now, if we look at “outlook”, you can see that it says here there are 4 missing values. If you count up these labels – 2, 4, and 4 – that’s 10 labels. Plus another 4 that are missing, to make the 14 instances. Let’s go back to J48 and run it again. We still get 50%, the same result. Of course, this is a tiny dataset, but the fact is that the results here are not affected by the fact that a few of the values are missing. However, if we run OneR, I get a much higher accuracy, 93% accuracy.
The rule that I’ve got is “branch on outlook”, which is what we had before I think.
Here it says there are 4 possibilities: if it’s sunny, it’s a yes; if it’s overcast it’s a yes; if it’s rainy, it’s a yes; and if it’s missing, it’s a no. Here, OneR is using the fact that a value is missing as significant, as something you can branch on. Whereas if you were to look at a J48 tree, it would never have a branch that corresponded to a missing value. It treats them differently. That’s very important to know and remember. The final thing I want to tell you about in this lesson is the “no free lunch” theorem. There’s no free lunch in data mining. Here’s a way to illustrate it. Suppose you’ve got a 2-class problem with 100 binary attributes.
Let’s say you’ve got a huge training set with a million instances and their classifications in the training set.
The number of possible instances is 2 to the 100 (2^100), because there are 100 binary attributes. And you know 10^6 of them. So you don’t know the classes of 2^100 – 10^6 examples. Let me tell you that 2^100 – 10^6 is 99.999...% of 2^100. There’s this huge number of examples that you just don’t know the classes of. How could you possibly figure them out? If you apply a data mining scheme to this, it will figure them out, but how could you possibly figure out all of those things just from the tiny amount of data that you’ve been given. In order to generalize, every learner must embody some knowledge or assumptions beyond the data it’s given.
Each learning algorithm implicitly provides a set of assumptions. The best way to think about those assumptions is to think back to the Boundary Visualizer You saw that different machine learning schemes are capable of drawing different kinds of boundaries in instance space. These boundaries correspond to a set of assumptions about the sort of decisions we can make. There’s no universal best algorithm; there’s no free lunch. There’s no single best algorithm. Data mining is an experimental science, and that’s why we’ve been teaching you how to experiment with data mining yourself. This is just a summary.
Be skeptical: when people tell you about data mining results and they say that it gets this kind of accuracy, then to be sure about that you want to have them test their classifier on your new, fresh data that they’ve never seen before. Overfitting has many faces. Different learning schemes make different assumptions about missing values, which can really change the results. There is no universal best learning algorithm. Data mining is an experimental science, and it’s very easy to be misled by people quoting the results of data mining experiments.
<End Transcript>

<-- 5.5 Quiz -->
Missing values
Question 1
Select the J48 classifier (default options). What is its accuracy, evaluated using 10-fold cross-validation?
26%
73%
74%
88%
---
Correct answer(s):
74%
---
Feedback correct:
Weka gives the result as 73.6842
---
Feedback incorrect:
That’s for incorrectly classified instances!

<-- 5.5 Quiz -->
Missing values
Question 2
Return to the Preprocess tab and remove all attributes with 33% or more missing values. How many attributes are left (excluding the class attribute)?
3
6
12
17
---
Correct answer(s):
6
---
Feedback correct:
When you select an attribute in the Preprocess panel, the number and percentage of missing values is shown. It’s easy to check the ones with 33% or more and remove them manually.
All 16 attributes (apart from the class) have missing values, and 10 of them have at least 33% of missing values.
---
Feedback incorrect:
When you select an attribute in the Preprocess panel, the number and percentage of missing values is shown. It’s easy to check the ones with 33% or more and remove them manually.

<-- 5.5 Quiz -->
Missing values
Question 3
Re-run J48 under the same conditions as before. What accuracy is achieved now?
73%
74%
81%
88%
---
Correct answer(s):
81%
---
Feedback correct:
Weka gives the result as 80.7018
---
Feedback incorrect:
That’s the result for the original 16-attribute dataset

<-- 5.5 Quiz -->
Missing values
Question 4
Now revert to the original dataset and apply the ReplaceMissingValues filter. This filter replaces missing values. What does it replace them with?
Blank for nominal attributes and zero for numeric attributes
Mean for numeric attributes and mode for nominal attributes
Random values for both nominal and numeric attrbutes
Mean for nominal attributes and mode for numeric attributes
---
Correct answer(s):
Mean for numeric attributes and mode for nominal attributes
---
Feedback correct:
Click the filter’s More button to get information about it.
The mean is the average value, and the mode is the most popular value.
---
Feedback incorrect:
Click the filter’s More button to get information about it.

<-- 5.5 Quiz -->
Missing values
Question 5
What is J48’s accuracy now?
73%
74%
81%
88%
---
Correct answer(s):
81%
---
Feedback correct:
The accuracy is 81% (80.7018%), which is the same as with the reduced attribute set. However, the confusion matrix is different. In general, it’s better to replace missing values rather than delete them entirely, since in many cases these attributes contribute some useful information.
---
Feedback incorrect:
That’s the result for the original 16-attribute dataset, with missing values

<-- 5.5 Quiz -->
Missing values
Question 6
Can you spot a methodological problem with this experiment?
No
Yes
---
Correct answer(s):

<-- 5.5 Quiz -->
Missing values
Question 6
Can you spot a methodological problem with this experiment?
No
Yes
---
Correct answer(s):
Yes
---
Feedback correct:
There’s a rather subtle (and almost certainly insignificant) problem. The ReplaceMissingValues filter calculates means and modes over the whole dataset. Thus for each fold of the cross-validation, some of the attribute values in the training set have been contaminated with information from the test set (although the effect is probably very small). This could produce results that are slightly different from those obtained from a completely independent test set in which missing values are replaced by means/modes from that test set.
---
Feedback incorrect:
This is a rhetorical question – the correct answer is obvious :-). But why? …

<-- 5.6 Video -->
Data mining and ethics
Data mining is a powerful technology, and I urge you to be ethical in its use. Data is sensitive stuff and should be treated with care. Personal data is particularly sensitive, and surprisingly difficult to anonymize: individuals can often be “re-identified” in apparently anonymized data. Note that the very purpose of data mining is discrimination, which frequently raises legal and ethical issues. Furthermore, data mining reveals correlation, which should not be confused with causation.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Before we start, I thought I’d show you where I live. I told you before that I moved to New Zealand many years ago. I live in a place called Hamilton. Let me just zoom in and see if we can find Hamilton in the North Island of New Zealand, around the center of the North Island.
This is where the University of Waikato is.
Here is the university; this is where I live.
This is my journey to work: I cycle every morning through the countryside. As you can see, it’s really nice. I live out here in the country. I’m a sheep farmer! I’ve got four sheep, three in the paddock and one in the freezer. I cycle in – it takes about half an hour – and I get to the university. I have the distinction of being able to go from one week to the next without ever seeing a traffic light, because I live out on the same edge of town as the university. When I get to the campus of the University of Waikato, it’s a very beautiful campus. We’ve got three lakes.
There are two of the lakes, and another lake down here. It’s a really nice place to work! So I’m very happy here. Let’s move on to talk about data mining and ethics. In Europe, they have a lot of pretty stringent laws about information privacy. For example, if you’re going to collect any personal information about anyone, a purpose must be stated. The information should not be disclosed to others without consent. Records kept on individuals must be accurate and up to date. People should be able to review data about themselves. Data should be deleted when it’s no longer needed. Personal information must not be transmitted to other locations. Some data is too sensitive to be collected, except in extreme circumstances.
This is true in some countries in Europe, particularly Scandinavia. It’s not true, of course, in the United States. Data mining is about collecting and utilizing recorded information, and it’s good to be aware of some of these ethical issues. People often try to anonymize data so that it’s safe to distribute for other people to work on, but anonymization is much harder than you think. Here’s a little story for you. When Massachusetts released medical records summarizing every state employee’s hospital record in the mid-1990’s, the Governor gave a public assurance that it had been anonymized by removing all identifying information – name, address, and social security number.
He was surprised to receive his own health records (which included a lot of private information) in the mail shortly afterwards! People could be re-identified from the information that was left there. There’s been quite a bit of research done on re-identification techniques. For example, using publicly available records on the internet, 50% of Americans can be identified from their city, birth date, and sex. 85% can be identified if you include their zip code as well. There was some interesting work done on a movie database. Netflix released a database of 100 million records of movie ratings. They got individuals to rate movies [on the scale] 1-5, and they had a whole bunch of people doing this – a total of 100 million records.
It turned out that you could identify 99% of people in the database if you knew their ratings for 6 movies and approximately when they saw them. Even if you only know their ratings for 2 movies, you can identify 70% of people. This means you can use the database to find out the other movies that these people watched. They might not want you to know that. Re-identification is remarkably powerful, and it is incredibly hard to anonymize data effectively in a way that doesn’t destroy the value of the entire dataset for data mining purposes.
Of course, the purpose of data mining is to discriminate: that’s what we’re trying to do! We’re trying to learn rules that discriminate one class from another in the data – who gets the loan? – who gets a special offer? But, of course, certain kinds of discrimination are unethical, not to mention illegal. For example, racial, sexual, and religious discrimination is certainly unethical, and in most places illegal. But it depends on the context. Sexual discrimination is usually illegal … except for doctors. Doctors are expected to take gender into account when they make their make their diagnoses. They don’t want to tell a man that he is pregnant, for example. Also, information that appears innocuous may not be.
For example, area codes – zip codes in the US – correlate strongly with race; membership of certain organizations correlates with gender. So although you might have removed the explicit racial and gender information from your database, it still might be able to be inferred from other information that’s there.
It’s very hard to deal with data: it has a way of revealing secrets about itself in unintended ways. Another ethical issue concerning data mining is that correlation does not imply causation.
Here’s a classic example: as ice cream sales increase, so does the rate of drownings. Therefore, ice cream consumption causes drowning? Probably not. They’re probably both caused by warmer temperatures – people going to beaches. What data mining reveals is simply correlations, not causation. Really, we want causation. We want to be able to predict the effects of our actions, but all we can look at using data mining techniques is correlation. To understand about causation, you need a deeper model of what’s going on. I just wanted to alert you to some of the issues, some of the ethical issues, in data
mining, before you go away and use what you’ve learned in this course on your own datasets: issues about the privacy of personal information; the fact that anonymization is harder than you think; re-identification of individuals from supposedly anonymized data is easier than you think; data mining and discrimination – it is, after all, about discrimination; and the fact that correlation does not imply causation.
<End Transcript>

<-- 5.7 Quiz -->
Correlation, causation, and reidentification
Question 1
Might this be true: A causes B?
True
False
---
Correct answer(s):
True
---
Feedback correct:
Yes. It might.
---
Feedback incorrect:
You cannot rule out the explanation that A causes B

<-- 5.7 Quiz -->
Correlation, causation, and reidentification
Question 2
Might this be true: B causes A?
True
False
---
Correct answer(s):
True
---
Feedback correct:
Yes. It might.
---
Feedback incorrect:
You cannot rule out the explanation that B causes A

<-- 5.7 Quiz -->
Correlation, causation, and reidentification
Question 3
Might this be true: Some third factor C is the cause of both A and B?
True
False
---
Correct answer(s):
True
---
Feedback correct:
Yes. It might.
---
Feedback incorrect:
You cannot rule out the explanation that some third factor is the cause of both A and B

<-- 5.7 Quiz -->
Correlation, causation, and reidentification
Question 4
Might this be true: The correlation between A and B is entirely coincidental?
True
False
---
Correct answer(s):
True
---
Feedback correct:
Yes. It might.
---
Feedback incorrect:
You cannot rule out the explanation that the correlation between A and B is entirely coincidental

<-- 5.7 Quiz -->
Correlation, causation, and reidentification
Question 5
In 2006, a text file was released on the web containing 20,000,000 search engine queries made by 650,000 users over a 3-month period, intended for research purposes. The file had been anonymized by replacing user names with random numbers, one per user. However, some of the queries contained clues to the user’s identity. The New York Times was able to locate an individual from these supposedly anonymized search records by cross referencing them with phonebook listings. Look up this classic example of reidentification and read about it.
What is the name of the person who was identified?
Dr. Abur Chowdhury
Thelma Arnold
User 927
---
Correct answer(s):
Thelma Arnold
---
Feedback correct:
You can read about this fiasco in many places, for example, the Wikipedia article entitled “AOL search data leak”, and the New York Times here.

<-- 5.7 Quiz -->
Correlation, causation, and reidentification
Question 5
In 2006, a text file was released on the web containing 20,000,000 search engine queries made by 650,000 users over a 3-month period, intended for research purposes. The file had been anonymized by replacing user names with random numbers, one per user. However, some of the queries contained clues to the user’s identity. The New York Times was able to locate an individual from these supposedly anonymized search records by cross referencing them with phonebook listings. Look up this classic example of reidentification and read about it.
What is the name of the person who was identified?
Dr. Abur Chowdhury
Thelma Arnold
User 927
---
Correct answer(s):

<-- 5.8 Video -->
Summary
There’s no magic in data mining! In fact, perhaps Weka makes things too easy. It is important to understand, and evaluate, what you’re doing, not just click around looking for good results. You’ve learned lots, but we’ve missed out plenty. Finally, I’d like to encourage you to be wise when using data mining technology. You’ve gained the power to analyze your own datasets. Use this
technology wisely, for the good of the world.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
One of the main points I’ve been trying to convey is that there’s no magic in data mining. There’s a huge array of alternative techniques, and they’re all fairly straightforward algorithms. We’ve seen the principles of many of them. Perhaps we don’t understand the details, but we’ve got the basic idea of the main methods of machine learning used in data mining. And there is no single, universal best method. Data mining is an experimental science. You need to find out what works best on your problem. Weka makes it easy for you. Using Weka you can try out different methods, you can try out different filters, different learning methods. You can play around with different datasets. It’s very easy to do experiments in Weka.
Perhaps you might say it’s too easy, because it’s important to understand what you’re doing, not just blindly click around and look at the results. That’s what I’ve tried to emphasize in this course – understanding and evaluating what you’re doing. There are many pitfalls you can fall into if you don’t really understand what’s going on behind the scenes. It’s not a matter of just blindly applying the tools in the workbench. We’ve stressed in the course the focus on evaluation, evaluating what you’re doing, and the significance of the results of the evaluation. Different algorithms differ in performance, as we’ve seen. In many problems, it’s not a big deal.
The differences between the algorithms are really not very important in many situations, and you should perhaps be spending more time on looking at the features and how the problem is described and the operational context that you’re working in, rather than stressing about getting the absolute best algorithm. It might not make all that much difference in practice. Use your time wisely. There’s a lot of stuff that we’ve missed out. I’m really sorry I haven’t been able to cover more of this stuff. There’s a whole technology of filtered classifiers, where you want to filter the training data, but not the test data.
That’s especially true when you’ve got a supervised filter, where the results of the filter depend on the class values of the training instances. You want to filter the training data, but not the test data, or maybe take a filter designed for the training data and apply the same filter to the test data without re-optimizing it for the test data, which would be cheating. You often want to do this during cross-validation. The trouble in Weka is that you can’t get hold of those cross-validation folds; it’s all done internally. Filtered classifiers are a simple way of dealing with this problem. We haven’t talked about costs of different decisions and different kinds of errors, but in real life different errors have different costs.
We’ve talked about optimizing the error rate, or the classification accuracy, but really, in most situations, we should be talking about costs, not raw accuracy figures, and these are different things. There’s a whole panel in the Weka Explorer for attribute selection, which helps you select a subset of attributes to use when learning, and in many situations it’s really valuable, before you do any learning, to select an appropriate small subset of attributes to use. There are a lot of clustering techniques in Weka.
Clustering is where you want to learn something even when there is no class value: you want to cluster the instances according to their attribute values. Association rules are another kind of learning technique where we’re looking for associations between attributes. There’s no particular class, but we’re looking for any strong associations between any of the attributes. Again, that’s another panel in the Explorer. Text classification. There are some fantastic text filters in Weka which allow you to handle textual data as words, or as characters, or n-grams (sequences of three, four, or five consecutive characters). You can do text mining using Weka. Finally, we’ve focused exclusively on the Weka Explorer, but the Weka Experimenter is also worth getting to know.
We’ve done a fair amount of rather boring, tedious, calculations of means and standard deviations manually by changing the random-number seed and running things again. That’s very tedious to do by hand. The Experimenter makes it very easy to do this automatically. So, there’s a lot more to learn. Let me just finish off here with a final thought. We’ve been talking about data, data mining. Data is recorded facts, a change of state in the world, perhaps.
That’s the input to our data mining process, and the output is information, the patterns – the expectations – that underlie that data: patterns that can be used for prediction in useful applications in the real world. We’ve going from data to information. Moving up in the world of people, not computers, “knowledge” is the accumulation of your entire set of expectations, all the information that you have and how it works together – a large store of expectations and the different situations where they apply. Finally, I’d like to define “wisdom” as the value attached to knowledge. I’d like to encourage you to be wise when using data mining technology. You’ve learned a lot in this course.
You’ve got a lot of power now that you can use to analyze your own datasets. Use this technology wisely for the good of the world. That’s my final thought for you.
<End Transcript>

<-- 5.9 Quiz -->
What classifiers do
Question 1
Works well and produces comprehensible models.
IBk
J48
LinearRegression
Logistic (Logistic regression)
NaiveBayes
OneR
SMO (Support vector machines)
ZeroR
---
Correct answer(s):
J48
---
Feedback correct:
The J48 classifier performs quite well, and provides the user with decision trees, which are easily understandable models.

<-- 5.9 Quiz -->
What classifiers do
Question 2
Works well when attributes contribute equally and independently to the decision.
IBk
J48
LinearRegression
Logistic (Logistic regression)
NaiveBayes
OneR
SMO (Support vector machine)
ZeroR
---
Correct answer(s):
NaiveBayes
---
Feedback correct:
The NaiveBayes classifier generally works well on datasets for which the independence assumption holds true.

<-- 5.9 Quiz -->
What classifiers do
Question 3
Simply stores the training data without processing it at all.
IBk
J48
LinearRegression
Logistic (Logistic regression)
NaiveBayes
OneR
SMO (Support vector machine)
ZeroR
---
Correct answer(s):
IBk
---
Feedback correct:
The nearest neighbor method IBk simply stores the training data. Although this overfits the training data, it can nevertheless work surprisingly well.

<-- 5.9 Quiz -->
What classifiers do
Question 4
Calculates a single linear decision boundary and outputs class probability estimates.
IBk
J48
LinearRegression
Logistic (Logistic regression)
NaiveBayes
OneR
SMO (Support vector machine)
ZeroR
---
Correct answer(s):
Logistic (Logistic regression)
---
Feedback correct:
The Logistic (Logistic regression) classifier calculates a linear decision boundary. (The boundaries obtained by IBk and SMO are piecewise linear.)

<-- 5.9 Quiz -->
What classifiers do
Question 5
Avoids overfitting, even with large numbers of attributes.
IBk
J48
LinearRegression
Logistic (Logistic regression)
NaiveBayes
OneR
SMO (Support vector machine)
ZeroR
---
Correct answer(s):
SMO (Support vector machine)
---
Feedback correct:
Through its use of a small number of support vectors, SMO (Support vector machine)  avoids overfitting even with large numbers of attributes.

<-- 5.9 Quiz -->
What classifiers do
Question 6
Determines the baseline performance for a dataset.
IBk
J48
LinearRegression
Logistic (Logistic regression)
NaiveBayes
OneR
SMO (Support vector machine)
ZeroR
---
Correct answer(s):

<-- 5.9 Quiz -->
What classifiers do
Question 6
Determines the baseline performance for a dataset.
IBk
J48
LinearRegression
Logistic (Logistic regression)
NaiveBayes
OneR
SMO (Support vector machine)
ZeroR
---
Correct answer(s):
ZeroR
---
Feedback correct:
By simply predicting the majority class, ZeroR provides a baseline performance measure for any dataset. OneR provides a slightly more sophisticated baseline.
---
Feedback correct:
By choosing to branch on a single attribute, OneR provides a baseline performance measure for any dataset. ZeroR provides a more rudimentary baseline.

<-- 5.10 Discussion -->
Reflect on this week's Big Question
The Big Question this week is, “What else is there to know?”
Ha! Lots!!!
Data mining is not just about producing classifiers that work well. That’s an important part of the process, of course. But there are larger issues, and we claimed that by the end of the week you would be able to address these in an informed manner.
This is what we said: You need to be able to interpret the results, and to be skeptical – what can possibly go wrong? (lots!). When working with data you need to be sensitive – why? Data mining is essentially about discrimination, which is sometimes unethical, even illegal(!) – when? And knowing things is good (of course), but it’s also good to have some idea what you don’t know: not just what techniques have you learned, but also what we have missed.
So: how did you get on? Can you talk knowledgeably about these things? Choose one (how about discrimination?) and discuss it with your partner, siblings, parents or kids. I’m hoping you’re the authority, you’re the expert. Are you?

<-- 5.11 Quiz -->
Post-course assessment
Question 1
Which two of these Weka filters turn numeric attributes into nominal ones?
Select all the answers you think are correct.
Discretize
NumericToNominal
NumericTransform
StringToNominal
Obfuscate
---
Correct answer(s):
Discretize
NumericToNominal

<-- 5.11 Quiz -->
Post-course assessment
Question 2
Use Weka’s Classify panel to determine who came up with the SMO algorithm for support vector machines, and what year their paper was published in.
Select all the answers you think are correct.
John Doe
John Plato
John Platt
1996
1998
2006
---
Correct answer(s):
John Platt
1998

<-- 5.11 Quiz -->
Post-course assessment
Question 3
What shape are the decision boundaries when AdaBoostM1 is used on the two-dimensional Iris data, iris.2D.arff?
Horizontal stripe pattern with crisp boundaries
Horizontal stripe pattern with fuzzy boundaries
Checkered pattern with crisp boundaries
Checkered pattern with fuzzy boundaries
Curved boundaries with gradient
Uneven, crisp boundaries
Uneven, fuzzy boundaries
---
Correct answer(s):
Checkered pattern with crisp boundaries

<-- 5.11 Quiz -->
Post-course assessment
Question 4
This question and the next two are about how accurately the weight of a slug can be predicted from its length. Download the slug.arff dataset, use linear regression to predict the weight from the length, and determine the correlation coefficient using 10-fold cross-validation. What is it?
0.88
0.89
0.90
0.91
0.92
---
Correct answer(s):

<-- 5.11 Quiz -->
Post-course assessment
Question 4
This question and the next two are about how accurately the weight of a slug can be predicted from its length. Download the slug.arff dataset, use linear regression to predict the weight from the length, and determine the correlation coefficient using 10-fold cross-validation. What is it?
0.88
0.89
0.90
0.91
0.92
---
Correct answer(s):
0.91

<-- 5.11 Quiz -->
Post-course assessment
Question 5
When visualizing the slug data in Weka, it seems from the graph of weight versus length that a logarithmic transform of one or both attributes may help the performance of linear regression. This can be done using Weka’s AddExpression filter. Figure out how to use this filter. What is the mean and standard deviation of the new attribute that is obtained by applying a logarithmic transform to the existing length attribute?
Select all the answers you think are correct.
Mean: 2.2
Mean: 2.3
Mean: 3.6
Mean: 42.1
StdDev: 0.62
StdDev: 3.02
StdDev: 4.54
StdDev: 24.80
---
Correct answer(s):
Mean: 3.6
StdDev: 0.62

<-- 5.11 Quiz -->
Post-course assessment
Question 6
Apply a logarithmic transform to both the length and weight attributes in the slug data. Use linear regression to predict log weight from log length, and determine the correlation coefficient using 10-fold cross-validation. What is it?
0.96
0.97
0.98
0.99
1
---
Correct answer(s):
0.97

<-- 5.11 Quiz -->
Post-course assessment
Question 7
Download the schizo.arff dataset and run ZeroR, OneR, J48 and SMO to predict the class (schizophrenic/non-schizophrenic), evaluating using 10-fold cross-validation. What order do the methods come in, from least accurate to most accurate?
---
Correct answer(s):

<-- 5.11 Quiz -->
Post-course assessment
Question 8
The predictions of OneR and J48 on the schizo dataset in the last question seem suspiciously good. In fact, they are anomalous because of the existence of one particular attribute. Identify this attribute, remove it, and test the four methods again (also using 10-fold cross-validation). What order do they come in now, from least accurate to most accurate?
---
Correct answer(s):

<-- 5.11 Quiz -->
Post-course assessment
Question 9
With the original schizo dataset (before the attribute was removed), two of the classifiers were overfitting badly. Which two?
Select all the answers you think are correct.
OneR
J48
SMO
ZeroR
---
Correct answer(s):
OneR
J48

<-- 5.11 Quiz -->
Post-course assessment
Question 10
Using 10-fold cross-validation, evaluate the percentage classification accuracy of IBk (k = 3) on the soybean dataset when the class value of all instances (both training and test instances) is corrupted by 25% noise using Weka’s AddNoise filter.
40%
56%
60%
63%
64%
---
Correct answer(s):
64%

<-- 5.11 Quiz -->
Post-course assessment
Question 11
Restore the original soybean.arff dataset and evaluate the performance of J48, selecting Percentage split as the test option with 90% as the parameter, using the random seed values 1, 2, 3, 4 and 5. What is the mean and standard deviation of the percentage classification accuracy?
Select all the answers you think are correct.
Mean: 89.7
Mean: 91.8
Mean: 94.1
StdDev: 2.0
StdDev: 2.1
StdDev: 2.2
---
Correct answer(s):
Mean: 91.8
StdDev: 2.2

<-- 5.11 Quiz -->
Post-course assessment
Question 12
Repeat the experiment in the previous question using 10-fold cross-validation as the test option and the same random seed values: 1, 2, 3, 4, and 5. What is the mean and standard deviation of the percentage classification accuracy now?
Select all the answers you think are correct.
Mean: 90.9
Mean: 91.3
Mean: 91.8
StdDev: 0.7
StdDev: 0.8
StdDev: 0.9
---
Correct answer(s):
Mean: 91.3
StdDev: 0.8

<-- 5.11 Quiz -->
Post-course assessment
Question 13
Reset the random seed value back to 1. Open the vote.arff dataset, run the four classifiers IBk, J48, Logistic, and SMO, and record the classification accuracy using 10-fold cross-validation. What order do the algorithms come in, from least accurate to most accurate?
IBk = Logistic < J48 < SMO
Logistic < IBk < J48 = SMO
J48 = IBk < Logistic < SMO
IBk < Logistic = SMO < J48
---
Correct answer(s):
IBk < Logistic = SMO < J48

<-- 5.11 Quiz -->
Post-course assessment
Question 14
Open the unbalanced.arff dataset, run the classifiers ZeroR, OneR, J48, and SMO, and record the classification accuracy using 10-fold cross-validation. Which of these is the most notable feature of the results?
They all take the same amount of time
They all have the same mean absolute error
They all have some unclassified instances
They all produce the same classification accuracy
---
Correct answer(s):
They all produce the same classification accuracy

<-- 5.11 Quiz -->
Post-course assessment
Question 15
The default configuration of AdaBoostM1 is 10 boosting iterations using the DecisionStump classifier. Performance might improve if 100 iterations were used instead. And it might improve if you kept to 10 iterations but used J48 instead of DecisionStump.
Using the ionosphere dataset and 10-fold cross-validation, what percentage classification accuracy measures result from AdaBoostM1 with:
a. 10 iterations using DecisionStump
b. 100 iterations using DecisionStump
c. 10 iterations using J48
Select all the answers you think are correct.
(a) 90.9
(a) 92.9
(b) 90.9
(b) 92.9
(b) 93.2
(c) 90.9
(c) 93.2
---
Correct answer(s):
(a) 90.9
(b) 92.9
(c) 93.2

<-- 5.12 Article -->
Farewell
Thanks for taking this course. We hope you’ve enjoyed it.
We’ve introduced you to practical data mining using the Weka workbench. We explained the basic principles of several popular algorithms and how to use them in practical applications. As we said at the beginning, the aim of the course is to dispel the mystery that surrounds data mining.
We claimed that after completing it you would be able to mine your own data – and understand what it is that you are doing!
What do you think? How did you get on? What did you learn? What did you struggle with? Have your views on data mining changed? What do you want to learn next? Please let us know by filling in the post-course survey. We welcome your suggestions on what could be improved!
This course will run again soon. Please tell anyone you think might benefit from it.
No matter where you go, there you are. (Sometimes attributed to Confucius)

<-- 5.13 Article -->
Index
(A full index to the course appears at the end of Week 1.)
      Topic
      Step
      Datasets
      Labor
      5.5
      Regression_outliers
      5.3
      Weather
      5.4
      Classifiers
      IBk
      5.9
      J48
      5.5, 5.9
      LeastMedSq
      5.3
      LinearRegression
      5.3
      Logistic
      5.9
      NaiveBayes
      5.9
      OneR
      5.4, 5.9
      SMO
      5.9
      ZeroR
      5.9
      Filters
      AddClassification
      5.3
      AddExpression
      5.2
      Discretize
      5.2
      Normalize
      5.2
      PrincipalComponents
      5.2
      RemoveUseless
      5.2
      ReplaceMissingValues
      5.5
      Standardize
      5.2
      Packages
      LeastMedSquared
      5.3
      Plus …
      Missing values
      5.4, 5.5
      Outliers
      5.3
      Reading CSV files
      5.3
      Reidentification
      5.6, 5.7
      Visualize panel
      5.3

