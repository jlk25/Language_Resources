<-- 1.0 Todo -->
Exploring Weka's interfaces, and working with big data
Hello again
This practical course on more advanced data mining follows on from Data Mining with Weka. You'll become an expert Weka user, and pick up many new techniques and principles of data mining along the way.
1.1
What will you learn?
video (03:05)
1.2
About this course
article
1.3
Welcome! Please introduce yourself
discussion
1.4
First, install Weka
article
1.5
Are you ready for this?
quiz
1.6
Well, are you ready for this?
discussion
What are Weka's other interfaces for?
Each week we’ll focus on a couple of “Big Questions” relating to data mining. This is the first Big Question for this week.
1.7
What are Weka's other interfaces for?
article
Exploring the Experimenter 
You can use the Experimenter to find the performance of classification algorithms on datasets, or to determine whether one classifier performs better (or runs faster) than another. In the Explorer, such things can be tedious.
1.8
Exploring the Experimenter
video (09:26)
1.9
Growing random numbers from seeds
article
1.10
Percentage split vs cross-validation estimates 
quiz
Comparing classifiers
The Experimenter can be used to compare classifiers. The "null hypothesis" is that they perform the same. To show that one is better than the other, we must *reject* this hypothesis at a given level of statistical significance. 
1.11
Comparing classifiers
video (07:00)
1.12
Comparing classifiers 
quiz
The Knowledge Flow interface 
The Knowledge Flow interface is an alternative to the Explorer. You can lay out filters, classifiers, evaluators on a 2D canvas ... and connect them up in different ways. Data and classification models flow through the diagram!
1.13
The Knowledge Flow interface 
video (06:59)
1.14
Looking inside cross-validation 
quiz
Using the Command Line
You can do everything the Explorer does (and more) from the command line.
One advantage is that you get more control over memory usage.
To access the definitive source of Weka documentation you need to learn to use JavaDoc.
1.15
Using the Command Line 
video (09:38)
1.16
Using Javadoc and the Simple CLI 
quiz
Can Weka process big data?
This week's second Big Question!
1.17
Can Weka process big data?
article
Working with big data 
The Explorer can handle pretty big datasets, but it has limits. However, the Command Line Interface does not: it works incrementally whenever it can. Some classifiers can handle arbitrarily large datasets.
1.18
Working with big data
video (09:48)
1.19
Prepare for the quiz
article
1.20
Experience big data
quiz
1.21
Experience the woes of big data
quiz
1.22
Reflect on your experience
article
1.23
Reflect on this week's Big Questions
discussion
1.24
Index
article

<-- 1.1 Video -->
What will you learn?
This video welcomes you to the course, summarizes what you will learn, and reviews what participants are assumed to know already. You will also learn that New Zealand is at the top of the world, and has a cool bird called a weka.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello, and welcome to More Data Mining with Weka. I’m Ian Witten, and I’m presenting the videos for this course, which is brought to you by the Computer Science Department at the University of Waikato in New Zealand. This course follows on from a previous course, Data Mining with Weka. It’s a practical course on how to use the advanced facilities of Weka for data mining. As in the previous course, we’re not going to cover programming, just the interactive interfaces to Weka. We’re going to pick up some basic principles of data mining along the way. We’re assuming that you know about a number of things that you will have learned in Data
Mining with Weka: what data mining is and why it’s useful, all the motivation, simplicity first, using the Explorer interface, popular classifier and filter algorithms, evaluating the result, interpreting the outputs, avoiding the pitfalls of training and testing sets, and the overall data mining process. We’re not going to cover any of that in this course. If you want a refresher, then you can go to YouTube and look at the WekaMOOC channel where you’ll see all the videos for the previous course. As you know, a “weka” is a bird found only in New Zealand, but from our point of view, it’s a data mining workbench – the Waikato Environment for Knowledge Analysis, which contains a lot of machine learning algorithms.
A very large number of algorithms for data mining tasks: preprocessing algorithms, feature selection, clustering, association rules – things like that. It’s a pretty comprehensive machine learning workbench. What you’re going to learn in this course is how to use the other interfaces to Weka. We already know how to use the Explorer, but we’re going to talk about the Experimenter, the Knowledge Flow Interface, and the Command Line interface. We’re going to talk about “big data” and how you deal with that in Weka. We’ll do some text mining. We’ll look at filtering using supervised and unsupervised filters. We’ll learn about discretization and sampling. We’ll learn about attribute selection. We’ll learn about classification rules, rules vs. trees, association rules, clustering, cost-sensitive evaluation and classification.
Most of all, I’m trying to get you to a point where you can use Weka on your own data, and – most importantly – understand what it is that you’re doing. Let me just finish off by saying this is where New Zealand is, at the top of the world. We think of you as being “down under”, not us as being “down under”. We’re in the top center of the world. Here in New Zealand – actually, I’ve turned this map around with North at the top, which is probably what you’re used to – you can see where the University of Waikato is, pointed to by the red arrow. That’s where I am.
<End Transcript>

<-- 1.2 Article -->
About this course
This course aims to extend your knowledge and experience of practical data mining, following on from Data Mining with Weka.
We’ll talk about “big data” and how to deal with that in Weka (you’ll process a dataset with 10 million instances). You’ll learn about mining text. You’ll look at filtering using supervised and unsupervised filters. You’ll learn about discretization and sampling. You’ll learn about attribute selection. You’ll learn about classification rules, rules vs. trees, association rules, clustering, cost-sensitive evaluation and classification. You already know how to use the Explorer, and we’re going to start by showing you Weka’s other interfaces.
Again, the aim of the course is to dispel the mystery that surrounds data mining. After completing it you will be even better equipped to mine your own data using more powerful methods. Most importantly, you’ll understand what it is that you’re doing!
Course structure
Teachers open the door. You enter by yourself. (Chinese proverb)
This is structured as a five week course:
  Week 1: Exploring Weka’s interfaces, and working with big data
  Week 2: Discretization and text classification
  Week 3: Classification rules, association rules, and clustering
  Week 4: Selecting attributes, and counting the cost
  Week 5: Neural networks, learning curves, and performance optimization
Each week focuses on a couple of “Big Questions.” For example, Week 1’s Big Questions are What are Weka’s other interfaces for? and Can Weka process big data?
The week covers a handful of activities that together address the questions. Each activity comprises:
  5-10 minute video
  Quiz. But no ordinary quiz! In order to answer the questions you have to undertake some practical data mining task. You don’t learn by watching someone talk; you learn by actually doing things! The quizzes give you an opportunity to do a lot of data mining.
I hear and I forget. I see and I remember. I do and I understand. (Confucius)
You will get additional benefits by purchasing an upgrade, including access to the tests:
  Mid-class test at the end of Week 2
  Post-class test at the end of Week 5
This week …
In Week 1 you will explore Weka’s other interfaces: the Experimenter, which allows you to run experiments that compare different methods; the Knowledge Flow interface, which lets you set up a graphical workflow for your data mining project; the Command Line interface, which accepts more complex commands to Weka. And you will learn about “big data” and how to deal with it in Weka. At the end of the week you’ll be equipped to use all Weka’s facilities, including stream-oriented processing for massive datasets.
Right now …
Please take the time to fill in the pre-course survey.
Teaching team
  Lead educator, Ian Witten
  Educator, Dave Nichols
  Educator, Jemma Konig
Production team
  Video editing, Louise Hutt
  Captions, Jennifer Whisler
  Music: 7 Mand på en Skude (7 men in a boat) by Rasmus Ørskov, performed by Rasmus Ørskov, Ashley Hopkins, Sarah Shieff and Ian Witten
Support
  Share what you are learning, including difficulties, problems and solutions, with others in the class in a weekly discussion focused on the Big Questions of the week and what you have learned
  Other discussions from time to time
  Transcripts are supplied for all videos
  Slides for all videos can be downloaded as a PDF file
Software requirements
If you have not already installed the Weka software you will need to do so right away (see step 1.4). It runs on any computer, under Windows, Linux, or Mac. It has been downloaded millions of times and is being used all around the world.
(Note: Depending on your computer and system version, you may need admin access to successfully install Weka.)
Prerequisite knowledge
You need no programming experience for this course. And no math, though some high-school statistical concepts are used (means and variances, maybe confidence intervals).
However, you do need to have completed the course Data Mining with Weka, or have equivalent knowledge. If you can do the Are you ready for this? quiz at the end of this Activity, you’ll be fine!

<-- 1.3 Discussion -->
Welcome! Please introduce yourself
Remember me? I’m Ian, in New Zealand. I hope you do remember me from the course Data Mining with Weka, because you should have completed that course (or have equivalent knowledge) before embarking on this one.
But just to remind you: formally I’m Professor Ian Witten from the University of Waikato in New Zealand, but everyone calls me Ian. I grew up in Northern Ireland, studied at Cambridge University, and worked at the Universities of Essex in England and Calgary in Canada before moving to paradise (aka New Zealand) 25 years ago. I became interested in data mining way back when, and also in open source software and open education. Data, they say, is the “new oil”: it affects all of us economically, socially, and politically.
I made this course because everyone needs to know about the data revolution (which far outshines the “computer revolution”) and what you can do with data – and what they can do with data. That’s why I’m so glad to have you here and learning!
Who are you? Where are you from? What do you do? Why are you here? What are you hoping to learn? What sort of data do have and why do you need to analyze it? Can you share an example?
See what other learners say and let them know if you share their interests (use ‘Reply’).
(We find the diversity of backgrounds, interests and locations of the people who join this course absolutely fascinating.)
You can make and reply to comments on almost every step of the course by clicking the pink plus icon. There will also be specific discussion steps from time to time (like this one and the regular ‘Weekly Reflections’ steps). Please join in!
There’s more information on FutureLearn discussions here.

<-- 1.4 Article -->
First, install Weka
Before beginning you should download and install Weka – if it isn’t already installed, which it will be if you have done the previous course Data Mining with Weka.
(Note: Depending on your computer and system version, you may need admin access to successfully install Weka.)
    Go to http://www.cs.waikato.ac.nz/ml/weka
    Click the Download button
    Now choose which one to download:
        the latest version, currently Weka-3-8-2
        (use the latest “stable” version; Weka-3-9 is a “developer” version)
        the appropriate link for your computer; Windows, Mac, or Linux
        if you don’t know what Java is, you probably want the file that includes a Java VM (= virtual machine)
Once it’s downloaded, Ian opens it to get a standard setup “wizard”.
    Just keep clicking “Next”! Install it in the default place – and remember the name of that place!
    After installation, uncheck the box that says “Start Weka” and click “Finish”.
    Then go to where Weka was installed and
        create a shortcut to the Weka program and put it on the desktop for future use.
        make a copy of the data folder (within the Weka folder) and put it in a convenient place for future use
        rename it “Weka datasets”

<-- 1.5 Quiz -->
Are you ready for this?
Question 1
Open the blood_fat_corrupted.arff dataset. (This is not in your Weka installation; you will have to download it. Make sure you save it with extension “.arff”.)
How many instances are there? How many attributes? What are the possible class values?
Select all the answers you think are correct.
26 instances
27 instances
2 attributes
3 attributes
class values low, medium, very_high
class values low, medium, high, very_high
class values weight, age, blood_fat
---
Correct answer(s):
27 instances
3 attributes
class values low, medium, high, very_high
---
Feedback incorrect:
You’ve got the number of instances wrong.
---
Feedback incorrect:
The class is included as one of the attributes.
---
Feedback incorrect:
Weight and age are attributes, not class values.

<-- 1.5 Quiz -->
Are you ready for this?
Question 1
Open the blood_fat_corrupted.arff dataset. (This is not in your Weka installation; you will have to download it. Make sure you save it with extension “.arff”.)
How many instances are there? How many attributes? What are the possible class values?
Select all the answers you think are correct.
26 instances
27 instances
2 attributes
3 attributes
class values low, medium, very_high
class values low, medium, high, very_high
class values weight, age, blood_fat
---
Correct answer(s):

<-- 1.5 Quiz -->
Are you ready for this?
Question 2
The blood_fat_corrupted dataset contains two corrupted values. What instances and attributes do they occur in?
Select all the answers you think are correct.
instance 5, attribute age
instance 5, attribute weight
instance 6, attribute age
instance 6, attribute weight
instance 7, attribute age
instance 7, attribute weight
instance 11, attribute age
instance 25 , attribute weight
---
Correct answer(s):
instance 5, attribute weight
instance 6, attribute age
---
Feedback correct:
You can view the data with the Edit button in the Preprocess panel.
The weight attribute for instance 5 has value –1. That’s impossible!
The age attribute for instance 6 has value 120. That’s highly unlikely!
---
Feedback incorrect:
You can view the data with the Edit button in the Preprocess panel.
The age attribute for instance 5 has value 25. Nothing wrong with that!
---
Feedback incorrect:
You can view the data with the Edit button in the Preprocess panel.
The weight attribute for instance 5 has value –1. That’s impossible!
---
Feedback incorrect:
You can view the data with the Edit button in the Preprocess panel.
The age attribute for instance 6 has value 120. That’s highly unlikely!
---
Feedback incorrect:
You can view the data with the Edit button in the Preprocess panel.
The weight attribute for instance 6 has value 60. Nothing wrong with that! (assuming Kg)
---
Feedback incorrect:
You can view the data with the Edit button in the Preprocess panel.
The age attribute for instance 7 has value 57. Nothing wrong with that!
---
Feedback incorrect:
You can view the data with the Edit button in the Preprocess panel.
The weight attribute for instance 7 has value 76. Nothing wrong with that! (assuming Kg)
---
Feedback incorrect:
You can view the data with the Edit button in the Preprocess panel.
The age attribute for instance 11 has value 57. Nothing wrong with that!
---
Feedback incorrect:
You can view the data with the Edit button in the Preprocess panel.
The weight attribute for instance 25 has value 85. Nothing wrong with that! (assuming Kg)

<-- 1.5 Quiz -->
Are you ready for this?
Question 3
Open the soybean.arff dataset. Use Weka to determine J48’s classification accuracy, evaluated using a 95% split into training and test instances. What is the result?
90.5%
91.5%
93.3%
96.3%
97.1%
---
Correct answer(s):
97.1%
---
Feedback incorrect:
Are you using Percentage split with the default value of 66%, instead of 95%?
---
Feedback incorrect:
Are you using Cross-validation instead of Percentage split?
---
Feedback incorrect:
Are you evaluating on the training set instead of using percentage split?

<-- 1.5 Quiz -->
Are you ready for this?
Question 4
What is J48’s classification accuracy on the soybean dataset, evaluated using 20-fold cross-validation with a random number seed of 42?
91.5%
92.2%
92.4%
92.9%
93.0%
---
Correct answer(s):
93.0%
---
Feedback incorrect:
Did you use 20-fold cross-validation with a random number seed of 42?
---
Feedback incorrect:
Did you use 10-fold cross-validation instead of 20-fold?
---
Feedback incorrect:
Did you leave the random number seed at the default value of 1?

<-- 1.5 Quiz -->
Are you ready for this?
Question 5
Which of the estimates in the last two questions would you expect to be more reliable?
Question 3
Question 4
---
Correct answer(s):
Question 4
---
Feedback correct:
With 20-fold cross-validation (Q.4), J48 is run 20 times with 20 different test sets, whereas with a 95% split (Q.3) it’s only run once, with 1 test set.

<-- 1.5 Quiz -->
Are you ready for this?
Question 6
Determine the performance of ZeroR, NaiveBayes, IBk, and OneR on the soybean dataset, evaluated using a 90% split into training and test instances (with the default random number seed). In terms of classification accuracy, which order do they come in?
---
Correct answer(s):
ZeroR
OneR
IBk
NaiveBayes
---
Feedback correct:
OK, I’ll tell you. But you need to be able to get these results yourself!
  ZeroR: 14.7059%
  NaiveBayes: 95.5882%
  IBk: 86.7647%
  OneR: 36.7647%

<-- 1.5 Quiz -->
Are you ready for this?
Question 7
Imagine a dataset with 150 instances and 3 class values. 25 of the instances belong to the first class, 50 to the second, and 75 to the third. What is the accuracy of ZeroR for this dataset, when evaluated on the training set?
25%
33%
50%
66%
75%
---
Correct answer(s):
50%
---
Feedback correct:
ZeroR assigns all instances to the majority class, that is, the third class (75 instances). Half the instances have a class that is different to this (25 in the first class and 50 in the second), so the accuracy is 50%.

<-- 1.5 Quiz -->
Are you ready for this?
Question 8
For the dataset in the previous question (150 instances of which 25, 50 and 75 belong to the three classes), would you expect exactly 50% accuracy if ZeroR were evaluated using (a) a 90% split into training and test instances, and (b) 10-fold cross-validation?
Select all the answers you think are correct.
(a) no
(a) yes
(a) perhaps
(b) no
(b) yes
(b) perhaps
---
Correct answer(s):
(a) no
(b) yes
---
Feedback correct:
(a) With a 90% split, the training set will have at least 68 instances of the third class (90% of the 75 instances in the original dataset). So ZeroR will assign all test instances to the third class.
However, it’s highly unlikely that a 90% split will produce a test set with exactly the same class distribution as in the entire set.
(b) With 10-fold cross-validation, in each fold the training set will have at least 68 instances of the third class (90% of the 75 instances in the original dataset). So ZeroR will assign all test instances to the third class.
In cross-validation, every instance in the dataset is used for testing exactly once. Since half the instances have a class that is different to the third class (25 in the first class and 50 in the second), the accuracy is again 50% – exactly 50%.
---
Feedback incorrect:
(a) With a 90% split, the training set will have at least 68 instances of the third class (90% of the 75 instances in the original dataset). So ZeroR will assign all test instances to the third class.
However, it’s highly unlikely that a 90% split will produce a test set with exactly the same class distribution as in the entire set.
(b) With 10-fold cross-validation, in each fold the training set will have at least 68 instances of the third class (90% of the 75 instances in the original dataset). So ZeroR will assign all test instances to the third class.
In cross-validation, every instance in the dataset is used for testing exactly once. Since half the instances have a class that is different to the third class (25 in the first class and 50 in the second), the accuracy is again 50% – exactly 50%.

<-- 1.5 Quiz -->
Are you ready for this?
Question 9
For the ionosphere.arff dataset, evaluate J48 five times using a 80% split into training and test instances, with the five random seeds 1, 4, 5, 6, and 8. What is the mean and standard deviation of the classification accuracy?
Select all the answers you think are correct.
mean is 86.0%
mean is 89.7%
standard deviation is 0.3%
standard deviation is 4.5%
standard deviation is 4.9%
standard deviation is 5.5%
---
Correct answer(s):
mean is 86.0%
standard deviation is 5.5%
---
Feedback correct:
The accuracies are 80%, 90%, 90%, 90%, and 80% for the five cases.
The variance is the sum of squared differences from the mean, divided by one less than the number of samples:
(0.06^2 + 0.04^2 + 0.04^2+ 0.04^2+ 0.06^2)/4 = 0.003.
The standard deviation is the square root of this, 0.05477.
---
Feedback incorrect:
The accuracies are 80%, 90%, 90%, 90%, and 80% for the five cases.
---
Feedback incorrect:
Did you remember to divide by 4 rather than 5 when computing the variance?
---
Feedback incorrect:
The variance is the sum of squared differences from the mean, divided by one less than the number of samples:
(0.06^2 + 0.04^2 + 0.04^2+ 0.04^2+ 0.06^2)/4 = 0.003.
The standard deviation is the square root of this, 0.05477.

<-- 1.5 Quiz -->
Are you ready for this?
Question 10
If J48 was evaluated on the ionosphere dataset using a single 5-fold cross-validation, would you expect the performance estimate to be more reliable or less reliable than in Question 9?
about the same
less reliable
more reliable provided the cross-validation is stratified
more reliable regardless of whether or not the cross-validation is stratified
---
Correct answer(s):
more reliable regardless of whether or not the cross-validation is stratified
---
Feedback correct:
Cross-validation (whether stratified or not) uses every instance in the dataset as a test instance exactly once, instead of leaving the test set up to chance each time.
---
Feedback incorrect:
You’re partially correct, but the cross-validation doesn’t have to be stratified.

<-- 1.5 Quiz -->
Are you ready for this?
Question 11
This question and the next two are about how accurately the weight of a slug can be predicted from its length. Download the slug.arff dataset, use linear regression to predict the weight from the length, and determine the correlation coefficient using 10-fold cross-validation. What is it?
0.88
0.89
0.90
0.91
0.92
---
Correct answer(s):
0.91

<-- 1.5 Quiz -->
Are you ready for this?
Question 12
When visualizing the slug data in Weka, it seems from the graph of weight versus length that a logarithmic transform of one or both attributes may help the performance of linear regression. This can be done using Weka’s AddExpression filter. Figure out how to use this filter. What is the mean and standard deviation of the new attribute that is obtained by applying a logarithmic transform to the existing length attribute?
Select all the answers you think are correct.
Mean: 2.2
Mean: 2.3
Mean: 3.6
Mean: 42.1
StdDev: 0.62
StdDev: 3.02
StdDev: 4.54
StdDev: 24.80
---
Correct answer(s):
Mean: 3.6
StdDev: 0.62
---
Feedback correct:
Use the More button for information on how to use AddExpression
---
Feedback incorrect:
Use the More button for information on how to use AddExpression

<-- 1.5 Quiz -->
Are you ready for this?
Question 13
Apply a logarithmic transform to both the length and weight attributes in the slug data. Use linear regression to predict log weight from log length, and determine the correlation coefficient using 10-fold cross-validation. What is it?
0.96
0.97
0.98
0.99
1
---
Correct answer(s):
0.97

<-- 1.6 Discussion -->
Well, are you ready for this?
How did you get on in the quiz? This is the place to discuss any problems with your fellow learners.
If you found it challenging, you might consider doing the previous course Data Mining with Weka first. Alternatively, if you just need a refresher, you could look at some of these videos.

<-- 1.7 Article -->
What are Weka's other interfaces for?
In the last course, Data Mining with Weka, you got to know the Weka Explorer. It’s great for exploring datasets and learning algorithms!
But sometimes you need to do other things. We’ll  look at the Experimenter and the Knowledge Flow interface. You’ll be wondering what kinds of experiments you might want to do, and what is a “knowledge flow” anyway? And we’ll look at the Command Line interface, which you’ll find a little … hmmm … geeky. Why would you want to do things at the command line, instead of through an interactive interface? Well, you’ll find out. And in this week’s second Big Question you’ll find out why you might have to work that way.
At the end of the week you’ll be able to use all these interfaces. More importantly, you’ll realize why they might be useful, even necessary. You’ll have a richer understanding of the spectrum of data mining activities.

<-- 1.8 Video -->
Exploring the Experimenter
Learn how to use Weka’s Experimenter interface.  It makes it easy to run different algorithms on different datasets and compare the results – whether one classifier is better than another on a particular dataset and whether the difference is statistically significant or not. The results of these tests can be output as an ARFF file – so you can do data mining on the results of data mining experiments!
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello! Welcome back to New Zealand for another few minutes of More Data Mining with Weka. By the way, I’d just like to thank all of those who did the first course for their nice comments and feedback. You know, the University of Waikato is just a little university on the far side of the world, but they listen. They listen when they hear feedback, and they’ve listened to you. As you can see, they’ve put me in a bigger office with more books and bigger plants. This has been great. They really appreciate the positive feedback that we’ve had from you for the previous course. Thank you very much indeed. Today we’re going to look at the Experimenter.
As you know, there are four [five] interfaces to Weka: the Explorer, which we looked at in the last course; the Experimenter and two [three] more. We’re going to look at the Experimenter today, and in the next lesson as well. It’s used for things like determining the mean and standard deviation performance of a classification algorithm on a dataset, which you did manually, actually, in the previous course. It’s easy to do several algorithms on several datasets, and you can find out whether one classifier is better than another on a particular dataset and whether the difference is statistically significant or not.
You can check the effect of different parameter settings for an algorithm, and you can actually express the results of these tests as an ARFF file. So you can sort of do data mining on the results of data mining experiments, if you’d like. In the Experimenter, sometimes the computation takes days or even weeks, and it can be distributed over several computers, like all of the computers in a lab.
When you invoke the Experimenter, you get three panels: the Setup panel, the Run panel, and the Analyse panel. Before we go to those, let me just refresh your memory. This is a slide from Data Mining with Weka, where we talked about the training set and the test set. A basic assumption of machine learning is that these are independent sets produced by independent sampling from an infinite population. We took a dataset, segment-challenge, and a learning algorithm J48, and we used a percentage split method of evaluation. We evaluated it and got a certain figure for the accuracy. Then we repeated that with different random number seeds, and in fact we got ten different figures for the accuracy.
From those we manually computed the sample mean and the variance, and hence the standard deviation. Also, while we’re at it, let me just remind you about cross-validation. In Data Mining with Weka, we looked at this technique of 10-fold cross-validation, which involved dividing the dataset into ten parts, holding out each part in turn, and averaging the results of the ten runs. Let’s get into the Experimenter. If I just go here and click Experimenter, I get the Setup panel. I’m going to start a new experiment. I’m just going to note that we’ve got 10-fold cross-validation by default, and we’re repeating the experiment ten times by default. I’m going to add a dataset. I’m going to add the segment-challenge dataset, which is here.
I’m going to add a machine learning algorithm. I’m going to use J48. You’ve seen this kind of menu before many, many times; it’s the same as in the Explorer. If I just select J48 and click OK, then I’ve got this dataset and this learning algorithm. Well, let’s just run it. So off I go to the Run panel and click Start. It’s running. You can see at the bottom here, it’s doing the 5th, 6th, 7th, 8th, 9th, 10th run, because we repeated the whole thing ten times. We repeated 10-fold cross-validation ten times. Now, if I go to the Analyse panel, it doesn’t show anything. I need to analyze the results of the experiment I just did. Click Experiment.
And I need to perform the test. You can see here that it’s showing for a dataset called “segment” that we’ve got an average of 95.71% correct using this J48 algorithm. We wanted to look at the standard deviation. If I click Show std. deviations, and perform the test again, then I get the standard deviation. We’ve effectively done what we did rather more laboriously in the first course by doing ten individual runs. Over on the slide here, this summarizes what we’ve done. In the Setup panel, we set things up. In the Run panel, we just clicked Start; and in the Analyse panel, we clicked Experiment, and we selected Show std. deviations and performed the test.
Now, what about those detailed results of the individual runs? I’m going to go back to the Setup panel here. I’m going to write the results to a CSV file. I think I’ll just do a percentage split. I’ll do 90% training, 10% test. I’ve got my dataset and my machine learning method, so I’ll just go and run.
If I look at the CSV file that’s been produced, well, here it is. We repeated the experiment 10 times. These are the 10 different runs. For each of these 10 runs, we’ve got a lot of information. A lot of information. The information that we’re really looking for here is Percent_correct. That’s the percent correct for each of those 10 separate runs. We’ve got all sorts of other stuff here, including, for example, the user time, the elapsed time, and lots and lots of other things. Maybe you should take a look at those yourself. That’s given us the detailed results for each of the 10 runs. I’m going to do 10-fold cross-validation now.
These are the 10 repetitions, right, and we did single percentage split. If I do 10-fold cross-validation, and write the result into a file, and run it again. It takes a little bit longer, because it’s doing cross-validation each time. Now it’s finished, and if we look at the resulting file, we get something that’s very similar but much bigger. We repeated the whole thing 10 times. We repeated 10-fold cross-validation 10 times. This is the first run, and there were 10 folds. So there are 10 folds of the first run. Here are the 10 folds of the second run, and so on. I’ve got the same results as I had before along here.
So I’ve got a very detailed account of what was done in that experiment. Just coming back to the slides here. To get detailed results, we went back to the Setup panel, selected CSV file, and put in a file name for the results. This is the file that we got with percentage split. Then we did the same thing for the cross-validation experiment, and we got a larger results spreadsheet. Let’s just review the Experimenter. We’ve got three panels. In the Setup panel, you can open an experiment, and you can save an experiment, but what we usually do is start a new experiment. We normally start by clicking here. There’s an Advanced mode.
We’re not going to talk about the Advanced mode here; we’re going to continue to use the simple mode of the Experimenter. You can set a file name for the results if you want, either an ARFF file or a CSV file or, in fact, a database file. You can do either a cross-validation or a percentage split. Actually, you can preserve the order in percentage split. The reason for that is that there’s no way of specifying a separate test file in the Experimenter. To do that, you would kind of glue the training set and test set together and preserve the order and specify the appropriate percentage so that those last instances were used as the test set.
Normally, if we’re not doing that, we just randomize things for the percentage split. We’ve got the number of repetitions. We repeated the whole thing 10 times, but we could have repeated it 100 times. Here we can add new datasets. We can add more datasets. We can delete datasets that we’ve added, delete this dataset. Here we add more learning algorithms. We can just add new learning algorithms into the learning algorithms list. That’s the Setup panel. Then there’s the Run panel. You don’t do much in the Run panel except click Start, and just monitor for errors here. There were zero errors in the 3 runs I did.
Then, in the Analyse panel, you can load results from a file or a database, but what we normally want to do is click Experiment here to get the results from the experiment we’ve just done. There are many options, and we’re going to be looking at some of these options as we go through this course. That’s the Experimenter.
<End Transcript>

<-- 1.9 Article -->
Growing random numbers from seeds
In the preceding video I talk about changing the random number seed in the Weka Explorer and getting a different result. (This was also done in the previous course, Data Mining with Weka.) In case you are mystified, an explanation follows.
Here’s the issue. Many data mining processes depend on some random process – like randomly splitting a dataset into training and test sets. This creates a conflict between getting repeatable results and realistic results. Realistically, the results should be slightly different each time, depending on the exact split. But in practice that would be a nightmare: you want to be able to repeat experiments and get the same results.
Here’s Weka’s solution. It uses a random number generator (a simple little program), but it generates the same sequence of numbers each time, so that you can do the same thing tomorrow with the same result. The sequence is controlled by number called a “seed”. You change it in the Explorer’s Classify panel, under More options. The default value is 1, but you get a different sequence of random numbers by changing the seed to something else – like 2, or 3, or 42, or anything.
In the Experimenter you don’t need to worry about changing the seed, because the random number generator is only initialized at the very beginning of an experiment, when you run it. Thus if the experiment involves several evaluations – say several cross-validations – they each take place with the random number generator in a different state, so they come up with different results. Yet the whole experiment is completely repeatable: if you run it again, the random number generator will be re-initialized.

<-- 1.10 Quiz -->
Percentage split vs cross-validation estimates 
Question 1
Given these results:
which do you think is a better estimate of the mean?
95.71%
95.22%
Don’t know
---
Correct answer(s):
Don’t know
---
Feedback correct:
There’s not enough evidence here to prefer one over the other because the standard deviations far outweigh the difference between the estimates.

<-- 1.10 Quiz -->
Percentage split vs cross-validation estimates 
Question 2
It’s curious that the standard deviation estimate for percentage split is smaller than that for cross-validation:
Re-run the experiment and determine the standard deviation estimate for a 95% split, repeated 10 times.
1.05
1.85
2.17
NaN
---
Correct answer(s):
2.17
---
Feedback incorrect:
Are you using a 66% split instead of a 95% split?
---
Feedback incorrect:
Are you using cross-validation instead of percentage split?
---
Feedback incorrect:
NaN is short for “Not a number”, and a common way of getting it is to accidentally divide by zero. If you set the Experiment Type to Train/Test Percentage Split (order preserved), the Experimenter runs the experiment just once instead of the usual 10 times—there’s no point in running it again because the result will be the same each time. You can’t compute the standard deviation of a single number; what happens mathematically is an attempt to divide by zero.

<-- 1.10 Quiz -->
Percentage split vs cross-validation estimates 
Question 3
Re-run the experiment with 20-fold cross-validation, repeated 10 times. What is the standard deviation estimate?
0.59
1.85
1.94
2.44
---
Correct answer(s):
2.44

<-- 1.10 Quiz -->
Percentage split vs cross-validation estimates 
Question 4
Re-run the experiment with an 80% split, repeated 10 times. What is the standard deviation estimate?
1.32
2.22
3.34
4.36
---
Correct answer(s):
1.32

<-- 1.10 Quiz -->
Percentage split vs cross-validation estimates 
Question 5
Re-run the experiment with 5-fold cross-validation, repeated 10 times. What is the standard deviation estimate?
1.32
1.39
2.22
3.35
---
Correct answer(s):
1.39
---
Feedback incorrect:
Are you using percentage split instead of cross-validation?

<-- 1.10 Quiz -->
Percentage split vs cross-validation estimates 
Question 6
Can you think of reasons why the standard deviation estimate tends to be smaller for percentage split than for the corresponding cross-validation?
Select all the answers you think are correct.
There’s no reason; it’s just a coincidence.
The estimate is made using a different number of samples in each case
There’s some overlap in the test sets for percentage split, but none for cross-validation.
---
Correct answer(s):

<-- 1.10 Quiz -->
Percentage split vs cross-validation estimates 
Question 6
Can you think of reasons why the standard deviation estimate tends to be smaller for percentage split than for the corresponding cross-validation?
Select all the answers you think are correct.
There’s no reason; it’s just a coincidence.
The estimate is made using a different number of samples in each case
There’s some overlap in the test sets for percentage split, but none for cross-validation.
---
Correct answer(s):
The estimate is made using a different number of samples in each case
There’s some overlap in the test sets for percentage split, but none for cross-validation.
---
Feedback correct:
One reason is that the estimate is made using a different number of samples in each case. Ten times 10-fold cross-validation generates 100 samples, a 90% split repeated ten times generates only 10 samples.
One reason is that there’s some overlap in the test sets for percentage split, but none for cross-validation. When you repeat percentage split, each time using a random sample as the test set, there is a good chance of overlap between the different test sets. With cross-validation, overlap doesn’t occur.
---
Feedback incorrect:
It’s not just a coincidence …
---
Feedback incorrect:
One reason is that the estimate is made using a different number of samples in each case. Ten times 10-fold cross-validation generates 100 samples, a 90% split repeated ten times generates only 10 samples.
---
Feedback incorrect:
One reason is that there’s some overlap in the test sets for percentage split, but none for cross-validation. When you repeat percentage split, each time using a random sample as the test set, there is a good chance of overlap between the different test sets. With cross-validation, overlap doesn’t occur.

<-- 1.11 Video -->
Comparing classifiers
How can you reliably compare two classifiers? Experimental results always depend on the random number sequence – might the conclusion be different if you used a different random number seed? 
Statisticians talk about the “null hypothesis”, which is that one classifier’s performance is the same as the other’s. We’re usually hoping that the results of an experiment reject the null hypothesis! This involves a certain level of statistical significance: we might reject the hypothesis at the 5% level of statistical significance, meaning that it’s highly unlikely (1 chance in 20)  that their performance is the same.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello, and welcome back! We’re going to be using the Experimenter quite a lot in this course. I’m going to show you how to use it to compare classifiers.
Here’s a question: Is J48 better than ZeroR or OneR on the Iris dataset? Of course, we could fire up the Explorer. You know how to do this, so I’m not going to do it for you. We can open the dataset, we can get the results for these three different machine learning methods, and we can see that J48 with 96% cross-validation accuracy is better than OneR, which is better than ZeroR. But the question is, how reliable is this comparison? Things could change if we happened to choose a different random number seed. The Experimenter helps produce more reliable comparisons between datasets and classification algorithms. I’m going to fire up the Experimenter.
I’m going to open the Iris dataset, and use the same 3 classification algorithms, and compare them. Here we are in the Experimenter. I’m going to create a new experiment. I’m going to open a dataset. I’m going to add 3 classification algorithms.
I can reorder these algorithms, by the way. If I select one and go up, and select another one and go down, I can reorder them. I’m going to go to Run and run this. Then I’m going to go to the Analyse panel and click Experiment – that’s important – and then click Perform test. Back to the slides here – that’s what I did. I switched to the Analyse panel and clicked these things and got these results, which look like this, actually. Now, we can see the 3 figures for the 3 classification algorithms on the Iris dataset. We can see that both OneR and ZeroR are worse than J48, just looking at the numbers.
The star (*) means that ZeroR is significantly worse than J48. The absence of the star on OneR means that we cannot be sure that OneR is significantly worse than J48 at the 5% level of statistical significance. In other words, J48 seems better than ZeroR, and we’re pretty sure (5% level) that this is not due to chance. It seems to be better than OneR, but this may be due to chance, we can’t rule it out at the 5% level of statistical significance. Now, I could add a bunch more datasets. In fact, I’ll just go and do that.
I’ll rerun the experiment.
It’ll take a little bit of time.
Then I’ll analyze the results.
Over here on the slide, these are the results I get. So I can see that at the 5% level of significance J48 is significantly better than both OneR and ZeroR on 3 of the datasets. That’s looking at the stars; the stars mean that those methods are significantly worse than J48. In other words J48 is significantly better than them. It’s significantly better than OneR on breast-cancer and german_credit, and it’s significantly better than ZeroR on the iris and pima_diabetes datasets. So you can see from the table of figures and the stars where the significant results are. Now, what if we wanted to know whether OneR was significantly better than ZeroR?
This does not tell us on this slide, because on this slide, we’re comparing everything to J48.
If we go back to the Experimenter and select something different for the test base: I’m selecting OneR for the test base and performing the test. Now, I’ve got OneR in the first column, and things are being compared with it. Going back to the slide, having changed the test base, I can see that OneR is significantly worse than ZeroR on the german_credit dataset, about the same on the breast-cancer dataset, and significantly better on all the rest of the datasets. Another thing that we can do is change the order of the columns in this matrix. If I go back to the Experimenter and select for the row – currently the Dataset is selected – I’m going to select Scheme for the row.
For the column, currently Scheme is selected, and I’m going to select Dataset for the column. Then perform the test again. Now we get the datasets going along horizontally here, this is the list of datasets; and we get the algorithms going vertically. So I can see whether J48 performs significantly better or worse on the iris dataset than it does say on the breast-cancer dataset. What we’ve looked at is comparing classifiers. In statistical terms, people talk about the “null hypothesis”. That is, that one classifier’s performance is the same as another. The result that we observe is highly unlikely if the null hypothesis is true. That is, we reject the null hypothesis.
We reject the hypothesis that they’re the same at the 5% level of statistical significance. So the Experimenter tells you when the null hypothesis is being rejected. Or, equivalently, we can say that A performs significantly better than B at the 5% level. In the Experimenter, we can change the significance level. It’s common to use 5%; 1% for critical applications, maybe medical applications; perhaps 10% for less critical applications. We can change the comparison field. We have used percent correct, but we can change that in the Explorer, and it’s common to compare over a set of datasets.
We might say on these datasets, method A has so many wins and so many losses over method B, referring to the number of statistically significant times A is better than B or B is better than A. There’s problem you ought to be aware of – the multiple comparison problem. If you make a large number of tests, some of them will appear to be significant just by chance. As usual, this is not an exact science. The interpretation of results requires a certain amount of care.
<End Transcript>

<-- 1.12 Quiz -->
Comparing classifiers 
Question 1
For one (and only one) of the datasets, some schemes significantly outperform J48 (at the 5% level). Which dataset?
iris
breast-cancer
credit-g
diabetes
glass
ionosphere
---
Correct answer(s):
credit-g
---
Feedback correct:
NaiveBayes, Logistic, and SMO significantly outperform J48 on the german_credit dataset – and no method outperforms it significantly on any other dataset. (Look for the “v”s, not the asterisks.)

<-- 1.12 Quiz -->
Comparing classifiers 
Question 2
One of the classifiers is not significantly different from J48 on any dataset. Which classifier?
NaiveBayes
IBk
Logistic
SMO
AdaBoostM1
---
Correct answer(s):
IBk

<-- 1.12 Quiz -->
Comparing classifiers 
Question 3
One of the classifiers is significantly better than J48 on exactly one dataset, and significantly worse than J48 on exactly one other dataset. Which classifier?
NaiveBayes
IBk
Logistic
SMO
AdaBoostM1
---
Correct answer(s):
Logistic
---
Feedback correct:
Logistic outperforms J48 significantly on the german_credit dataset, and underperforms it significantly on the breast-cancer dataset.

<-- 1.12 Quiz -->
Comparing classifiers 
Question 4
Change the analysis parameters to compare the various classifiers with OneR instead of with J48. (You can do this on the “Analyse” panel without re-running the experiment.)
There’s one dataset on which all classifiers significantly outperform OneR. Which dataset?
iris
breast-cancer
credit-g
diabetes
glass
ionosphere
---
Correct answer(s):
credit-g
---
Feedback correct:
Even ZeroR outperforms OneR significantly on this dataset!

<-- 1.12 Quiz -->
Comparing classifiers 
Question 5
Which classifier significantly outperforms OneR on the largest number of datasets?
NaiveBayes
IBk
Logistic
SMO
AdaBoostM1
---
Correct answer(s):
AdaBoostM1
---
Feedback correct:
AdaBoostM1 outperforms OneR significantly on 5 datasets. However, on the remaining dataset (Glass), AdaBoostM1 significantly underperforms OneR!

<-- 1.12 Quiz -->
Comparing classifiers 
Question 6
Change the analysis parameters to compare the various classifiers with SMO instead of with OneR.
Which classifier significantly outperforms SMO on the largest number of datasets?
NaiveBayes
J48
IBk
Logistic
AdaBoostM1
---
Correct answer(s):
J48
---
Feedback correct:
J48 significantly outperforms SMO on two datasets, breast-cancer and Glass.

<-- 1.12 Quiz -->
Comparing classifiers 
Question 7
Which other classifier significantly outperforms SMO on at least one dataset (as well as J48)?
NaiveBayes
IBk
Logistic
SMO
AdaBoostM1
---
Correct answer(s):
IBk
---
Feedback correct:
IBk significantly outperforms SMO on the Glass dataset.

<-- 1.12 Quiz -->
Comparing classifiers 
Question 8
Ignoring whether or not the differences are statistically significant, which classifier performs best of all on each of the following datasets (Q. 8, 9, 10, 11, 12, 13):
Best performing classifier for iris:
J48
IBk
NaiveBayes
Logistic
SMO
AdaBoostM1
---
Correct answer(s):
Logistic

<-- 1.12 Quiz -->
Comparing classifiers 
Question 9
Best performing classifier for breast-cancer:
J48
IBk
NaiveBayes
Logistic
SMO
AdaBoostM1
---
Correct answer(s):
J48

<-- 1.12 Quiz -->
Comparing classifiers 
Question 10
Best performing classifier for credit-g:
J48
IBk
NaiveBayes
Logistic
SMO
AdaBoostM1
---
Correct answer(s):
Logistic

<-- 1.12 Quiz -->
Comparing classifiers 
Question 11
Best performing classifier for diabetes:
J48
IBk
NaiveBayes
Logistic
SMO
AdaBoostM1
---
Correct answer(s):
Logistic

<-- 1.12 Quiz -->
Comparing classifiers 
Question 12
Best performing classifier for glass:
J48
IBk
NaiveBayes
Logistic
SMO
AdaBoostM1
---
Correct answer(s):
IBk

<-- 1.12 Quiz -->
Comparing classifiers 
Question 13
Best performing classifier for ionosphere:
J48
IBk
NaiveBayes
Logistic
SMO
AdaBoostM1
---
Correct answer(s):
AdaBoostM1

<-- 1.13 Video -->
The Knowledge Flow interface 
The Knowledge Flow interface is an alternative to the Explorer.  You lay out filters,
classifiers, evaluators, and visualizers interactively on a 2D canvas and connect them together with different kinds of connector. Data and classification models flow through the diagram!
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello again! We’re going to look at the Knowledge Flow Interface. The Knowledge Flow Interface is an alternative to the Explorer, and it lets you lay out filters, classifiers, and evaluators interactively on a 2D canvas. There are various other components like data sources, and visualization components, and so on. We have different kinds of connections between the components, and a feature of the Knowledge Flow Interface is that it can work incrementally on potentially infinite data streams. Let’s go ahead and set up a configuration in the Knowledge Flow Interface. I’ll just start it up here. I’m going to load an ARFF file with a DataSource called an ARFF Loader.
I’m going to configure that – this is a right-click, Configure – to use the iris dataset, which is here. Then I’m going to need a Class Assigner to assign the class. That’s here – Class Assigner. I can make a connection, and I’m going to make a Dataset connection to the Class Assigner. Then I’m going to get a Cross-Validation Fold Maker, because we’re going to evaluate this with cross-validation. I’m going to connect up the dataset to the CrossValidationFoldmaker. Then I’m going to get a classifier. I’ll use good old J48. Here are all of the classifiers. J48 is up here with the tree classifiers at the end. Let me put that there.
I’m going to connect both the Training Set and the Test Set from the CrossValidationFoldmaker to J48. I’m going to get a Classifier Performance Evaluator in the Evaluation tab. I’m going to connect the classifier – that is, the batch classifier produced by J48 – to this, and I’m going to connect the output to a Text Viewer. Here’s a Text Viewer, the textual output I’m going to connect. Then I’m going to start it all up. I’m going to run it. With my right-click here, I’m going to Start Loading. Let’s have a look at this Text Viewer; right-click to show the results. Here we go. These are the results that we’ve got. Well, we’ve seen these results before many times, of course.
There are a lot of different things back on my slide here. This is what I’ve done.
Here’s the configuration I set up. Next, I’m going to add a Model Performance Chart. Let’s find that. That would be under Visualization. Here’s our Model Performance Chart. I’m going to connect the VisualizableError to this. Then I’m going to have a look at the output. Let me just run this again (Start Loading). Now I’m going to look at the output (Show Chart). Here – well, you’ve seen this kind of chart before – I could plot, for example, the predicted class against the actual class. There are a lot of different things you could do.
Back on the slide here: let’s work with stream data. I’m going to take an ARFF loader in stream mode – not load a dataset, but a single instance at a time. We’re going to use an updateable classifier, an incremental evaluator, and look at a Strip Chart. We clear all of this over here. Select “Data Source”. Let’s get that ARFF loader going, and configure it to use the iris data.
Then I’m going to take that to a Class Assigner, which is in Evaluation.
This time I’m going to make an instance connection: I’m just going to send a single instance along here. And I’m not going to make cross validation folds; I’m going to take that straight to an updateable classifier. There’s an updateable version of NaiveBayes. Some classifiers are updateable and some aren’t.
NaiveBayes Updateable, let’s use that one. I’m going to connect that instance here to the updateable NaiveBayes classifier. Then I’m going to use an Incremental Classifier Evaluator.
It’s an incremental classifier that I’m going to connect up to this. Now I’m going to take the output from that and put it on a Strip Chart. Here’s a Strip Chart.
Take the output here to the chart I picked and put it there. Okay. Let’s show the Strip Chart, which is blank at the moment. Then with my ARFF Loader, I will Start Loading. You can see a little bit of output here. I’m going to use a larger dataset. I could configure this, of course, but the simplest thing is to use a larger dataset. Let me use the segment-challenge dataset and start loading again. Now we get this kind of output. This shows you how the class probabilities change for one class and for the other class as we go through. These are effectively learning curves in this situation. We’ve looked at the Knowledge Flow Interface.
The panels are broadly similar to the Explorer’s with some exceptions. Evaluation is a separate panel, for example. The facilities are broadly similar, as well, with just a couple of notable exceptions. We can deal incrementally with potentially infinite datasets. That’s what we just did – the configuration we just set up loaded from the file incrementally, so it was never stored in memory at the same time, which is what the Explorer does. The Explorer loads everything into memory. Also, you can look inside cross-validation at the models for individual folds. Some people really like graphical interfaces like this, and it’s really good to know about the Knowledge Flow Interface.
<End Transcript>

<-- 1.14 Quiz -->
Looking inside cross-validation 
Question 1
Among the 10 models there are two basically different structures. How many leaves do they have?
Select all the answers you think are correct.
4 leaves
5 leaves
7 leaves
9 leaves
---
Correct answer(s):
4 leaves
5 leaves

<-- 1.14 Quiz -->
Looking inside cross-validation 
Question 2
How many of the 10 models have five leaves?
3
5
7
9
---
Correct answer(s):
7
---
Feedback correct:
The models for folds 2, 3, 4, 6, 7, 9 and 10 are the ones with five leaves. (The fold number appears in brackets in the Result list.)

<-- 1.14 Quiz -->
Looking inside cross-validation 
Question 3
The number that appears in brackets after each leaf (or the first number, if there are two) is the number of instances that reach that leaf. The total number of instances that reach a leaf is the same for each of the ten trees. What is that number of instances?
122
130
135
150
---
Correct answer(s):
135
---
Feedback correct:
135 is 90% of 150, the number of instances used to train each fold of the cross-validation.
---
Feedback incorrect:
135 is 90% of 150, the number of instances used to train each fold of the cross-validation.

<-- 1.14 Quiz -->
Looking inside cross-validation 
Question 4
When two numbers appear in brackets after a leaf, the second number shows how many incorrectly classified instances reach that leaf. One of the 7 models with five leaves makes fewer total misclassifications than the other 6 models. How many does it make?
1
2
3
4
---
Correct answer(s):
2
---
Feedback correct:
The model for fold 7 makes a total of 2 misclassifications, whereas the models for folds 2, 3, 4, 6, 9 and 10 each make a total of 3 misclassifications.

<-- 1.14 Quiz -->
Looking inside cross-validation 
Question 5
The 3 models with four leaves (for folds 1, 5 and 8) all differ slightly from each other. How?
They branch on different attributes.
They branch on the same attributes but the cutoff values differ.
They have a smaller number of class values.
A different total number of instances reach the leaves.
---
Correct answer(s):
They branch on the same attributes but the cutoff values differ.
---
Feedback correct:
These models have the same structure but different cutoff values.

<-- 1.14 Quiz -->
Looking inside cross-validation 
Question 5
The 3 models with four leaves (for folds 1, 5 and 8) all differ slightly from each other. How?
They branch on different attributes.
They branch on the same attributes but the cutoff values differ.
They have a smaller number of class values.
A different total number of instances reach the leaves.
---
Correct answer(s):

<-- 1.15 Video -->
Using the Command Line 
You can do everything the Explorer does (and more) from the command line. Why would you want to – the Explorer is so easy? Well, some people find it faster  to type commands! More importantly, the command line gives you more control over memory usage, because it’s a lower-level way of accessing Weka’s facilities. You will also learn how to consult Javadoc, which is the definitive source of Weka documentation.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello again! Welcome back to New Zealand for a few minutes with More Data Mining with Weka. Let’s look at the Command Line interface in this lesson. Now, the Command Line interface isn’t for everyone, but it’s worth knowing about, just in case you might need to do some more advanced things. We’re going to run a classifier from within the Command Line interface. I’m going to run J48 on the iris data. The first thing I’m going to do is to print the J48 options. Let’s fire up the Simple Command Line interface. I’m going to type “java”. Everything’s going to begin with “java” in this. This is the one line we type into here.
I’m going to type “java weka.classifiers.trees.J48” (I’ll explain the name in a moment). I’m going to hit Enter, and here I’ve got printed out a bunch of information. Actually, this is error information.
It says “Weka exception: No training file and no object input file given.” Because it can’t interpret this command, Weka has kindly printed out the options for J48.
First of all, the general options: “–h” for help; “–t” for training file; “–T” for test file. We’ll be using those. Then, a bit further down after the general options, we’ve got the options specific to J48. There’s the “–c” option and the “–m” option, and a few more options for J48. To make sense of these options, I’ve opened the Explorer here, and this is J48 in the default configuration. You can see here the “–c” option and the “–m” option. These are things that we type into the Simple Command Line interface. This is the default configuration, and these are parameters for J48. I can actually copy here. I’m going to copy the configuration.
I did a right-click, and I’m going to Copy the configuration to the clipboard. Then I’m going to go back and find my Simple Command Line interface, and I’m going to paste. It’s Ctrl-v for paste.
Oh, I should have put “java” at the beginning. I’m going to run this Java program with these options copy and pasted from the Explorer. Then I need a training file. That’s “–t” followed by a space, and now I need to put a filename for my training file.
Here it is. It’s a fully qualified file name starting with the disk. Unfortunately, in the Simple Command Line interface, you need to have fully qualified file names. This is where my datasets are, and it’s the iris.arff file. I’ve surrounded in quotes, because there are actually spaces in this file name, and Windows doesn’t like file names with spaces unless you put quotes around it. Now I’m going to hit Enter again, and it should execute J48 on that dataset. There we go; this is the result. We’ve seen that kind of thing many times before. That’s how you run classifiers in the Simple Command Line interface. Over here on the slide, this is what we did.
We copied the classifier name and the options from the Explorer, then we put the training set afterwards manually. That’s a good way of using the Command Line interface. I want to talk about this complicated name “weka.classifiers.trees.J48”. J48 is a “class”, which roughly means a program in Java. It’s a collection of variables, along with some methods – that is, code – that operates on the variables.
Classes come in packages. A “package” is a directory containing related classes. J48 is in the “trees” package, and the trees package is part of the classifiers package. We can see all this stuff in Javadoc. It’s useful to be able to look at the definitive documentation for Weka, and we can find that in our Weka installation. If I go to where I installed Weka. Here’s My Computer. I’m going to go to C, and I installed it in Program Files (x86). I’m going to find Weka here. There’s Weka, and I’m going to find documentation.html. There is the documentation, and I want to look at the Package Documentation.
I can see the Weka Manual here, but I’m going to look at the Package Documentation. This is called the Javadoc, which is documentation generated from the Java program. This is the definitive source of documentation for Weka. I’m going to find the classifiers. These are the packages up here. It’s a little bit complicated. I’m going to find the classifiers.trees package and click that. Down here I’ve got the contents of the classifiers.trees package, and I can click J48. Here I can see information about the J48 class. Actually, I could have got to the same thing if I had clicked All classes here and looked through this alphabetical list down here for J48, which is here. I get the same information.
When I look at this Javadoc, when I go down here, you can see some computer-y stuff here and you can see the options. This is the definitive source of the options for J48. These are options that you can use in the Explorer or in the Simple Command Line interface. Then there’s a lot of other information. Back to the slide here. We found J48 in the “all classes” list and looked at its documentation.
Now, I know what you’re thinking: “what’s all this geeky stuff?” Well, don’t worry, just try to ignore things you don’t understand, and just power on through here. To set your mind at rest, we’re not going to be using the Simple Command Line interface very much in this course. In fact, we’ll use it in the next lesson, but after that, we won’t be using it at all. Just bear with us while we look at it. I want to find another thing in the Javadoc. If you go back to the Explorer. Perhaps you’ve never noticed this, but – I’ll just find the Explorer again, which is here – you may never have noticed that here we’ve got, this is Open a database.
Open DB..., and if I click this – this is on the Preprocess panel – it says “Open a set of instances from a database”. I get a rather formidable looking form I’ve got to fill in here without really any help. Now we can find the documentation on this in Javadoc. I happen to know this is actually a “converter”, the database converter, and it’s in a package called weka.core.converters, the core of Weka. There’s a bunch of packages in the central core of Weka, and “converters” is one of them. If I look at the database converter, and look at the database loader, that gives us some documentation on this converter.
It’s a little bit complicated here, because reading from a database is a little bit complicated. We’ve got to specify a number of things here, like the URL of the database, the username, a password, and a query, and so on. We can specify all those things. Well, I don’t want to use this converter now, I just wanted to show you that the Javadoc is a source of detailed documentation on different bits of Weka. Coming back to the slide. The database loader will load from any JDBC database. It’s in the Explorer Preprocess panel, but the documentation is here in the Javadoc.
It’s useful to be able to find your way around the Javadoc to see more information about some of the facilities in Weka. This is what we’ve talked about here, the Command Line interface. I showed you it quickly. It can do everything the Explorer does, from the command line. We specify a command with minus followed by a letter followed by a space and then an option like “–c .25” or “–t filename”. You only get one line in the Command Line interface to type things, and people often open a terminal window instead, which gives you some advantages.
You can do scripting, so you can script a sequence of Weka commands, but in order to do that, you need to be able to set up your environment properly, and we’re not going to cover that in this course. I showed you how you can copy and paste a configured classifier from the Explorer. The advantage of the Command Line interface is that it gives you more control over memory usage. It’s kind of a lower level way of accessing the facilities of Weka, and we’ll be doing a little bit of that in the next lesson. Javadoc, as I’ve said, is the definitive source of Weka documentation.
<End Transcript>

<-- 1.16 Quiz -->
Using Javadoc and the Simple CLI 
Question 1
Use Weka’s Javadoc to find out what the “–I” (capital i, not lower-case l ) option of the IBk classifier does. What does the documentation say about this?
Weight neighbors by 1 – their distance
The nearest neighbor search algorithm to use
Weight neighbors by the inverse of their distance
Number of nearest neighbors (k) used in classification
---
Correct answer(s):
Weight neighbors by the inverse of their distance
---
Feedback correct:
There are many ways to find this by clicking around the Javadoc; for example, click All Classes in the small pane at the top left and locate IBk in the list that appears in the pane below.

<-- 1.16 Quiz -->
Using Javadoc and the Simple CLI 
Question 2
In the Explorer, open any data file and select IBk as the classifier.
Configure IBk to use the “–I” option (by setting distanceWeighting to Weight by 1/distance); set the KNN parameter to 5; click OK; right-click the configuration (that is, the large field that appears to the right of the Choose button); and copy it to the clipboard.
Paste this command into the Simple CLI interface, preceded by “java” and followed by “–t iris.arff” (i.e., specify iris.arff as a training file; you will have to precede it by an appropriate directory specification, and if it contains spaces you may need to put the whole filename in quotes). Then press Enter to run the command.
What is the percent accuracy of correctly classified instances?
77%
82%
95%
96%
---
Correct answer(s):
95%
---
Feedback correct:
I hope you didn’t do this in the Explorer, though we both know that you could.

<-- 1.16 Quiz -->
Using Javadoc and the Simple CLI 
Question 3
In preparation for Working with big data, use Weka’s Javadoc to find out which classifiers are “updateable”, i.e., which ones implement the UpdateableClassifier interface.
How many updateable classifiers are there?
7
10
13
18
---
Correct answer(s):
10
---
Feedback correct:
To answer this, note that IBk is an updateable classifer. If you find it in the JavaDoc, links to all the interfaces it implements appear near the top of its Class page. Just click the appropriate one.
In Weka 3.8, the updateable classifiers are HoeffdingTree, IBk, KStar, LWL, MultiClassClassifierUpdateable, NaiveBayesMultinomialText, NaiveBayesMultinomialUpdateable, NaiveBayesUpdateable, SGD, SGDText
---
Feedback incorrect:
To answer this, note that IBk is an updateable classifer. If you find it in the JavaDoc, links to all the interfaces it implements appear near the top of its Class page. Just click the appropriate one.
---
Feedback incorrect:
To answer this, note that IBk is an updateable classifer. If you find it in the JavaDoc, links to all the interfaces it implements appear near the top of its Class page. Just click the appropriate one.
There were 13 updateable classifiers in Weka version 3.6 (AODE, AODEsr, DMNBtext, IB1, IBk, KStar, LWL, NaiveBayesMultinomialUpdateable, NaiveBayesUpdateable, NNge, RacedIncrementalLogitBoost, SPegasos and Winnow). However, in this course we use Weka version 3.8 or later.

<-- 1.16 Quiz -->
Using Javadoc and the Simple CLI 
Question 3
In preparation for Working with big data, use Weka’s Javadoc to find out which classifiers are “updateable”, i.e., which ones implement the UpdateableClassifier interface.
How many updateable classifiers are there?
7
10
13
18
---
Correct answer(s):

<-- 1.17 Article -->
Can Weka process big data?
What is “big data” anyway? The term is typically used to refer to data sets that are so large that data mining tools have difficulty in dealing with them. But how large is this? Well, that depends …
We will learn that there is a limit to the size of datasets that the Weka Explorer can deal with. It’s pretty big, but a fundamental limit is imposed by the way the Explorer works.
But wait! That’s the Explorer. The Command Line interface (and also the Knowledge Flow interface) can be used in a way that imposes no limit on dataset size. That’s right!—datasets can be infinite. (Well, of course, in any physical system there are always file size limits.) The limitation then becomes time: how long are you prepared to wait for an answer?
After this Activity you will be able to explain why the Explorer has a fundamental limit, and have a rough idea what it might be. And you’ll be able to explain why this limit can be transcended using the Command Line interface, and how to do that.

<-- 1.18 Video -->
Working with big data
Some classifiers work incrementally – that is, they update their model as the training dataset comes in, in a single pass through the dataset. When invoked from the command line, these classifiers can handle arbitrarily large datasets. In contrast, the Explorer loads in the entire dataset to begin with irrespective of which classifier is used, so it is limited by the amount of computer memory available. Note that cross-validation cannot work incrementally; you need to be careful about how you do the evaluation, maybe using an explicit test file.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
It’s time to talk about big data. Everyone’s talking about big data. I’ve heard people say it’s like teenage sex. Everyone talks about it, but no one’s actually doing it. Those people probably didn’t have teenage children. Anyway, different people mean different things by big data, and what I mean by big data is datasets that can’t fit into the Weka Explorer. The Explorer loads the entire dataset. When you load a dataset, it’s all got to fit into main memory. How much can it handle? Well, roughly speaking, a million instances with 25 attributes in the default configuration. Actually, if you go to the Explorer and right-click on Status, you can get memory information, and this gives three figures here.
The last figure is the total amount of memory that is allocated to Weka, which is actually a Gigabyte. That’s the default configuration. The other two figures, well, it’s a little bit complicated. The most important thing is the difference between these two figures. If you want to find out more, then you should look up the Java functions freeMemory() and totalMemory(). Although Weka initializes itself with a Gigabyte of memory, on my computer, there’s more. In fact, if I look on my computer, if I right-click on Computer here, I can get the properties. The properties will show me that I’ve got 8 Gb of memory.
I could, in fact, arrange for Weka to initialize itself with more main memory, but I’m not going to do that now. I’m going to try and break it. Let’s see what happens when you break Weka. Well, we can do this by downloading a large dataset. But I’m going to introduce you to Weka’s data generator instead. On the Preprocess panel, there is a Generate button, and that will generate random data according to particular patterns. I’m going to use the LED24 data, show it, and generate it. What this has generated is a dataset with a hundred instances of the LED data, which has got 25 attributes. There they are, the hundred instances. That’s what’s loaded in.
But I can easily generate more than the default 100 instances: let’s generate 100,000 instances by just adding three zeros to this. Generate that. Now it’s generated 100,000 instances. Let’s go and classify this. We could choose, say, J48.
I’m going to use percentage split here. Cross-validation would take a long time. J48 is working away.
It’s finished now, and it’s come up with a percentage accuracy of 73%. Or we could use NaiveBayes, which I think will be a little bit quicker, and that comes up with an accuracy of 74%. Let’s go and generate a million instances then with the data generator. We’ve got 100,000; so there’s a million. We can generate that. It’ll take a few seconds. There are a million instances, and we can go and classify that with NaiveBayes.
After a few seconds, I get the result. Here we go, 74% again. Now, I could try this with J48, but I happen to know that J48 uses more memory than NaiveBayes, and it will crash on this dataset. As things get bigger, the Explorer starts to crash. Actually, I could go and try to generate, say two million instances of this dataset. The Explorer would crash if I did that. When you’re doing this kind of thing, you’re better off using the console version of the Explorer. If you go to your All Programs menu, you’ll find that there are a couple of versions of Weka that are installed for you automatically. One is Weka “with console”.
That brings up this console window, and it’s the console window that reports when things crash, out of memory errors, and so on. If you’re going to mess around with this kind of thing, I’d recommend using that version of Weka.
This is the error message that you ought to get when J48 crashes. Unfortunately, when things break, they tend to break in different ways on different computers, so you might not get this error message. You might see that Weka just goes into an infinite loop and waits forever. It depends. That’s why the console version is a better thing to use. To go further, first of all, we mustn’t use the Explorer, because it loads the entire dataset in. Secondly, we need to use “updateable” classifiers. These are incremental classification models that process a single instance at a time. They don’t load the whole dataset into memory. There are a few of them.
In fact, we looked at them in the activity associated with the last lesson. The one we’re going to use is NaiveBayesUpdateable, which is just like NaiveBayes but an updateable implementation. IBK is also an updateable classifier, and there are a few others. How much data can Weka handle? If you use the Simple Command Line interface and updateable classifiers, then it’s unlimited. Let’s open up the Simple Command Line interface. Here it is. I’m going to create a huge dataset. Actually, I’m going to create a pretty small dataset here with 100,000 instances in. I’m going to run the LED24 data generator and put that in this file here.
That has created that dataset of 100,000 instances, which I’m going to use as a test file. For a training file, I’m going to use 10 million instances. I could change this to 10 million and put this in the training file. However, that would take a few minutes, so I’m not going to do that. Instead, I’ve prepared these files in advance. Let me show you. Here we’ve got test.arff. The test file is a half a Mb, with 100,000 instances. The training file is half a Gb, with 10 million instances. I’ve done a really big training file here, which is 5 Gb, with 100 million instances. Those are the files I’m going to use.
I just need to run the NaiveBayesUpdateable classifier with the training file. This is the very large training file. This is the much smaller test file. If I run that by typing Enter here, it’ll take 4 minutes and produce 74% accuracy with NaiveBayesUpdateable. I can’t do it with J48 because that’s not an updateable classifier. I can try it with a really big file, with any size file. If I were to use my 5 Gb training file with 100 million examples in it, then it would run. It takes about 40 minutes on my computer. There you have it. The Explorer can handle about a million instances with 25 attributes, say. It depends.
You can increase the amount of memory allocated to the Explorer if your computer’s got more than 1 Gb of main memory. We haven’t talked about how to do that, but it’s not difficult. The Simple Command Line interface works incrementally wherever it can. It doesn’t load the dataset into main memory the way the Explorer does. If you use updateable classifier implementations – you can find which ones are updateable using the Javadoc – then the Simple Command Line interface will work incrementally. Then you can work with arbitrarily large files, many gigabytes or hundreds of gigabytes. However, you shouldn’t use cross-validation.
If you were to specify cross-validation in the Simple Command Line interface, then it would have to load the file all in at once. The Command Line interface only doesn’t load the file if you’re not using cross-validation. That’s why we use an explicit test file instead of the default of cross-validation. Working with big data can be difficult and quite frustrating.
<End Transcript>

<-- 1.19 Article -->
Prepare for the quiz
Before starting the quiz we suggest you reproduce what Ian did in the video, using the LED24 data in the Command Line Interface.
Make a test file with 100,000 instances. Precede all filenames below with an appropriate directory specification, surrounded by quotation marks if necessary.
java weka.datagenerators.classifiers.classification.LED24 -n 100000 
        -o test.arff
(The Command Line Interface gives no output; you need to look for the file to see if it has worked.)
Make a training file with 10,000,000 instances:
 java weka.datagenerators.classifiers.classification.LED24 -n 10000000 
        -o train.arff
Apply NaiveBayesUpdateable:
 java weka.classifiers.bayes.NaiveBayesUpdateable -t train.arff 
        -T test.arff -v
(This takes about 30 secs on my computer. The “-v” suppresses evaluation on the training file, which the Command Line Interface does by default.)
Verify that Weka runs out of memory if cross-validation is attempted:
 java weka.classifiers.bayes.NaiveBayesUpdateable -t train.arff
Weka will become unresponsive, although you will probably not get an error message. (Unfortunately it is difficult to trap and report out-of-memory errors in a Java program.)
If you feel brave, repeat the exercise with a 100,000,000-instance training file.

<-- 1.20 Quiz -->
Experience big data
Question 1
Using the Explorer, determine the percentage accuracy of Naive Bayes on covtype.small, evaluated on the training set.
65%
66%
67%
68%
---
Correct answer(s):
66%
---
Feedback correct:
(Weka gives the result as 65.8%)
---
Feedback incorrect:
Are you sure you evaluated on the training set? – this is the result you get when using cross-validation.

<-- 1.20 Quiz -->
Experience big data
Question 2
Using the Explorer, determine the percentage accuracy of Naive Bayes on the same dataset, evaluated by cross-validation.
65%
66%
67%
68%
---
Correct answer(s):
65%
---
Feedback correct:
(Weka gives the result as 65.42%)
---
Feedback incorrect:
The cross-validation accuracy will not be greater than the accuracy when evaluated on the training set, will it?

<-- 1.20 Quiz -->
Experience big data
Question 3
Using the Simple CLI, determine the percentage accuracy of NaiveBayesUpdateable, evaluated on the training set.
65%
66%
67%
68%
---
Correct answer(s):
66%
---
Feedback correct:
The appropriate command is
java weka.classifiers.bayes.NaiveBayesUpdateable 
        -t covtype.small.arff  -T covtype.small.arff
NaiveBayesUpdateable gives the same result as NaiveBayes (65.8%).
I hope you used the Simple CLI, not the Explorer!
---
Feedback incorrect:
The appropriate command is
java weka.classifiers.bayes.NaiveBayesUpdateable 
        -t covtype.small.arff  -T covtype.small.arff
Be sure to specify the training set as the test file for evaluation.
---
Feedback incorrect:
The appropriate command is
java weka.classifiers.bayes.NaiveBayesUpdateable 
        -t covtype.small.arff  -T covtype.small.arff

<-- 1.20 Quiz -->
Experience big data
Question 4
Using the Simple CLI, determine the percentage accuracy of NaiveBayesUpdateable, evaluated by cross-validation.
65%
66%
67%
68%
---
Correct answer(s):

<-- 1.20 Quiz -->
Experience big data
Question 4
Using the Simple CLI, determine the percentage accuracy of NaiveBayesUpdateable, evaluated by cross-validation.
65%
66%
67%
68%
---
Correct answer(s):
65%
---
Feedback correct:
The appropriate command is
java weka.classifiers.bayes.NaiveBayesUpdateable
        -t covtype.small.arff
Note that cross-validation is the default option for evaluation.
NaiveBayesUpdateable gives exactly the same result as NaiveBayes (65.42%).
---
Feedback incorrect:
The appropriate command is
java weka.classifiers.bayes.NaiveBayesUpdateable
        -t covtype.small.arff
Note that cross-validation is the default option for evaluation.

<-- 1.21 Quiz -->
Experience the woes of big data
Question 1
Use the Simple CLI to train NaiveBayesUpdateable on the full covtype dataset and test it on the same set – i.e. evaluate it on the training set.
[Hint: speed things up by using the “-v” switch]
What is the percentage accuracy?
65.428%
65.435%
65.436%
65.536%
---
Correct answer(s):
65.436%
---
Feedback correct:
Use the command
java weka.classifiers.bayes.NaiveBayesUpdateable 
    -t covtype.arff -T covtype.arff -v
(Weka gives the result as 65.4358%)
---
Feedback incorrect:
Use the command
java weka.classifiers.bayes.NaiveBayesUpdateable 
    -t covtype.arff -T covtype.arff -v
Did you specify evaluation on the training set?
---
Feedback incorrect:
Use the command
java weka.classifiers.bayes.NaiveBayesUpdateable 
    -t covtype.arff -T covtype.arff -v
Be sure to round the answer correctly.
---
Feedback incorrect:
Use the command
java weka.classifiers.bayes.NaiveBayesUpdateable 
    -t covtype.arff -T covtype.arff -v

<-- 1.21 Quiz -->
Experience the woes of big data
Question 2
What is the cross-validation accuracy?
65.427%
65.428%
65.436%
65.528%
---
Correct answer(s):
65.428%
---
Feedback correct:
Use the command
java weka.classifiers.bayes.NaiveBayesUpdateable
        -t covtype.arff
(Weka gives the result as 65.4281%)
---
Feedback incorrect:
Use the command
java weka.classifiers.bayes.NaiveBayesUpdateable
        -t covtype.arff
Be sure to round the answer correctly.
---
Feedback incorrect:
Use the command
java weka.classifiers.bayes.NaiveBayesUpdateable
        -t covtype.arff
Note that cross-validation is the default option for evaluation.
---
Feedback incorrect:
Use the command
java weka.classifiers.bayes.NaiveBayesUpdateable
        -t covtype.arff

<-- 1.21 Quiz -->
Experience the woes of big data
Question 3
The above figures for training-set evaluation and cross-validation are almost identical, yet in your experience cross-validation usually gives an appreciably worse result than training-set evaluation. Has this happened by chance?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
The effect of overfitting to the training data reduces as the dataset grows, particularly if there are few classes and they are approximately equally represented in the dataset.
Moreover, NaiveBayes is a simple classifier that collects statistics independently for each attribute and does not overfit as much as, for example, a decision tree learner.
---
Feedback incorrect:
Um … this is a rhetorical question!

<-- 1.21 Quiz -->
Experience the woes of big data
Question 4
But wait … we haven’t really exercised Weka’s incremental operation at all! The fact that the experiment involved cross-validation means that it must have been processed non-incrementally.
Here’s an even more advanced exercise:
Make two datasets with 1,162,024 and 1,743,036 instances by editing the ARFF file to include two and three copies of the data respectively. I did this with the commands
copy covtype.arff + covtype.data covtype2.arff
copy covtype2.arff + covtype.data covtype3.arff
Run NaiveBayesUpdateable on these two datasets, again using cross-validation. Does it work?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
Using cross-validation forces Weka to work non-incrementally, and both these files are too big for that. Weka runs out of heap space  …
… unless you have explicitly increased the Java heap space on your system, or you have more than 8Gb of RAM – because the Java Virtual Machine generally sets the default maximum heap size to a quarter of the available RAM.
---
Feedback incorrect:
… another rhetorical question!

<-- 1.21 Quiz -->
Experience the woes of big data
Question 5
Re-run NaiveBayesUpdateable, this time using covtype.small.arff as a test dataset instead of cross-validation.
[Hint: restart Weka to clear memory before running this]
What is the classification accuracy when trained on the original 581,012-instance data file?
58.52%
58.56%
58.60%
58.61%
---
Correct answer(s):
58.56%
---
Feedback correct:
Use the command
java weka.classifiers.bayes.NaiveBayesUpdateable 
    -t covtype.arff -T covtype.small.arff -v
---
Feedback incorrect:
Use the command
java weka.classifiers.bayes.NaiveBayesUpdateable 
    -t covtype.arff -T covtype.small.arff -v

<-- 1.21 Quiz -->
Experience the woes of big data
Question 6
What is the accuracy when tested on covtype.small.arff but trained on the double-size (1,162,024-instance) dataset?
58.52%
58.56%
58.60%
58.61%
---
Correct answer(s):
58.60%

<-- 1.21 Quiz -->
Experience the woes of big data
Question 7
What is the accuracy when tested on covtype.small.arff but trained on the triple-size (1,743,036-instance) dataset?
58.52%
58.56%
58.60%
58.61%
---
Correct answer(s):

<-- 1.21 Quiz -->
Experience the woes of big data
Question 7
What is the accuracy when tested on covtype.small.arff but trained on the triple-size (1,743,036-instance) dataset?
58.52%
58.56%
58.60%
58.61%
---
Correct answer(s):
58.61%
---
Feedback correct:
Note: this dataset is too large to be loaded into the Explorer, but can easily be processed in the Simple CLI if everything works incrementally.

<-- 1.22 Article -->
Reflect on your experience
Well over a million instances of this 55-attribute dataset are needed before it becomes too big for Weka to load into memory. Beyond that point you have to use “updateable” classifiers, and then there is no limit on size.
As you discovered in the quiz, you could load the 581,000-instance covtype dataset into the Explorer. You could also have loaded the double-size version (1,162,000 instances; try it if you like), but not the triple-size version (1,743,000 instances).
The command line interface allows larger files, provided it’s configured in a suitable way – updateable classifiers and no cross-validation. Using a test set and NaiveBayesUpdateable, it was able to process the triple-size training file; and in fact both training and test files of any size could be processed.
Weka reports the time that NaiveBayesUpdateable takes to build the model and then test it on the training data. On my computer, with the triple-size dataset, training takes 10 secs and testing 0.5 secs. Training takes only 20 times longer than testing – despite the fact that the training file is 175 times larger (1,743,000 instances compared with the test file’s 10,000)!
Why? This involves thinking about how the Naive Bayes algorithm works. Processing a single instance when training involves incrementing 55 counts, one for each attribute. Processing an instance when testing involves 55 multiplications for each of the 7 class values. Thus if the time taken for multiplication and addition were comparable, one would expect testing to take 7 times as long as training.

<-- 1.23 Discussion -->
Reflect on this week's Big Questions
The Big Questions this week are, “What are Weka’s other interfaces for?” and “Can Weka process big data?”
We promised that by the end you’d be able to use the Experimenter, the Knowledge Flow interface, and the Command Line interface. More importantly, you’d realize why they might be useful, even necessary.
Well, can you use them? And have you some idea why they might be necessary?
We also promised that you’d be able to explain why the Explorer has a fundamental limit, and have a rough idea what it might be, and how this limit can be transcended using the Command Line interface.
In the previous course, Data mining with Weka, we challenged you to explain to your partner, siblings, parents or kids what you learned each week. Well, I still think that teaching is the best form of learning, but things may be getting a bit deep for your family members (if they’re not, great!). So in this course, I suggest that you choose a colleague. Tell them about Weka’s other interfaces. Tell them what “big data” means in the context of Weka. And explain how Weka can process really big data.
Then tell your fellow learners how you got on.

<-- 1.24 Article -->
Index
At the end of each week is an index of topics covered that week.
A full index to the course appears under DOWNLOADS, below.
      Topic
      Step
      Datasets
      Breast-cancer
      1.12
      Covtype
      1.21
      Covtype.small
      1.20, 1.21
      Credit-g
      1.12
      Diabetes
      1.12
      Glass
      1.12
      Ionosphere
      1.12
      Iris
      1.11, 1.12, 1.13, 1.14, 1.15
      LED24 data generator
      1.18, 1.19
      Segment-challenge
      1.8, 1.10, 1.12, 1.13
      Classifiers
      IBk
      1.12, 1.16
      J48
      1.8, 1.10, 1.11, 1.12, 1.13, 1.14, 1.15, 1.18
      Logistic
      1.12
      NaiveBayes
      1.12, 1.18, 1.20
      NaiveBayesUpdateable
      1.13, 1.18, 1.19, 1.21, 1.22
      OneR
      1.11, 1.12
      SMO
      1.12
      ZeroR
      1.11, 1.12
      Metalearners
      AdaBoostM1
      1.12
      Plus …
      Cross-validation
      1.10, 1.11, 1.14
      Experimenter interface
      1.8, 1.10, 1.11, 1.12
      Javadoc documentation
      1.15, 1.16
      Knowledge Flow interface
      1.13, 1.14
      Random number seed
      1.9
      Simple CLI
      1.15, 1.16, 1.18, 1.20, 1.21
      Updateable classifiers
      1.16
      Weka installation
      1.4

<-- 2.0 Todo -->
Discretization and text classification 
How can you discretize numeric attributes? 
This week's first Big Question!
2.1
How can you discretize numeric attributes?
article
Discretizing numeric attributes
There are two basic methods for converting numeric attributes to nominal: equal-width binning and equal-frequency binning. A third method is able to preserve the ordering information inherent in numeric values.
2.2
Discretizing numeric attributes 
video (08:37)
2.3
Unsupervised discretization
quiz
Supervised discretization 
"Supervised" discretization takes the class into account when setting discretization boundaries. But when testing, the boundaries must be determined from the *training* set. You can do this with Weka's FilteredClassifier.
2.4
Supervised discretization 
video (07:34)
2.5
Examining the benefits of cheating 
quiz
Discretization in J48 
Some classifiers (e.g. C4.5/J48) incorporate discretization internally, as they go along. But pre-discretization may outperform internal discretization. Whether it does or not is an experimental question!
2.6
Discretization in J48 
video (07:07)
2.7
Pre-discretization vs. built-in discretization 
quiz
How do you classify documents?
This week's second Big Question!
2.8
How do you classify documents?
article
Document classification 
In Weka, documents are represented as "string" attributes, and the StringToWordVector filter creates one attribute for each word. But the overall classification accuracy isn't necessarily what we really care about?
2.9
Document classification 
video (12:28)
2.10
Document classification with Naive Bayes 
quiz
Evaluating 2-class classification 
Threshold curves show different tradeoffs between error types. "Receiver Operating Characteristic" (ROC) curves are a particular type of threshold curve, and the area under the ROC curve measures a classifier's overall quality.
2.11
Evaluating 2-class classification 
video (10:28)
2.12
Activity: Comparing AUCs 
quiz
Multinomial Naive Bayes 
Multinomial Naive Bayes is a classification method designed for text, which is generally better, and a lot faster, than plain Naive Bayes. In addition, the StringToWordVector filter has many useful options.
2.13
Multinomial Naive Bayes 
video (08:42)
2.14
Document classification with Multinomial Naive Bayes 
quiz
2.15
Reflect on this week's Big Questions 
discussion
How are you getting on?
We're well into the course now. Let's just take stock.
2.16
Mid-course assessment
test
This is a test step, it helps you verify your understanding. If you want to earn a Certificate of Achievement on this course you need to complete this test and any others, scoring an average of 70% or above.
To take tests you need to upgrade this course.  It costs $94, which also gets you:
Unlimited access to the course for as long as it exists on FutureLearn, so you can learn at your own pace
A Certificate of Achievement when you’re eligible, to prove what you’ve learned
Upgrade
Find out more
2.17
How are you getting on?
discussion
2.18
Index
article

<-- 2.1 Article -->
How can you discretize numeric attributes?
Converting numeric attributes to nominal is called “discretization”.
But wait! Why would you want to do this? Well, for one thing, some machine learning methods only work on nominal attributes. For another, a model like a decision tree that branches on nominal values like very_big, big, medium, small, very_small may be easier to understand than one that uses numbers. Also, it may actually be better to determine split-points using global information from the whole dataset rather than individually for each branch of the tree.
Supervised discretization is when you take class information into account when determining the split-points. This might produce better split points. But it introduces a subtle but crucial issue concerning the use of class information when creating a model, for then what should you do when faced with with test data that is (of course) completely unlabelled? This important issue transcends discretization, but it’s easier to grasp in the specific context of discretization.
At the end of this week you will be able to explain various discretization strategies: equal width and equal frequency; unsupervised and supervised. You will be able to discretize in a way that preserves the ordering information inherent in numeric attributes, even though the resulting nominal attributes have no intrinsic ordering. You will appreciate why pre-discretization might be better than building the same discretization method into a classifier—and why it might work the other way round! And you will be able to use Weka’s FilteredClassifier to fairly evaluate a classification method that involves supervised discretization.

<-- 2.2 Video -->
Discretizing numeric attributes 
Discretizing is transforming numeric attributes to nominal. You might want to do that in order to use a classification method that can’t handle numeric attributes (unlikely), or to produce better results (likely), or to produce a more comprehensible model such as a simpler decision tree (very likely). This video explains two simple methods, equal-width and equal-frequency binning; and a third, non-obvious, method that preserves the ordering information implicit in a numeric attribute even though it has been converted to nominal. Using these methods in Weka is easy!
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Discretizing is transforming numeric attributes to nominal. There are a couple of obvious ways of doing this. We’ve got a numeric attribute with a certain range. We could take that range and chop it into a certain number of equal parts, or “bins”. Just divide it into equal bins, and wherever a numeric value falls, we take that bin and use its identification as the discretized version of the numeric value. Or instead of using equal-sized bins, we can adjust the size to make the number of instances
that fall into each bin approximately the same: equal-frequency binning. We’re going to talk about those two things. We’ll talk briefly about the choice of the number of bins. Then we’ll talk about how to exploit the ordering information that’s implicit in a numeric value, but not implicit in a nominal value that you convert it to. Let’s look at equal-width binning. I’m going to take ionosphere.arff, which has got a lot of numeric attributes. I’m going to use J48. I’ve set Weka up here with ionosphere. I’ve run J48, and I get 91.5% accuracy. Let’s go and look at some of these numeric attributes. The first one, a1, has got just two distinct values, 0 and 1, actually.
You can see the two values here. The third attribute has got a bunch of different values ranging between –1 and +1, and kind of scrunched up towards the top end. The fourth attribute also varies between –1 and +1. It looks like it could almost be a normal distribution. I’m going to go to a filter here, an unsupervised attribute filter called Discretize. Amongst the parameters here is the number of bins, and I’m going to use 40 bins. And equal frequency – we’re going to use equal-width binning, not equal frequency, leave that as False. I’m going to run it, and look at the result. Here is the first attribute from 0 to 1, just two values.
Here’s the one that was all scrunched up to the top end. This is –1, this is 0, and this is +1. Here is the one that looked kind of normal. You can see it is sort of normal-ish except for a bunch of extra values down here at –1 and +1. I can look at those in the Edit panel, actually. If I undo the effect of that, and go and look in the Edit panel and sort by this attribute, you can see all the –1’s here, and then a bunch of numbers, and then up at the top you can see a bunch of extra +1’s in this column. Now I’ve applied the filter again.
I’m going to classify it and see what J48 makes of that. We get 87.7% accuracy, which is not very good. I can go back and change the number of bins. I’m going to go straight to 2 bins here. I’m going to, first of all, undo the effect of this, and then apply the 2-bin version. You can see that – well, this was two bins to start off with – but you can see that this attribute, there are only two possible values, and this attribute is discretized into 2 bins. If I run J48 again, I get 90.9%, which is pretty good, actually. Going back to the slide, you can see the results for different numbers of bins here.
The last one, 90.9% is about the same, not too much worse than the original undiscretized version. What’s more, the tree has only got 13 nodes. It’s a much smaller, much more economical tree than the one we had before and very little loss in accuracy. So that looks really good. I’m going to move now to equal-frequency binning. Let’s go back here, and take the Discretize filter and change it to equal frequency. I’m going to go back to 40 bins here, and I’m going to run that. First, I need to undo the discretization, and then I’m going to apply this filter. Well, it can’t do much with the first attribute; that was binary to start off with.
But here, you can see that this is where they were all scrunched up towards the top end. This is –1, this is 0, and this is +1. You can see that, where possible, it’s chosen the size of the bins to equalize the frequency. It can’t do anything with this large bin at the top, or this one at the bottom, or this one in the middle, because all of the instances have +1 and here they’ve got 0 and here they’ve got –1. But where it can, it has equalized the frequency. This is the one that used to look normal. You can see there are some extra –1’s, 0’s, and +1’s, and it’s equalized the frequency by choosing appropriate bin widths.
I can go and classify. J48 gives me 87%. It’s a bit disappointing, not very good at all. I can try with different numbers of bins. Let me change this to 2 bins. I need to undo this one first. Then apply. It hasn’t done much here – which was originally just 2 bins – but you can see that here we’ve got 2 equal-sized bins. That’s what histogram
equalization, equal frequency, is trying to do: make bins with the same number of instances in each. If I just run J48 on that, I get 83%, which, again, is pretty disappointing. Coming back to the slide, you can see that all of these equal-frequency binning results are worse than the original results. The size of the tree is not hugely smaller, either. So they’re not really very good. Which method should you use? How many bins should you use? Well, these are experimental questions. There’s a theoretical result called “proportional k-interval discretization” which says that the number of bins should be proportional to the square root of the number of instances.
That doesn’t really help you very much in choosing the number of bins, because it doesn’t tell you what the constant of proportionality should be. It’s an experimental question. A more interesting question is how to exploit ordering information. In the numeric version of the attribute – and this is it at the top, the attribute value – we’ve got a value v here, and there’s an ordering relationship between different values of this attribute. However, when we discretize it here into five different bins, then there’s no ordering information between these bins. Which is a problem, because we might have a test in a tree, “is x < v?”, before discretization.
After discretization, to get the equivalent test, we would need to ask “is y = a?”, “is y = b?”, “is y = c?” and replicate the tree underneath each of these nodes. That’s clearly inefficient, and is likely to lead to bad results. There’s a little trick here. Instead of discretizing into five different values a to e, we can discretize into four different binary attributes, k–1 binary attributes. The first attribute here says whether the value v is in this range, and the second attribute, z2, says whether it’s in this range, a or b. The third, z3, says whether it’s in this range, a, b, or c. The fourth says whether it’s in the first four ranges.
If in our tree we have a test “is x < v?”, then if x is less than v then z1, z2, and z3 are True and z4 is False. So an equivalent test on the binary attributes are “is z3 = True?” If we take that tree we have before, testing on “x < v”, an equivalent test is “is z3 True”. Then we have the same kind of economy of the tree underneath this without replicating different subtrees. That’s very easy in Weka. We just go to our filter, and we set makeBinary to True. That allows us to retain the ordering information that’s implicit in the original numeric attribute.
<End Transcript>

<-- 2.3 Quiz -->
Unsupervised discretization
Question 1
In general, would you expect equal-frequency binning to outperform equal-width binning?
Yes
No
---
Correct answer(s):
Yes
---
Feedback correct:
Equal-frequency binning is sensitive to the data distribution, which will probably make it perform better.

<-- 2.3 Quiz -->
Unsupervised discretization
Question 2
In general, would you expect the binary-attribute version (with the makeBinary option) to improve results for both equal-width and equal-frequency binning?
Yes
No
---
Correct answer(s):
Yes
---
Feedback correct:
The binary attributes produced by the makeBinary option include information about the ordering of the original numeric attribute values.

<-- 2.3 Quiz -->
Unsupervised discretization
Question 3
Create four new versions of the ionosphere data, all of whose attributes are discrete, by applying the unsupervised discretization filter with the four option combinations and default number of bins (10), and write out each resulting dataset. Then use the Experimenter (not the Explorer) to evaluate the classification accuracy using ten repetitions of 10-fold cross-validation (the Experimenter’s default), using J48 and the five datasets (including the original ionosphere.arff).
What percentage accuracy do you get using J48? Fill in the table below.
[Hint: the answers are 87, 89, 90, 92, 93 – but in a different order]
---
Correct answer(s):
90
87
89
93
92

<-- 2.3 Quiz -->
Unsupervised discretization
Question 4
These results contain some small surprises. What is the most striking surprise?
The equal-frequency-binned version outperforms the unfiltered version.
With binary attributes, equal-width bins outperform equal-frequency bins.
---
Correct answer(s):
With binary attributes, equal-width bins outperform equal-frequency bins.
---
Feedback correct:
We would expect binary-attribute discretization to do well, but would not expect equal-width binning to outperform equal-frequency binning when both are producing binary attributes.

<-- 2.3 Quiz -->
Unsupervised discretization
Question 5
Using the Experimenter, compare the binary-attribute and non-binary-attribute versions of equal-width-binning at the 5% level (Weka’s default). (Note: you will have to re-select the row and column in the Analyse panel, and then re-select the test base.)
For equal-width binning, is the binary-attribute version significantly better than the non-binary-attribute version (at the 5% level)?
Yes
No
---
Correct answer(s):
Yes
---
Feedback correct:
For equal-width binning the binary-attribute version yields 93% accuracy, which is significantly better than the 87% accuracy of the non-binary-attribute version.

<-- 2.3 Quiz -->
Unsupervised discretization
Question 5
Using the Experimenter, compare the binary-attribute and non-binary-attribute versions of equal-width-binning at the 5% level (Weka’s default). (Note: you will have to re-select the row and column in the Analyse panel, and then re-select the test base.)
For equal-width binning, is the binary-attribute version significantly better than the non-binary-attribute version (at the 5% level)?
Yes
No
---
Correct answer(s):

<-- 2.3 Quiz -->
Unsupervised discretization
Question 6
Similarly, compare the binary-attribute and non-binary-attribute versions of equal-frequency-binning at the 5% level. (Note: you will have to re-select the test base.)
For equal-frequency binning, is the binary-attribute version significantly better than the non-binary-attribute version (at the 5% level)?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
It’s not significantly better. Although the binary attribute version (92%) is better than the non-binary attribute version (89%), the difference is not significant at the 5% level.

<-- 2.4 Video -->
Supervised discretization 
“Supervised” discretization methods take the class into account when setting discretization boundaries, which is often a very good thing to do. But wait! You mustn’t use the test data when setting discretization boundaries, and with cross-validation you don’t really have an opportunity to use the training data only. Weka’s solution is the FilteredClassifier, and it’s important because the same issue occurs in other contexts, not just discretization.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello again! We’re going to look in this lesson at another discretization technique, supervised discretization. Now you’re probably thinking, “why doesn’t he just tell us the best method of discretization and get on with it, instead of going into all of these different methods?” The answer is, there is no universally best method. It depends on the data, and what I’m trying to do in this course is to equip you to find out the best method for use with your data. Also, this class is about the FilteredClassifier, which is a slightly different classification methodology, which is useful more generally than just in discretization. Let’s take a look.
Discretization is about transforming numeric attributes to nominal, and supervised discretization is about taking the class into account when making those discretization decisions. Here’s the situation. In this diagram, we have numeric x attribute values along the top, and then underneath the discretized versions, which we’ll call y.
There are two bins shown here, out of many, probably: bins c and d. It so happens that all the instances for which the x attribute is in bin c have class 1, and all the instances where the x attribute is in bin d has class 2 – with just one exception, there’s just one thing in bin c which has got class 2. It might be a good idea to shift that boundary just a little bit downwards in the direction of the arrow. Then we’d find that this particular attribute, the discretized version of the attribute, has a precise correspondence with the class values of the instances.
There’s a bit of motivation for why it might be useful to take the class values into account when making those discretization decisions. Supervised discretization. How do you do it? Well, the most common way is to use an entropy heuristic, like the one that’s pioneered by C4.5 (which we call J48 in Weka). I’m looking now at the numeric weather data and the temperature attribute, which ranges from 64 to 85. Underneath are the class values of the instances. So there’s an instance with temperature 64 which has got a “yes” class. And there are two instances with temperature of 72, one’s got a “no” class, and the other’s got a “yes” class.
If we take the entropy of that boundary there, underneath the boundary are 4 “yes”s and 1 “no”, and above the boundary are 5 “yes”s and 4 “no”s. We can calculate the entropy as 0.934 bits. We talked about entropy when we talked about C4.5 in the previous class, and I’m not going to go over that again. The heuristic then is to choose the split point with the smallest entropy, which corresponds to the largest information gain, and to continue recursively until some stopping criterion is met. At the bottom, we’ve continued splitting and made 5 splitting decisions, creating 6 bins for our supervised discretization. Let’s do this in Weka. We’re going to use the supervised discretization filter on the ionosphere data.
We already know that J48 gets 91.5% accuracy on this data. Let’s go now to Weka, and I’ve got the data loaded in here. I’m going to choose the supervised discretization filter, not the unsupervised one we looked at in the last lesson. It’s got some parameters. The most useful is makeBinary. We saw how useful that was in the activity that we’ve just done, associated with the last lesson, but we’ll leave it at False for the moment. I’m going to apply that discretization. Now we can see that a1, the first attribute, has been discretized into 2 bins, which is not surprising, because, in fact, it was a binary attribute.
The second attribute has been discretized into 1 bin, so that will not participate in the decision in any form. The third attribute has been discretized into 4 bins, the fourth into 5 bins, and so on. I think all of the attributes are discretized into somewhere between 1 and 6 bins. Now we can just go ahead and apply J48. But there’s a problem here, if we’re using cross-validation. Because if we were to apply J48 with cross-validation to this discretized dataset, then the class values in the test sets have been used to help with the discretization decision. That’s cheating! You shouldn’t use any information about the class values in the test set to help with the learning method.
So I’m going to undo the effect of this filter. This is quite a general problem. Actually, if I just go to my slide here. We use a FilteredClassifier, which is a meta classifier. We met a meta classifier in the last course when we looked at bagging. Now we’re going to look at another meta classifier. I’m going to choose the FilteredClassifier from “meta”. If I look at the “More” information, it tells us it’s a “class for running an arbitrary classifier on data that has been passed through an arbitrary filter.” That’s exactly what we want. The structure of the filter is based exclusively in the training data. That’s exactly what we want.
It’s got two parameters here: the classifier, which by default is J48 and happens to be the one that we want; and a filter, which by default is the supervised discretization filter, which is also what we want. We can just run this now. That gives us 91.2% accuracy, which is quite good, nearly as good as on the original, undiscretized attribute. We found before that setting the makeBinary option was quite productive. I’m going to set that to True, and run it again. We get a very good result, 92.6% accuracy, which is better than what we got on the undiscretized version of the [dataset]. How big is the tree here? The tree has got 17 nodes, so it’s not too big.
That’s using the FilteredClassifier to avoid cheating, so that the test sets used within the cross-validation do not participate in choosing the discretization boundaries. Let’s try cheating! We’ll probably get a better result if we were to set makeBinary to True in the filter. Filter the data. Apply the filter, and then go back to Classify and apply J48. This is cheating, because we’re using cross-validation and the class values in the test set have been used to help get those discretization boundaries. Sure enough, cheating pays off – 94% accuracy. That is not representative of what we’d expect if we used this system on fresh data. OK, that’s it.
Supervised discretization is when you take the class into account when making discretization boundaries, which is often a good idea. It’s important that the discretization is determined solely by the training set and not the test set. To do this when you’re cross-validating, you can use the FilteredClassifier, which is designed for exactly this situation. It’s useful with other supervised filters, as well.
<End Transcript>

<-- 2.5 Quiz -->
Examining the benefits of cheating 
Question 1
Using OneR on the discretized dataset is cheating. What classification accuracy is obtained?
81%
82%
88%
90%
---
Correct answer(s):
88%
---
Feedback correct:
(More specifically, 87.75%)

<-- 2.5 Quiz -->
Examining the benefits of cheating 
Question 2
Using the FilteredClassifier on the original dataset is not cheating. What classification accuracy is obtained?
85%
86%
88%
90%
---
Correct answer(s):
86%
---
Feedback correct:
(More specifically, 85.87%)

<-- 2.5 Quiz -->
Examining the benefits of cheating 
Question 3
Is the difference significant at the 5% level? (The Experimenter doesn’t compare one method on one dataset with the other method on the other dataset. However, in this case, both classifiers will necessarily produce identical results on one of the datasets. Think about it.)
Yes
No
---
Correct answer(s):
Yes
---
Feedback correct:
The two classifiers produce the same results on the discretized dataset because further filtering has no effect – nominal attributes are unchanged by discretization. Thus you should compare the FilteredClassifier on the two datasets. Do this by selecting the Scheme as the Row and the Dataset as the Column, and looking at the row corresponding to the FilteredClassifier.
---
Feedback incorrect:
The two classifiers produce the same results on the discretized dataset because further filtering has no effect – nominal attributes are unchanged by discretization. Thus you should compare the FilteredClassifier on the two datasets. Do this by selecting the Scheme as the Row and the Dataset as the Column, and looking at the row corresponding to the FilteredClassifier.

<-- 2.5 Quiz -->
Examining the benefits of cheating 
Question 4
Would you expect OneR’s performance to improve if you used the binary-attribute version of discretization?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
It would certainly not improve (though it could stay the same). OneR chooses a single attribute to branch on, and an individual attribute in the binary-attribute version cannot be more informative than the original attribute it was derived from (in general, it’s far less informative).

<-- 2.5 Quiz -->
Examining the benefits of cheating 
Question 5
Replace OneR with J48. How does the result of “cheating” compare with not cheating.
Cheating is significantly better than not cheating.
Cheating is somewhat better than not cheating.
They are the same.
Cheating is somewhat worse than not cheating.
Cheating is significantly worse than not cheating.
---
Correct answer(s):

<-- 2.5 Quiz -->
Examining the benefits of cheating 
Question 5
Replace OneR with J48. How does the result of “cheating” compare with not cheating.
Cheating is significantly better than not cheating.
Cheating is somewhat better than not cheating.
They are the same.
Cheating is somewhat worse than not cheating.
Cheating is significantly worse than not cheating.
---
Correct answer(s):
Cheating is somewhat worse than not cheating.
---
Feedback correct:
In this case J48 obtains 89.49% on the discretized dataset (cheating) and 90.12% when used in the filtered classifier on the original dataset (not cheating). This is slightly surprising. In general one would expect “cheating” to help a little, but not necessarily in every case.
---
Feedback incorrect:
In some cases it may be, but not always.

<-- 2.6 Video -->
Discretization in J48 
J48 effectively discretizes numeric attributes as it goes along, which sounds good because split points are chosen in a local context, taking into account just the instances that reach that node of the tree. But discretizing numeric attributes in advance may outperform this, because more data is available in the global context, leading to more reliable decisions. Which is better? it’s an experimental question! Luckily Weka makes it easy to perform the necessary experiments.
Note: in the 4th slide, “Information gain for the temperature attribute”, the entropy after the split should be 0.895 (not 0.939), making the information gain 0.045 (not 0.001).
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello again! We’re just going to take one last look at discretization. We’re going to look at how J48 does discretization. Because it does deal with numeric attributes, so somehow, inside of J48, it’s got to effectively discretize an attribute. Or at least it’s got to be able to determine a split point for a numeric attribute. Let’s just review how J48 works. It’s a top-down, recursive, divide-and-conquer algorithm.
The algorithm involves first of all selecting an attribute to split on at the root node (that’s the “outlook” attribute in this case), creating a branch for every possible value of that attribute (“sunny”, “overcast”, and “rainy”), splitting the instances into subsets, then going down those three branches and repeating recursively for each branch – selecting an attribute and so on – using only instances that reach the branch.
The key questions are: what’s the best attribute to split on?
And: when should you stop the process? The answer to the first question is, the attribute with the greatest “information gain” – at least, that’s J48’s answer. Information gain is the amount of information that’s gained by knowing the value of the attribute, which is the entropy of the distribution before the split minus the entropy of the distribution after it. Entropy is defined in terms of p log p’s. We talked about that briefly in the previous course. The details we didn’t really go into, and I don’t think they’re too important for you at this point.
In the previous example, the weather data, the information gain for “outlook” was 0.247 bits according to that calculation, and that was the greatest information gain of all of the attributes. So that’s the one that was split on. That’s how it works. Now let’s look at a numeric attribute. This is the “temperature” attribute in the numeric weather data. Here the split point is a number. The trouble is, there’s an infinite number of numbers, so we can’t possibly try them all. However, we’re going to choose split points mid-way between adjacent values in the training set. This reduces it to a finite problem, n–1 possibilities.
So here, for the “temperature” attribute, it goes from 64 at the bottom end to 85 at the top end.
Below are the class values of the instances: when the value of temperature was 64 it was a “yes” instance; and there were two instances where the value was 72, one “no” and one “yes” instance. There are just n–1 possibilities, and we’re going to try them all. Try all possible boundaries. If we take that split point that’s shown, on the left side of the split point we’ve got 4 “yes”s and 1 “no”, and on the right side we’ve got 5 “yes”s and 4 “no”s. We can calculate the entropy before the split and the entropy after the split and subtract them, and the information gain in this case is 0.001 bits.
We might choose that if that’s the greatest information gain of all the possible split points. That’s how it’s going to work for numeric attributes. Here’s an example. We’ve already split, let’s say, on “outlook”. We’ve chosen that at the root. If we look at the “sunny” branch, then there are 5 instances whose outlook is “sunny”. Those are in that little table there. If you look at the value of “humidity”, it’s 70 and 70 for the two “yes” instances, and 85, 90, and 95 for the three “no” instances. That neatly splits the instances into “yes”s and “no”s if we choose a split point somewhere between 70 and 85. We’re going to choose it halfway, at the point 75.
Well, 75 isn’t halfway between 70 and 85, but we’ve got two things at 70 and one thing at 85, so we’re going to use sort of a weighted halfway point. It’s going to be a third of the way from 70 to 85. That’s where we get the 75 from. That’s the split point, and in this case, that’s the end of the matter. We’ve discriminated the instances into “yes” and “no” instances down that branch. In a more complicated example, you can imagine we might split on the same attribute more than once.
If we have a nominal attribute, once we split on it – once we split on “outlook” here – then we’ve used all of the information in the “outlook” attribute, and we certainly aren’t going to split further down the tree on the same attribute. But that’s not true for numeric attributes. We could split with a certain threshold for a numeric attribute, and further down the tree we might split again on the same attribute, but with a different threshold.
Let’s just think about this issue: about discretization when building a tree, as I’ve described, versus discretization in advance, which we looked at in the last couple of lessons. When you do discretization when building a tree, you’re determining the boundaries in a more specific context, in the context of that subtree. A subset of the instances get down here, so you’ve got a more specific dataset to maybe give you a better determination of where a discretization boundary should be. On the other side, the negative side, your decision is based on a small subset of the overall information.
You’ve always got to remember when you’re working with a tree that as you get further down to the bottom of the tree, you’ve got smaller and smaller and smaller numbers of instances. Although you might have a large dataset to begin with, by the time you get way down the tree you’re only dealing with a small number of instances, which is maybe not a good basis on which to make a decision. Another issue is computational complexity. As I’ve described the algorithm, for every internal node, the instances that reach it must be sorted separately for each numeric attribute, so that you can work out which split point on which attribute gives you the best information-gain value.
The complexity of sorting is n log n, where n is the number of things you’re sorting. It looks like you’ve got to repeatedly sort instances, which could be a bit demanding computationally. But in fact repeated sorting can be avoided with a better data structure. So it’s not computationally disastrous to do internal discretization. That’s it. C4.5 incorporated discretization very early on. Pre-discretization, as we’ve seen in the last lessons, is an alternative, which has been refined later. Supervised discretization uses essentially the same entropy heuristic as C4.5. We can retain the ordering information that numeric attributes imply. We don’t have to keep on sorting them as we go further down the tree. Will internal discretization in J48 outperform pre-discretization?
Well, there are arguments both for and against, which we’ve talked about.
<End Transcript>

<-- 2.7 Quiz -->
Pre-discretization vs. built-in discretization 
Question 1
For which datasets does J48’s built-in discretization give better results than the FilteredClassifer?
Select all the answers you think are correct.
diabetes
glass
ionosphere
iris
schizo
---
Correct answer(s):
diabetes
iris
schizo
---
Feedback incorrect:
J48’s built-in discretization gives better results on the diabetes, iris, and schizo datasets.

<-- 2.7 Quiz -->
Pre-discretization vs. built-in discretization 
Question 2
For which dataset does J48’s built-in discretization give significantly better results (at the 5% level)?
diabetes
glass
ionosphere
iris
schizo
---
Correct answer(s):
schizo

<-- 2.7 Quiz -->
Pre-discretization vs. built-in discretization 
Question 3
For which dataset (if any) does J48’s built-in discretization give significantly worse results?
diabetes
glass
ionosphere
iris
schizo
(none)
---
Correct answer(s):
(none)

<-- 2.7 Quiz -->
Pre-discretization vs. built-in discretization 
Question 4
Repeat with the classifiers JRip and PART (use default settings throughout), comparing them on the above datasets with and without Weka’s supervised Discretize filter with the makeBinary option set. This time, look at just the results that are statistically significant (at the 5% level).
For which dataset (if any) does JRip’s built-in discretization gives significantly better results than the Filtered Classifer?
diabetes
glass
ionosphere
iris
schizo
(none)
---
Correct answer(s):
schizo

<-- 2.7 Quiz -->
Pre-discretization vs. built-in discretization 
Question 5
For which datasets (if any) does JRip’s built-in discretization gives significantly worse results than the Filtered Classifer?
diabetes
glass
ionosphere
iris
schizo
(none)
---
Correct answer(s):
(none)

<-- 2.7 Quiz -->
Pre-discretization vs. built-in discretization 
Question 6
For which dataset (if any) does PART’s built-in discretization gives significantly better results than the Filtered Classifer?
diabetes
glass
ionosphere
iris
schizo
(none)
---
Correct answer(s):
(none)

<-- 2.7 Quiz -->
Pre-discretization vs. built-in discretization 
Question 7
For which dataset (if any) does PART’s built-in discretization gives significantly worse results than the Filtered Classifer?
diabetes
glass
ionosphere
iris
schizo
(none)
---
Correct answer(s):
(none)

<-- 2.7 Quiz -->
Pre-discretization vs. built-in discretization 
Question 8
The classifiers SMO and SimpleLogistic implement linear decision boundaries in instance space.
How would you expect pre-discretization (with makeBinary enabled) to affect their performance?
Make it worse than without discretization.
Make it better than without discretization.
---
Correct answer(s):
Make it better than without discretization.
---
Feedback correct:
Discretization (with makeBinary enabled) would probably improve performance, because it allows a more complex model, one that is no longer linear in the decision space.

<-- 2.7 Quiz -->
Pre-discretization vs. built-in discretization 
Question 9
Confirm your intuition (using default settings throughout) by testing the above datasets.
For which datasets does pre-discretization significantly improve SMO’s performance?
Select all the answers you think are correct.
diabetes
glass
ionosphere
iris
schizo
---
Correct answer(s):
glass
schizo

<-- 2.7 Quiz -->
Pre-discretization vs. built-in discretization 
Question 10
For which dataset (if any) does pre-discretization make SMO’s performance significantly worse?
diabetes
glass
ionosphere
iris
schizo
(none)
---
Correct answer(s):
(none)

<-- 2.7 Quiz -->
Pre-discretization vs. built-in discretization 
Question 11
For which datasets does pre-discretization significantly improve SimpleLogistic’s performance?
Select all the answers you think are correct.
diabetes
glass
ionosphere
iris
schizo
---
Correct answer(s):
ionosphere
schizo

<-- 2.7 Quiz -->
Pre-discretization vs. built-in discretization 
Question 12
For which dataset (if any) does pre-discretization make SimpleLogistic’s performance significantly worse?
diabetes
glass
ionosphere
iris
schizo
(none)
---
Correct answer(s):
(none)

<-- 2.7 Quiz -->
Pre-discretization vs. built-in discretization 
Question 13
IBk can implement arbitrary boundaries in instance space.
How would you expect pre-discretization to affect IBk’s performance?
Pre-discretization would improve its performance
Pre-discretization would make its performance worse
Pre-discretization would not change its performance significantly
---
Correct answer(s):
Pre-discretization would not change its performance significantly
---
Feedback incorrect:
Although it may do so at times, this is probably coincidental.

<-- 2.7 Quiz -->
Pre-discretization vs. built-in discretization 
Question 14
Confirm your intuition (using default settings throughout) by testing the above datasets.
For which dataset (if any) does pre-discretization significantly improve IBk’s performance?
diabetes
glass
ionosphere
iris
schizo
(none)
---
Correct answer(s):
(none)

<-- 2.7 Quiz -->
Pre-discretization vs. built-in discretization 
Question 15
For which dataset (if any) does pre-discretization make IBk’s performance significantly worse?
Select all the answers you think are correct.
diabetes
glass
ionosphere
iris
schizo
(none)
---
Correct answer(s):
(none)

<-- 2.7 Quiz -->
Pre-discretization vs. built-in discretization 
Question 15
For which dataset (if any) does pre-discretization make IBk’s performance significantly worse?
Select all the answers you think are correct.
diabetes
glass
ionosphere
iris
schizo
(none)
---
Correct answer(s):

<-- 2.8 Article -->
How do you classify documents?
Document classification is a popular and important application of data mining.
But how can it be done? Weka allows string attributes, and it’s simple to load an entire document into a single “string” attribute, but what then? We need to be able to get inside the document, to somehow look at its content, in order to stand a chance of classifying it.
It’s easy once you know how. That’s what you’re about to find out.
You’ll also learn about a new classifier, Multinomial Naïve Bayes, which is particularly appropriate for text classification. And you’ll learn ways of evaluating two-class classification that are more nuanced than the “percent correct” we have been using so far.
Aside: I learned about the problems of evaluating correctness years ago, when we were trying to apply machine learning to detect when a cow is on heat from various behavioral attributes. (Really! This is an important economic issue for artificial insemination. Believe it or not, semen is expensive.) Cows have similar menstrual cycles to women, and remain in estrus for about a day. If you always predict “This cow is not in estrus today”, you’ll be right about 96% of the time (27/28). That’s an impressive correctness figure. But it’s not necessarily what you want.
At the end of this week you’ll be able to use Weka to classify documents. And you’ll be able to use “threshold curves” to show different tradeoffs between error types (e.g. predicting “in estrus” for a cow that is not in estrus is a different type of error than predicting “not in estrus” for a cow that is in estrus)*.

<-- 2.9 Video -->
Document classification 
A document classification problem can be represented in the ARFF format with two attributes per instance, the document text in a “string” attribute and the document class as a nominal attribute. But what can Weka do with a string? Every value – every document’s text – is different from all the others. To accomplish something meaningful, you need to look inside the string. And that’s exactly what the StringToWordVector filter does: it creates an attribute for each word. But – hey! – test documents will contain different words, some of which aren’t in any of the training documents. This conundrum is solved by using the FilteredClassifier, which you’ve already encountered. Cool!
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello again! And now, as they say, for something completely different. The second half of this class is about document classification, this lesson and the next two. And the only thing it has to do with the first half of the class is that both use the Filtered Classifier. Let’s look at some documents. Here are 6 documents. They are very short documents (we’ll look at a much larger example in a minute), just a single sentence each, and they’re classified into “yes” and “no” classes. You can see when you read these that they are all about oil.
The “yes” documents are about oil coming from the ground, and the “no” documents are about oil as used in cooking, “the food was very oily,” for example. We code this training set into ARFF in the standard way, with string attributes. For string attributes we just take the text and surround it by quotes, just as I’ve shown in the bottom here. I’ve loaded this dataset into Weka. We can just have a look at it here. There it is, just what you saw on the slide. And of course we can’t do anything with this at the moment. There are 6 distinct values for the text attribute, and no learning system can learn anything from these 6 different values.
What we’re going to do is use a filter, the StringToWordVector filter – Unsupervised, Attribute, StringToWordVector – which is here. It’s got a bunch of options, but let’s just apply it.
Wow! Look at all these attributes. We’ve got 34 attributes. They’re words like “Crude” and “Demand” and “The”. When you look at it, these are just the words that appeared in the training documents. Actually, the type, the “yes” or “no” thing, has been moved to the first attribute, not the last attribute. When we look at the individual word attributes, like the one for “crude”, it’s just
a number, it’s a numeric attribute with two values, 0 or 1: 0 if it doesn’t appear in that document, and 1 if it does appear in that document. Let’s go and classify this. Let’s use J48.
It’s in gray, actually. I can still select it, but I can’t start it. The reason why I can’t start it is that, by default, Weka is predicting the last attribute, and the last attribute is numeric, the word “was”. So I’ll just change this to predict the “type”. Then I can run J48, but there’s a problem evaluating it, because there are only 6 instances and we’re trying to do 10-fold cross-validation, which isn’t going to work. Let’s just evaluate this on the training set for the moment. The most useful thing to look at in the result here is the decision tree that’s produced, which is here. Let’s look at the tree. You can see that it tests on the single word “crude”.
If “crude” does not appear, then it’s a “no” document – that is, it’s about food. If “crude” does appear, it’s a “yes” document – that is, it’s about oil coming out of the ground. It makes kind of sense; it’s a pretty of trivial example, I guess. I’ll just go back to the slide. This is what we’ve done. We loaded the data set into Weka. We looked at the string attributes. We applied this filter, which created a lot of new attributes, one for each word. They were binary (2-valued) numeric attributes. We used J48, had to set the class attribute, and evaluated on the training set. Then we looked at the tree. I want to evaluate this on a “Supplied test set”.
I want to see what the predictions are on this test set. These are the documents in the test set. I’ve coded them as Unknown, that is, a question mark in the ARFF file. We’ve never done this before. We haven’t ever looked at predictions for individual test documents or test instances. Let me now go and get the Supplied test set, which I have here. Now I’ve got that test set. I can start this running. Well, it’s obvious really – there’s a problem evaluating the classifier, because, you know, when I look at the test documents, it’s an ARFF file with string attributes, and the training documents are an ARFF file with word attributes.
Of course, I can take these test documents and convert them using the StringToWordVector filter, but that still wouldn’t solve the problem, because I might have different words in a different order here, so I’d still have a different structure to the ARFF file. We’ve got to do something different. That’s where the FilteredClassifier comes in.
Just going back to the slide, there’s a problem evaluating the classifier: we can’t simply apply StringToWordVector to the test file. The solution is the FilteredClassifier. As we saw previously, the FilteredClassifier will create a filter from the training set and use it for the test set. That’s exactly what we’re going to do here. Coming back to Weka, I’m going to undo the effect of this filter, so I’ve got the original string attribute. I’m going to find the FilteredClassifier (meta>FilteredClassifier). I’m going to configure that to use J48 as the classifier, which is done by default,
and I’m going to use the StringToWordVector filter: it’s an Unsupervised Attribute filter. Let me just run this.
Here we get the result.
That’s actually not very interesting, because these documents had question marks instead of classifications. What I wanted to do was output the predictions, and I can do that in the More options menu. If I click Output predictions and run it again, now I can see the predictions for the test instances. As you can see, there’s 1 “yes” and 3 “no” predictions. The actual class is a question mark in each case. Coming back to the slide. That’s not exactly what I wanted. The first instance is certainly “yes”, oil coming out of the ground, but so is the third. That should have been a “yes”, and, in fact J48 has predicted a “no” for that document “Iraq has significant oil reserves”.
Obviously, it doesn’t contain the word “crude”, which is the test that J48 is doing. Well, these are tiny little documents. Let’s look at something a bit more substantial. I’m going to take a big dataset, ReutersCorn-train.arff. Let’s just look at it in a minute. I’m going to open it now.
There are 1,554 documents. This is a lot bigger. If I apply the StringToWordVector filter, then – it just takes a second – I get a lot of attributes, corresponding to words. Actually, there are 2,234 attributes. Again, the class attribute has been moved to the top, attribute number 1. I’m going to undo the effect of this [filter], because we’re going to classify this using the FilteredClassifier. I’m going to set a different test set. I’m going to open ReutersCorn-test.arff. Then I’m going to run this with J48. The FilteredClassifier. It’s just going to take a second. It’s finished now. I get 97% accuracy. Before we go on, let’s have a look at what this dataset looks like.
I’m going to open up the file, the training file. Here it is.
There are two attributes: a string attribute and a class attribute which is 0 or 1. Here’s the beginning of the first string, and it’s a long string. In fact, this open quote, right down to the closing quote here, this whole bit of text is one string attribute value. It’s followed by a 0, which means the classification of that document is 0. For this dataset, that means it’s not about corn. You can see this is regular text except these “\n”s, those are new lines. If we just had a regular newline in a string, then Weka would get confused when you tried to load in that ARFF file, because it would think that the continuation of the line was the next instance.
So we just encode newlines as “\n”. This is one instance, classified as 0. The next thing starts with a quote – this is the string – and it ends here. That’s a 1 document; this document is about corn. That doesn’t necessarily mean it just contains the word “corn”, it means that a human has decided whether this document is about corn or not about corn. I don’t know a lot about corn, but an expert will have made that decision. These are the documents, and, like I said, there are 1,554 of them. Each instance contains this extensive string. If I now go back and have a look, well, I’ve got really high accuracy, 97%, which sounds really good.
Unfortunately, though, when I look at this, the documents that are about corn, the “1” documents – there’s only 24 of them – and the accuracy there is 15 correct out of 24, which is not so good. For the “0” documents, the ones which aren’t about corn, then I’ve got 573 correct out of 580, which is very good. When I combine those two, that’s what gives me this rather high-looking 97% accuracy. When I look at the tree – here it is – it’s little bit more complicated. We’re going to branch on the word “corn”. If the document contains the word “corn”, then we’re going to look for the word “planted”. If it contains the word “planted”, then it’s a “0”.
If it doesn’t contain the word “planted”, then it’s a “1”, that is, it’s about corn. Down here, we’re looking for the word “1986/87”, which is a very strange thing to be looking for. We’re looking for the word “maize”. Here we’re looking for the word “the”. This tree doesn’t look like is makes a huge amount of sense. And yet it does get 97% accuracy. This is what we’ve done here. We looked at this dataset. We applied the StringToWordVector filter. We just had a look, and we found that there were 2,234 attributes. Then we used the FilteredClassifier to get 97% classification accuracy, but we discovered that the accuracy on the 24 corn-related documents was only 62%.
That’s a shame, because those are probably the documents we’re most interested in. These are the ones that aren’t about corn, and we get very high accuracy on those. Which makes you wonder whether the overall classification accuracy is really the right thing to optimize. This is what we’ve done in this lesson. We looked at string attributes. We looked at the StringToWordVector filter, which creates one attribute for each different word. We looked at the options for the StringToWordVector – no we didn’t look at the options. Let’s have a really quick look back in Weka here at the options for the StringToWordVector filter.
Suffice it to say, there are a lot of options: it’s a pretty comprehensive kind of filter. We’ll look at those options in a subsequent lesson. We looked at J48 models for text data. J48 is not necessarily a very sensible learning scheme to use on text data. Then we looked at the overall classification accuracy. Is it really what we care about? Perhaps not. That’s what we’re going to look at in the next lesson.
<End Transcript>

<-- 2.10 Quiz -->
Document classification with Naive Bayes 
Question 1
What’s the first thing you should do before starting to evaluate classifiers on a new dataset?
Apply a filter to the data.
Try a simple baseline classifier.
Check the source of the data for missing values.
---
Correct answer(s):
Try a simple baseline classifier.
---
Feedback correct:
In this case, ZeroR gets 96.0%, so it doesn’t outperform J48’s 97.4%. Phew! But OneR, a slightly more sophisticated baseline, gets 97.5%. Oh no! And it yields a really simple model, branching on the single attribute CORN, whereas J48’s model is far more complex.
I should practice what I preach!
---
Feedback incorrect:
Probably not a bad idea, but not the answer we’re looking for …

<-- 2.10 Quiz -->
Document classification with Naive Bayes 
Question 2
Using the new dataset ReutersGrain-train and ReutersGrain-test, evaluate the FilteredClassifier with the StringToWordVector filter and the J48 classifier (default parameters throughout).
What is the overall classification accuracy on the test set?
78%
96%
97%
98%
---
Correct answer(s):
96%
---
Feedback correct:
(It’s actually 96.3576%.) In this case the ZeroR baseline gives appreciably worse overall accuracy than J48 (91%), and OneR is slightly worse too (96%).

<-- 2.10 Quiz -->
Document classification with Naive Bayes 
Question 3
What are the percentage classification accuracies (rounded to the nearest integer)?
---
Correct answer(s):
67
99

<-- 2.10 Quiz -->
Document classification with Naive Bayes 
Question 4
Now repeat this exercise using Naive Bayes as the classifier instead of J48.
What are the percentage classification accuracies (rounded to the nearest integer)?
---
Correct answer(s):
80
81

<-- 2.10 Quiz -->
Document classification with Naive Bayes 
Question 5
If you apply the StringToWordVector filter in the Preprocess panel, you will notice that that although the attributes all have values 0 and 1, they are nevertheless defined as “numeric”. In fact, this causes NaiveBayes to treat them differently to nominal attributes (technically, it assumes that they are distributed according to a Gaussian distribution). So let’s apply the NumericToNominal filter to convert the attributes to nominal, and re-evaluate NaiveBayes.
But now we have two filters! How can we use the FilteredClassifier with multiple filters? The answer lies in the MultiFilter, which applies several filters successively. It appears at the top of the list of filter choices.
Figure out how to apply it (the interface is a little weird: you’ll have to experiment). First check that you get the same results as before if you configure MultiFilter to use the single StringToWordVector filter. Now add NumericToNominal to convert the attributes to nominal.
Re-evaluate the classification accuracies for NaiveBayes. What are they (as percentages, rounded to the nearest integer)?
---
Correct answer(s):

<-- 2.10 Quiz -->
Document classification with Naive Bayes 
Question 5
If you apply the StringToWordVector filter in the Preprocess panel, you will notice that that although the attributes all have values 0 and 1, they are nevertheless defined as “numeric”. In fact, this causes NaiveBayes to treat them differently to nominal attributes (technically, it assumes that they are distributed according to a Gaussian distribution). So let’s apply the NumericToNominal filter to convert the attributes to nominal, and re-evaluate NaiveBayes.
But now we have two filters! How can we use the FilteredClassifier with multiple filters? The answer lies in the MultiFilter, which applies several filters successively. It appears at the top of the list of filter choices.
Figure out how to apply it (the interface is a little weird: you’ll have to experiment). First check that you get the same results as before if you configure MultiFilter to use the single StringToWordVector filter. Now add NumericToNominal to convert the attributes to nominal.
Re-evaluate the classification accuracies for NaiveBayes. What are they (as percentages, rounded to the nearest integer)?
---
Correct answer(s):
87
77
88

<-- 2.11 Video -->
Evaluating 2-class classification 
In the last lesson we encountered a two-class dataset where the accuracy on one class was high and the accuracy on the other was low. Because the first class contained an overwhelming majority of the instances, the overall accuracy looked high. But life’s not so simple. In practice, there’s a tradeoff between the two error types – a different classifier may produce higher accuracy on one class at the expense of lower accuracy on the other. We need a more subtle way of evaluating classifiers that make this tradeoff explicit. Enter the ROC curve …
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello again! At the end of the last lesson we were looking at a two-class dataset where the accuracy on one of the classes was very high and the accuracy of the other class was not very high. But because an overwhelming majority of the instances were in the first class, the overall accuracy looked very high. In this lesson, we’re going to take a closer look at this kind of situation and come up with a more subtle way of evaluating classifiers under these circumstances. Here in Weka I’ve opened the weather data. 14 instances, a simple, artificial dataset, and I’m going to classify it with Naive Bayes. I’ve selected NaiveBayes here, and there it is. I’m interested in the Confusion Matrix.
In fact, I’ve put over on the slide here. Here is the confusion matrix. You can see there are “a”s and “b”s, “yes”s and “no”s. There are 7 “a”s that are classified as “a”s, and 2 “a”s that are classified as “b”s, incorrectly. There’s 1 “b” that’s classified as “b” – that’s correct – and 4 “b”s that are classified as “a”, incorrectly. I want to introduce some terminology here. We’re going to talk about “true positives”, those 7 correctly classified “a”s; and “true negatives”, that 1 correctly classified “b”. “False positives” are negative instances that are incorrectly assigned to the positive class. They look like they’re positives, but they’re false. That’s the 4. And “false negatives” conversely.
We’re going to be interested in the “true positive rate”, that is the accuracy on class “a”, which is 7 (the number of true positives), divided by the total size of class “a”, that is 9; and the “false positive rate”, which is the number of false positives, 4, divided by the total number of negative instances, that is 5. That’s 0.80. That’s 1 minus the accuracy on class “b”. The main point of this lesson is that there’s a tradeoff between these things. You can trade off the accuracy on class “a” against the accuracy on class “b”. You can get better accuracy on class “a” at the expense of accuracy on class “b”, and vice versa.
To show you what I mean, let’s go back to Weka. In the More options menu, I’m going to output the predictions. Let’s just run Naive Bayes again. I’m interested in this table of predictions. These are the 14 instances. For this instance, which is actually a “no”, Naive Bayes had a prediction probability of 92.6% for the “yes” class and 7.4% for the “no” class. These two things add up to 1. Because the probability for the “yes” class was greater than the probability for the “no” class, Naive Bayes predicted “yes”. Incorrectly, as it turns out, because it was actually a “no” – that’s why there’s a plus in this error column. That’s the way Naive Bayes gets all of its predictions.
It takes the “yes” probability and the “no” probability, and sees which is larger, and predicts a “yes” or a “no” accordingly. Over on the slide, I’ve got the same data, and then I’ve processed it on the right into a simpler table, with just the actual class and the probability of the “yes” class that’s output by Naive Bayes. I’ve sorted the instances in decreasing order of prediction probability. At the top, we’ve got an instance which is actually a “no” that Naive Bayes predicts to be a “yes”, because the prediction probability for “yes” is 0.926, which is way larger than the prediction probability for a “no”, 1 minus that.
In fact, if you think about it, it’s like Naive Bayes is drawing a line at the 0.5 point – that horizontal line – and everything above that line it’s predicting to be a “yes”; everything below that line it’s predicting to be a “no”. The true positives are those “yes”s above the line – that’s 7 of them.
The “yes”s below the line are incorrectly predicted positive instances. So the “true positive” rate is 7 / 9. Conversely, for the “no” class, things below the line are predicted as a “no”. There’s only one correct prediction there below the line. That’s the very last entry. There are 4 “no”s above the line that are incorrectly predicted to be “yes”s because they are above the line. That gives a false positive rate of 0.8. Like I say, there’s a tradeoff. We could change things if we put the line in a different place. Naive Bayes puts it at 0.5.
But if we were to move the line from 0.5 (that’s the P line) to 0.75 (that’s the Q line), then we’d have a true positive rate of 5/9 – that’s those 5 “yes”s above the line compared with the 4 “yes”s below the line – and a false positive rate of 0.2. That’s the Q line. We’re going to plot these points on a graph. We’re going to plot the accuracy on class “a” (TP) against 1 minus the accuracy on class “b” (FP). You can see the P and Q points on the graph. Now we can get other points on the graph by putting the line in different places.
In the extreme, we could put the line right at the very top above the first instance. That means that we’d be classifying everything as a “no”, which gives us 100% accuracy on the “no” class – that’s an FP rate of 0 – and 0% accuracy on the “yes” class – that’s a TP rate of 0. That’s the 0, 0 point on the graph.
Then, if we take our horizontal line and move it down the table one by one, we’re going to be moving up along that red line until we get to the top, the upper right-hand corner, which corresponds to a line underneath the whole table where we classify everything as a “yes”, getting 100% accuracy on the “yes” class; and nothing as a “no”, getting 0% accuracy on the “no” class, the “b” class. You can get different tradeoffs between accuracy on class “a” and accuracy on class “b” by putting the line at different points. That’s for a single machine learning method. What about a different machine learning method? Well, different machine learning methods will give you different red lines.
There’s one, the dashed line down a little bit below. That’s actually worse than the Naive Bayes line with the P and the Q on it, because where you want to be is in the top left-hand corner. The top left-hand corner corresponds to perfect accuracy on class “a” and perfect accuracy on class “b”. That’s where you’d like to be. So lines that push up toward that top corner, that top red dotted line, are better. That’s where you want to be. One way of evaluating the overall merit of a particular classifier, say the Naive Bayes one shown in the P–Q line, is to look at the area under the curve. That’s the area shown there.
If that area is large, then we’re going to get a better classifier evaluated across all the different possible tradeoffs, the different thresholds. The area under the curve is a way of measuring classifier accuracy independent of the particular tradeoff that you happen to choose. Actually, in Weka, you can look at this curve. It’s called a “threshold curve”, and we’re going to visualize the threshold curve for the positive class. That’s what we get. It’s not a smooth curve, it’s a bit of a jagged curve. In fact, we plot the y axis against the x axis – true positive rate against false positive rate – and each of these points corresponds to a particular point in the table.
There are 13 points, plus 1 at the beginning and 1 at the end; 15 points altogether. The point that I’ve circled there corresponds to a false positive rate of 2/5 and a true positive rate of 5/9. All the other points correspond to different points on the curve. What we want to measure is the area under the curve. It’s called an ROC, “Receiver Operating Characteristic”, curve, for historical reasons. Weka prints out the area under the ROC curve. In this case it’s 0.5778. If we could find a classifier that pushed a bit more up towards the top left, then that would be better, give us a better area.
And actually, if we were to evaluate J48 – which I won’t do, but it’s very simple – on the same dataset (just run J48 and look at the curve), we’ll get a curve like this, the dashed blue line, which is better. The area under that curve is 0.6333, which is better than Naive Bayes. We’re looking at threshold curves that plot the accuracy of one class against the accuracy on the other class, and that depict the tradeoff between these two things. ROC curves plot the true positive rate against the false positive rate. They go from the lower left to the upper right, and good ones stretch up towards the top left corner.
In fact, a diagonal line corresponds to a random decision, so you shouldn’t go below the diagonal line. The area under the curve is a measure of the overall quality of a classifier. It turns out that it’s equal to the probability that the classifier ranks a randomly chosen positive test instance above a randomly chosen negative one.
<End Transcript>

<-- 2.12 Quiz -->
Activity: Comparing AUCs 
Question 1
What is the weighted-average ROC Area for J48?
0.51
0.69
0.90
0.91
---
Correct answer(s):
0.69

<-- 2.12 Quiz -->
Activity: Comparing AUCs 
Question 2
What is the weighted-average ROC Area for Naive Bayes?
0.90
0.95
0.96
0.98
---
Correct answer(s):
0.96

<-- 2.12 Quiz -->
Activity: Comparing AUCs 
Question 3
What is the weighted-average ROC Area for NaiveBayes with nominal attributes? (Hint: use the NumericToNominal filter as well as the StringToWordVector filter, and combine them with the MultiFilter.)
0.89
0.92
0.93
0.96
---
Correct answer(s):
0.89

<-- 2.12 Quiz -->
Activity: Comparing AUCs 
Question 4
Judging by the ROC Area, which of these three methods perform best?
J48
NaiveBayes with nominal attributes
NaiveBayes with numeric attributes
---
Correct answer(s):
NaiveBayes with numeric attributes
---
Feedback correct:
This method produces a weighted-average ROC area of 0.962, significantly better than J48, and better than NaiveBayes using nominal attributes (which is perhaps a little surprising).

<-- 2.12 Quiz -->
Activity: Comparing AUCs 
Question 5
Examine the ROC curve for J48, which you obtain by right-clicking its line in the Result list and selecting “Visualize threshold curve” for 0.
Which part of the graph corresponds to more than 75% accuracy for the second class?
The leftmost 25% of the horizontal axis.
The rightmost 25% of the horizontal axis.
The upper 25% of the vertical axis.
---
Correct answer(s):
The leftmost 25% of the horizontal axis.
---
Feedback correct:
The horizontal (X) axis plots the False Positive (FP) rate, which is
1 – accuracy of the second class, 
so the region from 0 to 0.25 corresponds to more than 75% accuracy for the second class.
---
Feedback incorrect:
The horizontal (X) axis plots the False Positive (FP) rate, which is
1 – accuracy of the second class
---
Feedback incorrect:
The error rate for the second class is plotted on the horizontal axis.

<-- 2.12 Quiz -->
Activity: Comparing AUCs 
Question 6
Which of these statements regarding this ROC curve do you agree with?
An accuracy of 75% for the second class can be combined with with an accuracy of 75% for the first class.
An accuracy of more than 75% for the second class can only be achieved with an accuracy of less than 5% for the first class.
An accuracy of 75% for the second class can be combined with an accuracy of 25% for the first class.
---
Correct answer(s):
An accuracy of more than 75% for the second class can only be achieved with an accuracy of less than 5% for the first class.

<-- 2.12 Quiz -->
Activity: Comparing AUCs 
Question 7
Examine the corresponding ROC curve for NaiveBayes with numeric attributes (i.e., the default configuration).
Which part of the graph corresponds to more than 80% accuracy for the first class?
The rightmost 20% of the horizontal axis.
The lower 20% of the vertical axis.
The upper 20% of the vertical axis.
---
Correct answer(s):
The upper 20% of the vertical axis.
---
Feedback correct:
The vertical (Y) axis plots the True Positive (TP) rate, which is the same as the accuracy for the first class.

<-- 2.12 Quiz -->
Activity: Comparing AUCs 
Question 8
Which of these statements regarding this ROC curve do you agree with?
An accuracy of more than 90% for the first class can be combined with an accuracy of more than 90% on the second class.
An accuracy of more than 75% for the first class can be combined with an accuracy of more than 95% for the second class.
An accuracy of more than 95% for the first class can be combined with an accuracy of more than 80% for the second class.
---
Correct answer(s):
An accuracy of more than 75% for the first class can be combined with an accuracy of more than 95% for the second class.

<-- 2.13 Video -->
Multinomial Naive Bayes 
Naive Bayes has three flaws when applied to document classification. First, a word’s non-appearance counts just as much its appearance, whereas surely a document’s class is determined by the words that are in it rather than those that aren’t? Second, Naive Bayes doesn’t take account of the number of appearances of a word, whereas surely frequently occurring words should have a greater influence on the class than ones that only appear once? Third, it treats all words the same, whereas surely unusual words like “weka” and “breakfast” should count more than common ones like  “and” and “the”? Multinomial Naive Bayes is a classification method that solves these problems and is generally better and faster than plain Naive Bayes.
(Note: Ian sets “stopList” to “True” in this video. In the version of Weka you are using you should set “stopwordsHandler” to “Rainbow”.)
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello again! This is the last lesson in Class 2, and we’re going to get back to some actual document classification. In fact, we’re going to introduce a new classifier, Multinomial Naive Bayes, designed for document classification. I’d like you to recall the Naive Bayes classifier. We talk about the probability of the event H, that is, the probability of a particular class, given evidence E, that is, a particular set of attribute values for an instance. Naive Bayes updates the prior probability of H without knowing anything about the instance. So in the weather data, I think there are 9 “play” instances and 5 “don’t play” instances, so the prior probability of “play” is 9/14 without knowing anything about the instance.
Naive Bayes updates that with information about the instance, that is, the attribute values, to get the probability of H, the class, given the instance. The “naive” part is that it takes these attribute values, this evidence, and splits it into independent parts, one for each attribute, and multiplies these together. This is a good thing to do if the attributes really are independent. So E1 is like the first attribute value, and E2 is like the second attribute value and so on. That’s how Naive Bayes works. There are a couple of problems here for document classification. First of all, the non-appearance of a word counts just as much in Naive Bayes as the appearance of a word.
It makes intuitive sense that the class of a document is more determined by the words that are in it than the words that aren’t in it. Secondly, Naive Bayes doesn’t account for the fact that a word might occur multiple times in a document. A word that occurs lots probably should have a greater influence on the class of the document than a word that only appears once. Thirdly, it treats all words the same. The word “and” or “the” is treated the same as an unusual word like “weka” or “breakfast”, and that doesn’t sound reasonable, either. Multinomial Naive Bayes is an enhancement of Naive Bayes that solves these problems.
We take that complicated formula and replace it by the thing at the bottom. Just forget about those exclamation marks for the moment. This is basically a product over all the words in the document of p_i, that is the probability of word i, to the power n_i, that is the number of times that word appears in that document. It’s like treating each word appearance as an independent event and multiplying them all together. And those n-factorials are just a technicality that account for the possibility of different word orderings. That’s the theory; you don’t have to understand that. It’s very easy to use Multinomial Naive Bayes in Weka. This is what we’re going to do. I’m going to open a training set.
We’re going to use ReutersGrain, which is like the “corn” dataset we used previously, only it’s about documents that are about grain. I’m going to open that training file. Then I’m going to use a Supplied test set, that is, the corresponding test file. Then I’m going to use J48. When I try to choose J48, well, it’s grayed out.
We know why it’s grayed out: it’s grayed out because the training file contains a string attribute, and J48 can’t deal with string attributes. We know that what we’re supposed to do here is to use the FilteredClassifier, which is here. Configure that to have J48 as the classifier, which is the default; and for the filter we’re going to choose the unsupervised attribute filter called StringToWordVector. There it is. Let me just run that. Here I get 96% accuracy, but if I look at the accuracy in the minority class, the one that we’re most interested in, the “grain” class, the accuracy is not very good. I get 38 correct out of a total of 57 (19 + 38).
That’s not very good accuracy at all. We know now that I should be looking at the ROC area, which is 0.906.
Going back to the slide: I’ve summarized that information. I could run NaiveBayes; I won’t do that, but let me just tell you that I would get quite a bit worse classification accuracy but a better success rate on the [grain]-related documents, 46/57, and a slightly worse ROC Area (0.885). I’m going to run Multinomial Naive Bayes. I’m going to go back to my FilteredClassifier and configure it to choose NaiveBayesMultinomial.
Run that. It’s very quick. I don’t get very good classification accuracy, but I get rather a good ROC area, and not a bad accuracy on the minority class, 52 out of 57. That’s not too bad, a definite improvement in terms of ROC Area and minority class accuracy over J48. I can actually mess around with some of the parameters in the StringToWordVector filter. There are a lot of parameters here, and they’re very useful. One of the parameters is to output word counts. By default, the filter outputs a 1 if the document contains that word and a 0 otherwise. But we can output the number of appearances of that word in the document, which is suitable for Multinomial Naive Bayes.
I’m going to do a few other things at the same time. I can change all the tokens, all the words, into lower case. I’m going to do that, so that it doesn’t matter whether a word is expressed in uppercase or lowercase, it’s going to count as the same word. Also, I’m going to use a “stoplist”. “Stop words” are those common words, like “and” and “the”, and there’s a standard stoplist for English. If I set this to True, then it’s going to disregard common words, words on the stoplist in Weka. Let me run that again and see what I get. Here I get a slightly better accuracy, a pretty good accuracy actually.
I get a much better ROC Area, and I get phenomenal accuracy on the minority class: just 1 error out of 57 here.
Going back to my slide: with J48 I got really good classification accuracy; now I’m not quite at the same level with NaiveBayesMultinomial. When I first did NaiveBayesMultinomial, it wasn’t too bad, but then when I set outputWordCounts, well, it got slightly worse, actually. I got a worse ROC Area, which is a little bit surprising; better accuracy on the minority class, 54 out of 57. Then when I set lowerCaseTokens and the stoplist as well, I got very good accuracy on the minority class, and a very good ROC Area of 0.978. That’s it. Multinomial Naive Bayes is a machine learning method that’s designed for use with text. It takes into account word appearance, rather than word non-appearance.
It accounts for multiple repetitions of a word, and it treats common words differently from unusual ones by looking at the frequency with which they appear in the document collection. It’s actually a lot faster in Weka than plain Naive Bayes. For one thing, it ignores words that don’t appear in a document – when you think about it, most words don’t appear in a document! Internally, Weka uses what’s called a “sparse representation” of the data; Multinomial Naive Bayes takes advantage of that. The StringToWordVector filter has many interesting options. We looked at some of those. It actually outputs the results in sparse format, which Multinomial Naive Bayes takes advantage of.
<End Transcript>

<-- 2.14 Quiz -->
Document classification with Multinomial Naive Bayes 
Question 1
What is the weighted-average ROC Area for NaiveBayesMultinomial?
0.694
0.952
0.957
0.962
---
Correct answer(s):
0.952

<-- 2.14 Quiz -->
Document classification with Multinomial Naive Bayes 
Question 2
In the StringToWordVector filter, set outputWordCounts and lowerCaseTokens to true; set minTermFreq to 5.
What is NaiveBayesMultinomial’s weighted-average ROC Area now?
0.912
0.974
0.976
0.993
---
Correct answer(s):
0.976

<-- 2.14 Quiz -->
Document classification with Multinomial Naive Bayes 
Question 3
It might help to stem words, that is, remove word endings like -s and -ing. SnowBall is a good stemming algorithm, so set this as the stemmer in the StringToWordVector filter. Also, it might help to reduce the number of words kept per class, so change wordsToKeep from 1000 to 800.
What is NaiveBayesMultinomial’s weighted-average ROC Area now?
0.932
0.934
0.974
0.976
---
Correct answer(s):
0.976
---
Feedback correct:
Disappointingly, these tweaks have not improved the result at all.

<-- 2.14 Quiz -->
Document classification with Multinomial Naive Bayes 
Question 4
Of course, tiny changes in the ROC Area are probably insignificant, so let’s do a proper comparison using the Experimenter. Here, you can’t specify training and test sets separately, so we’ll just use the ReutersCorn-train and ReutersGrain-train training sets, with a 66% percentage split (“data randomized”) – which will be a lot faster than cross-validation.
Set up the Experimenter to use these two files with the FilteredClassifier, the StringToWordVector filter, and the following four configurations:
  NaiveBayes with default parameters for StringToWordVector
Three instances of MultinomialNaiveBayes with the three parameter settings for StringToWordVector that you tested above:
  default parameters
  outputWordCounts, lowerCaseTokens, and minTermFreq = 5
  the above settings plus SnowballStemmer and wordsToKeep = 800.
How do these NaiveBayesMultinomial configurations compare with NaiveBayes?
NaiveBayes performs better than all the NaiveBayesMultinomial methods.
Some NaiveBayesMultinomial configurations perform better than NaiveBayes
All NaiveBayesMultinomial configurations perform significantly better than NaiveBayes.
---
Correct answer(s):
All NaiveBayesMultinomial configurations perform significantly better than NaiveBayes.

<-- 2.14 Quiz -->
Document classification with Multinomial Naive Bayes 
Question 5
Of all the NaiveBayesMultinomial configurations you have tested, which performed the best?
StringToWordVector with outputWordCounts and lowerCaseTokens set to true, and using the SnowballStemmer
StringToWordVector with default parameters
StringToWordVector with outputWordCounts and lowerCaseTokens set to true
---
Correct answer(s):
StringToWordVector with default parameters
---
Feedback correct:
This is true for both data files. However, the differences with the other NaiveBayesMultinomial configurations are not large.

<-- 2.14 Quiz -->
Document classification with Multinomial Naive Bayes 
Question 6
The Experimenter’s default “Comparison field” is Percent_correct, which, as we know, may not be appropriate for this data. Change it to Area_under_ROC. (You don’t have to re-do the experiment; just the test.)
What are the significant differences now?
There are no significant differences
For the Grain dataset, NaiveBayesMultinomial configurations perform significantly better than NaiveBayes
For the Corn dataset, NaiveBayesMultinomial configurations perform significantly better than NaiveBayes
For both datasets, NaiveBayesMultinomial configurations perform significantly better than NaiveBayes
---
Correct answer(s):
For the Grain dataset, NaiveBayesMultinomial configurations perform significantly better than NaiveBayes
---
Feedback correct:
There are no significant differences for Corn, but for Grain all NaiveBayesMultinomial configurations are significantly better than NaiveBayes.

<-- 2.14 Quiz -->
Document classification with Multinomial Naive Bayes 
Question 6
The Experimenter’s default “Comparison field” is Percent_correct, which, as we know, may not be appropriate for this data. Change it to Area_under_ROC. (You don’t have to re-do the experiment; just the test.)
What are the significant differences now?
There are no significant differences
For the Grain dataset, NaiveBayesMultinomial configurations perform significantly better than NaiveBayes
For the Corn dataset, NaiveBayesMultinomial configurations perform significantly better than NaiveBayes
For both datasets, NaiveBayesMultinomial configurations perform significantly better than NaiveBayes
---
Correct answer(s):

<-- 2.15 Discussion -->
Reflect on this week's Big Questions 
The Big Questions this week are, “How can you discretize numeric attributes?” and “How do you classify documents?”
We promised that by the end you’d be able to explain various discretization strategies, and how to discretize in a way that preserves the ordering information inherent in numeric attributes, and why pre-discretization might be better than building the same discretization method into a classifier—and why it might work the other way around! Perhaps the most important skill is how to fairly evaluate a classification method that involves supervised filters (using the FilteredClassifier).
We also promised that you’d be able to use Weka to classify documents, and use threshold curves to show different tradeoffs between error types.
Well, can you explain these things to a colleague? The most challenging ones are:
  how to preserve ordering information in nominal attributes;
  why it’s non-trivial to evaluate a classification method that involves supervised filters;
  threshold curves.
So: choose one, explain it, and tell your fellow learners how you got on.

<-- 2.16 Quiz -->
Mid-course assessment
Question 1
Using a suitable Weka interface, determine the standard deviation of J48’s accuracy (Percent_correct) when 10-fold cross-validation is repeated 10 times on the datasets ionosphere.arff and breast-cancer.arff. What are the standard deviations for the two datasets?
Select all the answers you think are correct.
4.38 for ionosphere
4.44 for ionosphere
5.97 for ionosphere
6.05 for ionosphere
6.63 for ionosphere
4.38 for breast-cancer
4.44 for breast-cancer
5.97 for breast-cancer
6.05 for breast-cancer
8.23 for breast-cancer
---
Correct answer(s):
4.38 for ionosphere
6.05 for breast-cancer

<-- 2.16 Quiz -->
Mid-course assessment
Question 2
Re-run the experiment in Question 1, writing the results into an ARFF file. Load this file into the Explorer and determine the number of instances and attributes in it.
Select all the answers you think are correct.
50 instances
70 instances
100 instances
150 instances
170 instances
200 instances
50 attributes
70 attributes
100 attributes
150 attributes
170 attributes
200 attributes
---
Correct answer(s):
200 instances
70 attributes

<-- 2.16 Quiz -->
Mid-course assessment
Question 3
Load the ARFF file you produced for Question 2 into the Explorer and use J48 to predict Key_Dataset from the other attributes. You will get a simple tree, branching on one attribute. What is that attribute?
Number_of_testing_instances
Area_under_ROC
measureNumLeaves
measureNumRules
Number_of_training_instances
---
Correct answer(s):
Number_of_training_instances

<-- 2.16 Quiz -->
Mid-course assessment
Question 4
Use the Experimenter to compare the Logistic classifier with AdaBoostM1 on the datasets iris.arff, breast-cancer.arff, credit-g.arff, diabetes.arff, glass.arff and ionosphere.arff (default parameters throughout). Record the number of wins and losses for Logistic, and the number that are statistically significant (5% level).
---
Correct answer(s):
4
3
2
0

<-- 2.16 Quiz -->
Mid-course assessment
Question 5
Use a suitable Weka interface to examine the models produced in each fold of the cross-validation when the J48 classifier (default parameters) is run on the labor.arff dataset and evaluated using 5-fold cross-validation. Among the five models produced,
  (a) how many different models are there?
  (b) how many of these are the same except for the branch values for numeric attributes?
Select all the answers you think are correct.
(a) 1
(a) 2
(a) 3
(a) 4
(a) 5
(b) 0
(b) 1
(b) 2
(b) 3
---
Correct answer(s):
(a) 5
(b) 2

<-- 2.16 Quiz -->
Mid-course assessment
Question 5
Use a suitable Weka interface to examine the models produced in each fold of the cross-validation when the J48 classifier (default parameters) is run on the labor.arff dataset and evaluated using 5-fold cross-validation. Among the five models produced,
  (a) how many different models are there?
  (b) how many of these are the same except for the branch values for numeric attributes?
Select all the answers you think are correct.
(a) 1
(a) 2
(a) 3
(a) 4
(a) 5
(b) 0
(b) 1
(b) 2
(b) 3
---
Correct answer(s):

<-- 2.16 Quiz -->
Mid-course assessment
Question 6
In the previous question, there is one attribute that is used in all five J48 models. What is it?
working-hours
wage-increase-first-year
statutory-holidays
longterm-disability-assistance
vacation
---
Correct answer(s):
wage-increase-first-year

<-- 2.16 Quiz -->
Mid-course assessment
Question 7
In the Explorer, test the the effect of different kinds of unsupervised discretization (10 bins) of the labor dataset by performing discretization using the Preprocess panel and executing J48 on the result, evaluating using cross-validation (default parameters).
What order do different settings of the equal width/equal frequency parameter and the binary-attribute parameter come in? (< denotes worse performance)
no discretization < equal frequency < equal width < equal frequency with binary attributes < equal width with binary attributes
no discretization < equal width < equal frequency < equal width with binary attributes < equal frequency with binary attributes
equal frequency < equal width < no discretization < equal frequency with binary attributes < equal width with binary attributes
equal width < equal frequency < no discretization < equal width with binary attributes < equal frequency with binary attributes
equal width < equal frequency < no discretization < equal frequency with binary attributes < equal width with binary attributes
---
Correct answer(s):
equal width < equal frequency < no discretization < equal frequency with binary attributes < equal width with binary attributes

<-- 2.16 Quiz -->
Mid-course assessment
Question 8
Consider the following filtering configurations:
a. supervised discretization using the filtered classifier
b. supervised discretization without the filtered classifier
c. equal-width discretization (10 bins) using the filtered classifier
d. equal-width discretization (10 bins) without the filtered classifier
e. equal-frequency discretization (10 bins) using the filtered classifier
f. equal-frequency discretization (10 bins) without the filtered classifier 
Suppose you were to use each configuration on a particular dataset with a particular classifier, evaluated with cross-validation in the usual way. Which of the following best describes the performance you would expect? (> denotes better performance, = denotes exactly the same, ≈ denotes about the same.)
a ≈ b > c ≈ d > e ≈ f
a ≈ b > e ≈ f > c ≈ d
a > b > c = d > e ≈ f
b > a > e ≈ f > c = d
a = b > c = d > e ≈ f
---
Correct answer(s):
b > a > e ≈ f > c = d

<-- 2.16 Quiz -->
Mid-course assessment
Question 9
Determine the classification accuracy of Naive Bayes and Multinomial Naive Bayes on the ReutersCorn-test.arff dataset, using the ReutersCorn-train.arff dataset for training and the StringToWordVector filter with default parameter settings.
Select all the answers you think are correct.
86.4% for Naive Bayes
87.1% for Naive Bayes
87.6% for Naive Bayes
88.1% for Naive Bayes
91.4% for Multinomial Naive Bayes
92.1% for Multinomial Naive Bayes
93.0% for Multinomial Naive Bayes
93.7% for Multinomial Naive Bayes
---
Correct answer(s):
86.4% for Naive Bayes
93.7% for Multinomial Naive Bayes

<-- 2.16 Quiz -->
Mid-course assessment
Question 10
Load the ionosphere.arff data into the Explorer and display the ROC curve for Naive Bayes and J48 (using cross-validation and default parameters throughout), for the first (“b”) class.
The interface allows you to change the X and Y axes to different variables from the FP rate and TP rate that are used for ROC curves. Change them to True Positives (X) and True Negatives (Y).
Which of the following shapes do you get for Naive Bayes and J48 respectively?
a
b
c
d
---
Correct answer(s):
a

<-- 2.17 Discussion -->
How are you getting on?
By now the course is well underway: two weeks down and three to go.
    What do you think of it so far?
    Do you want to discuss any issues that have arisen?
This is the place to do it!

<-- 2.18 Article -->
Index
(A full index to the course appears at the end of Week 1.)
      Topic
      Step
      Datasets
      Diabetes
      2.7
      Glass
      2.7
      Ionosphere
      2.2, 2.3, 2.5, 2.7
      Iris
      2.7
      Reuters-Corn-train/test
      2.9, 2.12, 2.14
      Reuters-Grain-train/test
      2.10, 2.13
      Schizo
      2.7
      Weather
      2.4, 2.6, 2.11
      Classifiers
      IBk
      2.7
      J48
      2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.9, 2.10, 2.11, 2.12, 2.13
      JRip
      2.7
      NaiveBayes
      2.10, 2.11, 2.12, 2.14
      NaiveBayesMultinomial
      2.13, 2.14
      OneR
      2.5
      PART
      2.7
      SimpleLogistic
      2.7
      SMO
      2.7
      Metalearners
      FilteredClassifier
      2.4, 2.5, 2.9, 2.10, 2.12, 2.13
      Filters
      Discretize (supervised)
      2.4, 2.5, 2.7
      Discretize (unsupervised)
      2.2, 2.3
      MultiFilter
      2.10, 2.12
      NumericToNominal
      2.10, 2.12
      StringToWordVector
      2.9, 2.10, 2.12, 2.13, 2.14
      Plus …
      Experimenter interface
      2.3, 2.5, 2.14
      ROC curve (AUC)
      2.11, 2.12, 2.13
      Stemming
      2.14
      Stop words
      2.13

<-- 3.0 Todo -->
Classification rules, association rules, and clustering 
Is it better to generate rules or trees?
This week's first Big Question!
3.1
Is it better to generate rules or trees?
article
Decision trees and rules 
Any decision tree has an equivalent set of rules ... and for any set of rules there's an equivalent decision tree. But the complexities may be very different – particularly if the rules are to be executed in a predetermined order.
3.2
Decision trees and rules 
video (07:24)
3.3
Adding PRISM to Weka
article
3.4
What could possibly go wrong?
discussion
3.5
Simple decision rules
quiz
3.6
Small rule sets for the contact-lenses data
discussion
Generating decision rules 
"PART" makes good rules by repeatedly creating partial decision trees.
Incremental reduced-error pruning is a standard pruning technique.
"Ripper" follows this by complex optimization to make very small rule sets.
3.7
Generating good decision rules 
video (06:42)
3.8
Comparing rule-based classifiers
quiz
What if there's no "class" attribute?
This week's second Big Question!
3.9
What if there's no "class" attribute?
article
Association rules 
Instead of predicting a "class", association rules describe relations between any of the attributes.
Support and Confidence are basic measures of a rule.
"Apriori" is the standard association-rule-learning algorithm.
3.10
Association rules 
video (05:02)
3.11
Association rules
quiz
Learning association rules 
Apriori's strategy is to specify a minimum Confidence, and iteratively reduce Support until enough rules are found.
It generates high-support "item sets" and turns them into rules.
3.12
Learning association rules 
video (09:06)
3.13
Try some market basket analysis
article
3.14
Here's what I did
article
3.15
Market basket analysis
discussion
Representing clusters 
With clustering, there is no "class" attribute. Instead of predicting the class, we try to divide the instances into natural groups, or "clusters".
There are many different representations for clusters.
3.16
Representing clusters 
video (07:40)
3.17
Clustering the Iris data
quiz
Evaluating clusters
It's hard to evaluate clustering, except perhaps by visualization.
Different clustering algorithms use different metrics for optimization.
If the dataset has a "class" attribute, you can do "classes to clusters" evaluation.
3.18
Evaluating clusters 
video (08:48)
3.19
Comparing clusterers using classification-by-clustering
quiz
3.20
Reflect on this week's Big Questions
discussion
3.21
Index
article

<-- 3.1 Article -->
Is it better to generate rules or trees?
We haven’t talked much about rules.
We’ve spent a lot of time generating decision trees from datasets – the data mining method you’ve encountered most frequently so far is J48, which generates trees. In fact, the only rules we’ve met are the trivial ones created by the ZeroR and OneR baseline methods.
Are rules the same as trees? In one sense they are: given a tree it’s easy to read off a set of rules that makes the same decisions. However, things are not quite so obvious as they appear on the surface. Rule sets are different from trees.
For one thing, they tend to be easier for people to comprehend. This is because each rule has the appearance of being a standalone nugget of knowledge, whereas interpreting bits of a tree depends on what has gone on above. But appearances are deceptive!
Another difference is that rule sets are often far more compact than trees (although the reverse can be true as well). And new methods are required to generate compact rule sets.
At the end of this week you will be able to explain important differences between rules and trees as knowledge representation methods. You’ll know how to read off an equivalent set of rules from a decision tree, and explain why this rule set may well be excessively redundant. And you’ll be able to use two state-of-the-art rule-generating methods in Weka, and explain – at a high level – how they work.

<-- 3.2 Video -->
Decision trees and rules 
Any decision tree has an equivalent set of rules … and any rule set has an equivalent decision tree. So they’re the same? It’s not so simple. If you read rules off a tree, you usually end up with far more rules than necessary, particularly if – as is usual – the rules end up being checked one at a time, in order. And conversely, there are small rule sets for which the equivalent tree is outrageously complex. Whereas trees are created top-down using a “divide and conquer” approach, rules are more naturally created bottom-up, using a covering process.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello again! Welcome to Class 3 of More Data Mining with Weka. In this class, we’re going to look at rules and clustering. In the first couple of lessons, we’re going to look at decision rules. I’m going to look in this lesson at rules versus trees, in abstract, as it were. In the next lesson, we’ll look at how to generate decision rules. We talked a lot about decision trees in Data Mining with Weka. For any decision tree, you can read off an equivalent set of rules. You just go along the leaves. For each of the five leaves, you just read off the conditions above that leaf that get you from the root to the leaf.
“If outlook is sunny and humidity is high then ‘no’” – that’s a rule corresponding to the left-most leaf. That’s easy. We call this a “decision list” if we execute these rules in order. If we execute them in order, then we can delete some of these clauses. We can delete the “and humidity is normal” in the second rule, because we’ve already dealt with the “humidity is high” case in the first rule, and there are no other options – if it’s not high, it’s got to be normal.
Similarly, in the fourth rule, we can delete the “outlook = rainy” bit, because we’ve already dealt with sunny and overcast, and if we pass through those rules, then the outlook has got to be rainy. Using this technique, if we execute our rules as a decision list, we can make the rule set a little bit simpler. There are other rule sets that are equivalent that are even simpler. Let’s look at this 3-rule set.
Imagine: is there an equivalent tree? For any tree, you can make rules; for any set of rules, can you make a tree? The answer is, you can. This is slightly more abstract than the previous example.
I’ve got: “If x=1 and y=1 then a. If z=1 and w=1 then a. Otherwise, b.” I’m assuming that both x and y have three possible values. We branch on x and y down the left-hand branch. If they’re both 1, that’s a; fantastic! Otherwise, though, if y=2, then we’ve got to check z and w. That’s done in that gray tree underneath, which is a little bit complex. To make matters much worse, we’ve got to repeat this tree. If y=3, then we’ve got to repeat the same tree. That little gray triangle stands for that whole gray tree replicated. There are a total of 4 copies of it in this.
We start with a rather simple set of rules, and we end up with a pretty complicated tree. So in one sense, trees and rules are equivalent.
They can describe the same things: given a tree, you can create a set of rules; given a set of rules, you can create a tree. But in practice, they’re very different. Because – particularly if rules are expressed as a decision list and they’re executed in order – they can be much smaller than trees.
People like rules: they’re easy to read and understand. It’s tempting to view them as independent “nuggets of knowledge”. If you’re executing them as a decision list – and you usually are – then the meaning of the rule must be taken in the context of the rules that precede it. They don’t really stand independently, although they look like they do. One way of creating rules – let’s say you want to create rules – you could just create a decision tree. We know how to do that – the top-down, divide-and-conquer method used by J48 – and read rules off the tree, one rule for each leaf, like we did at the beginning. Very straightforward, but the rules will contain repeated tests.
You can get rid of some of those quite easily, but more effective conversions are not so easy to do. Another completely different approach for generating rules is to do it bottom-up, a “covering method” that’s called separate-and-conquer. We work on the different classes in turn. For each class, in turn, we find the rules that cover all its instances. We do that by first identifying a useful rule, then separating out those instances it covers; and then carry on and conquer the remaining instances in that class, finding more rules for them. Here’s a simple little example. We’re going to generate a rule for class a. We start out with a rule that says “Everything is class a!”
Of course, that rule is not correct. So we add clauses to that rule. “If x > 1.2 then class = a.” That rule is still not correct, but it’s better than the first rule.
Then we can add another clause to make it even more correct: “If x > 1.2 and y > 2.6, then class = a.” That’s completely correct. It does miss out one other a. We could add a new rule for that, or we could decide that maybe we don’t need to because it’s just one instance that’s being missed out.
Then, for a rule for class b, we could say “if x <= 1.2 than class = b”: that gets half of them. Another rule could be “if x > 1.2 and y <= 2.6, then class = b.” We could get rid of the first test if we knew those rules were going to be executed in order. We could add more rules to get a perfect rule set, or we could stop there with the rules that we have. The termination conditions are something that you have to decide on when you devise a rule-making algorithm. Here’s a decision tree that corresponds to the rule set we just looked at. Rules sets can be more perspicuous than decision trees.
Decision trees sometimes have to contain replicated subtrees. Also, there’s a big difference between the divide-and-conquer and the separate-and-conquer, the top-down and the bottom-up, algorithms. In a multi-class situation, the covering algorithm focuses on one class at a time, it gets rules for that class; whereas the divide-and-conquer decision tree algorithm takes all the classes into account when making any of the decisions. Here are the details of a simple bottom-up, covering algorithm for creating rules. It’s called PRISM. It consists of three loops. The outer loop is over each class.
Then, [in] the middle loop, we’re going to cover some of the instances in that class, and then carry on creating more rules that cover more of the instances until we’re satisfied we’ve covered enough. The inner loop is taking a rule and adding clauses to the rule until it’s accurate enough. It’s a simple, iterative algorithm for covering a dataset class-by-class, instance-by-instance, bottom-up; elaborating the rules with more conditions as we go along. That’s it. We’ve seen that decision trees and rules have got the same expressive power, but either can be more perspicuous than the other. It just depends on the dataset. Rules can be created using a bottom-up covering process. They’re often executed as decision lists, in order.
If rules assign different classes to an instance, then the first rule wins if you’re executing them in order – which means that the rules are not really independent nuggets of knowledge. Still, people like rules, and they often prefer them to trees.
<End Transcript>

<-- 3.3 Article -->
Adding PRISM to Weka
Before attempting the quiz that follows, you will need to download Weka’s PRISM package.
We explained Weka’s “package” system in the previous course Data Mining with Weka. Here’s a refresher.
Weka has hundreds of different classifiers and scores of filtering algorithms. To simplify the interface, when you download Weka it comes with a small set of key classifiers and filters, along with a simple mechanism that allows you to add new methods that are stored in separate “packages”.
The Package Manager is accessed from the Tools menu in Weka’s GUI Chooser panel, which appears when you start up Weka:
The very first time it is accessed, the Manager downloads information about all available packages. This requires an internet connection.
Here is the resulting display:
The top half is a scrollable list of packages, beneath which is information about the currently selected package – in this case the userClassifier package.
To install (or uninstall) a package, click the install (or uninstall) button near the top of the window. When you do this, you will be asked to close any Weka application windows – like the Explorer. When you reopen it, the new package will be incorporated.
You can display all packages, or just the ones that are installed, or just the ones that are not installed. The list gives the package name, the broad category it belongs to, the version currently installed (if any), and the most recent version. It can be sorted, in ascending or descending order, by clicking the package or the category column header.
For successful operation the Package Manager requires Weka version 3.8.1 or later. If yours predates this you should install the latest version.
Please install the simpleEducationalLearningSchemes package now. It contains PRISM, which you’ll need for the quiz that follows.
Problems? If you have technical difficulties with the installation, raise them in the Discussion step that follows (“What could possibly go wrong?”).

<-- 3.4 Discussion -->
What could possibly go wrong?
Ha!
Hopefully nothing. If you’ve successfully installed the simpleEducationalLearningSchemes package, feel free to ignore this discussion.
But you never know with computers – particularly since Weka’s package manager reaches out to access packages over the internet. For example, if you are operating inside your institution’s firewall Weka may be prevented from accessing the Web because you haven’t supplied a password.
How did it go? It should be easy, but you never know … Please share your experiences, and any tips, with fellow learners. If you have problems, post them. (Don’t forget to include details such as the computer, maybe system version too.) If you’ve figured out the solution, post that. And if you can help someone else, please do so! You’re a community.
OK, I’ll kick it off:
Problem. I installed the userClassifier package and everything seemed to go fine. But when I restart the Explorer and look at the classifiers in the Classify panel, the UserClassifier isn’t there!
Solution. Hmmm. Are you sure you’re looking in the right place? Under trees? Where J48 is? Yes?? … well, maybe it didn’t install. When you clicked the Package manager’s install button, and you were asked to close any Weka application windows, did you close the Explorer, and re-open it once installation was complete? Yes? And, before closing, did you first wait for the package to finish installing (look at the progress bar)? It might take a little time. Computers are very picky about all these things: you have to get it exactly right.

<-- 3.5 Quiz -->
Simple decision rules
Question 1
How many rules are there?
2
3
4
5
---
Correct answer(s):
5
---
Feedback correct:
J48 generates 5 rules for the weather.nominal.arff dataset:
    if outlook = sunny and humidity = high: no
    if outlook = sunny and humidity = normal: yes
    if outlook = overcast: yes
    if outlook = rainy and windy = TRUE: no
    if outlook = rainy and windy = FALSE: yes

<-- 3.5 Quiz -->
Simple decision rules
Question 2
How many tests does the rule set involve?
7
8
9
10
---
Correct answer(s):
9
---
Feedback correct:
The rule set generated from J48’s tree for the weather.nominal.arff dataset  has 9 tests. Count the “=” signs!
    if outlook = sunny and humidity = high: no
    if outlook = sunny and humidity = normal: yes
    if outlook = overcast: yes
    if outlook = rainy and windy = TRUE: no
    if outlook = rainy and windy = FALSE: yes

<-- 3.5 Quiz -->
Simple decision rules
Question 3
Deduce an equivalent set of smaller rules by assuming that the rules will be executed in order – i.e., that they form a decision list. How many tests does the decision list involve?
3
4
5
6
---
Correct answer(s):
5
---
Feedback correct:
In the video, Ian showed this decision list:
outlook = sunny AND humidity = high -> no
outlook = sunny -> yes
outlook = overcast -> yes
windy = false -> yes
-> no
There are 5 rules and 5 tests (5 “=” signs)

<-- 3.5 Quiz -->
Simple decision rules
Question 4
On the same dataset (weather.nominal.arff), run the PRISM algorithm for generating rules. How many rules and tests does it generate?
Select all the answers you think are correct.
5 rules
6 rules
7 rules
8 rules
8 tests
9 tests
10 tests
11 tests
---
Correct answer(s):
6 rules
11 tests
---
Feedback correct:
PRISM generates 6 rules for the weather.nominal.arff dataset
---
Feedback incorrect:
PRISM generates 6 rules for the weather.nominal.arff dataset

<-- 3.5 Quiz -->
Simple decision rules
Question 5
If the order of the rules is shuffled, will PRISM’s rule set have the same effect?
Yes
No
---
Correct answer(s):
Yes

<-- 3.5 Quiz -->
Simple decision rules
Question 6
As you can see, PRISM is not very successful on this example: it generates 6 rules with 11 tests, compared with the 5 rules and 9 tests generated by J48.
In fact, PRISM is only intended to illustrate the principle of rule generation using the separate-and-conquer method; it’s not a practically useful algorithm in general.
Generate the smallest possible rule set for the nominal weather data, by hand, that when executed in sequence classifies the training set perfectly. How many rules does it contain?
2
3
4
6
---
Correct answer(s):
3
---
Feedback correct:
Here are 3 rules, involving 4 tests:
outlook = sunny AND humidity = high -> no
outlook = rainy AND windy = true -> no
-> yes

<-- 3.5 Quiz -->
Simple decision rules
Question 7
Load the contact-lenses.arff dataset and use PRISM to generate a rule set. How many rules and tests does it contain?
Select all the answers you think are correct.
8 rules
9 rules
12 rules
9 tests
21 tests
26 tests
32 tests
---
Correct answer(s):
9 rules
26 tests

<-- 3.5 Quiz -->
Simple decision rules
Question 8
Run J48 on the same dataset (contact-lenses.arff).
The tree it generates makes two errors on the training data, so alter J48’s configuration to force it to generate an exact tree (set minNumObj to 1 and unpruned to true).
Generate rules by reading them off the tree. How many rules and tests does this rule set contain?
Select all the answers you think are correct.
5 rules
9 rules
15 rules
15 tests
24 tests
30 tests
---
Correct answer(s):
9 rules
30 tests

<-- 3.5 Quiz -->
Simple decision rules
Question 8
Run J48 on the same dataset (contact-lenses.arff).
The tree it generates makes two errors on the training data, so alter J48’s configuration to force it to generate an exact tree (set minNumObj to 1 and unpruned to true).
Generate rules by reading them off the tree. How many rules and tests does this rule set contain?
Select all the answers you think are correct.
5 rules
9 rules
15 rules
15 tests
24 tests
30 tests
---
Correct answer(s):

<-- 3.6 Discussion -->
Small rule sets for the contact-lenses data
You saw in the preceding Quiz that for the contact-lenses dataset, reading rules off the J48 tree gives 9 rules and 30 tests, whereas PRISM generates 9 rules and 26 tests.
In this case PRISM is slightly more successful than J48 in minimizing the size of the rule set. But, nevertheless, it sucks: there are far smaller rule sets that classify this dataset exactly. Here’s one with 8 rules and 10 tests that I found by hand:
tear-prod-rate = reduced -> none
astigmatism = no AND age = young -> soft
astigmatism = no AND age = pre-presbyopic -> soft
astigmatism = yes AND spectacle-prescrip = myope-> hard
age = young -> hard
astigmatism = yes -> none
spectacle-prescrip = myope -> none
-> soft
Finding small rule sets is a black art. Can you find a smaller one that classifies the contact-lenses dataset exactly? Tell your colleagues …
Spoiler alert: the Ripper rule learner that we’ll meet in the next video finds a ruleset with just 3 rules! Isn’t that amazing? The moral is: don’t use PRISM for actual data mining, it’s just an educational tool to explain the principles.

<-- 3.7 Video -->
Generating good decision rules 
Here are a couple of schemes for rule learning. The first, called PART, is a way of forming rules from partial decision trees. The second, called Ripper (JRip in Weka), uses incremental reduced-error pruning, followed by a fiendishly complicated global optimization step that’s detailed, complex, unprincipled, but works well and often generates unbelievably small rule sets that do an excellent job. I’ll spare you the details – you really don’t want to know! Both methods are easy to use in Weka.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
In the last lesson, we looked at decision rules versus decision trees. They are kind of similar representations, but they are interestingly different in how they express things. We also looked at a bottom-up covering algorithm for generating rules called PRISM. You will realize that PRISM is not really a terribly good machine learning method. It’s not really meant for serious use. In this lesson, we’re going to look at a couple of schemes for rule learning and show how to use them in Weka. The first scheme we’re going to look at is called PART, and it’s a way of forming rules from partial decision trees.
It’s the basic separate and conquer algorithm: make a rule, remove the instances it covers, and continue creating rules for the remaining instances. To make a rule, PART builds a tree. It builds and prunes a decision tree for the current set of instances, and reads off the rule for the largest leaf – the most important rule, if you like. Then it throws the tree away, and carries on with the covering algorithm. It seems very wasteful, and I suppose perhaps it is a bit wasteful, but it turns out you can build just a partial tree – you don’t have to build a whole tree. That’s how PART works.
The second method is called RIPPER: in fact, in Weka it’s called JRip. It’s a basic incremental reduced-error pruning algorithm. There’s class of algorithms that go by this name. The idea is that PRISM is good at producing rules, but it produces exact rules. Typically, we want to produce rules that are not necessarily exact, but merely very good. What incremental reduced-error pruning does is to take the instances, the training set,
and split them into two sets, one called Grow and one called Prune, in the ratio 2:1. It uses the Grow set for growing rules, adding clauses to rules until you get a perfect rule. Then it uses the Prune set when you’re pruning rules, deleting the clauses from the rule until you’re left with a good rule. For each class, while there are instances of that class in both these sets, we’re going to use PRISM to create the best perfect rule for that class. Then we’re going to calculate the worth of the rule. Now, we need some measure of the “worth” of a rule.
There are different ways of measuring the worth of a rule, and different incremental reduced-error pruning algorithms do different things. For example, you might just use the success rate, or you might use some more complicated thing, perhaps even some entropy metric. Anyway, whatever you do, let’s assume you’ve got a way of measuring the worth of a rule. We calculate the worth of that rule, and then we omit the final condition – the last one that we added – and look at the worth of that. If it’s worthwhile, then we take away that final condition and carry on trying to remove conditions from the rule until we get an optimal version of the rule.
So we build up the rule on the Grow set, and then we prune it back on the Prune set until we get a rule whose worth is good. It turns out it’s better to prune backwards than to prune on the way forwards. Again, it sounds a bit wasteful, but it’s a good idea to build up the whole rule and then prune backwards. Then we just carry on. We select the rule with the largest worth, and we prune it and remove the instances it covers, carrying on with the basic covering algorithm. RIPPER follows this by a fiendishly complicated global optimization step that’s really detailed, really complex, not really very principled, but works really well.
I’m not going to tell you about that. It’s just not worthwhile – you’d never remember it, it’s just too hard to – I mean, I don’t remember it. It’s just really complicated. But this is the basic kind of incremental reduced-error pruning algorithm that it uses to generate the rule set in the first place. All right, let’s go to Weka. I’ve loaded the diabetes dataset. I’m going to try J48, PART, and JRip. So here we are in Weka.
Here’s the diabetes dataset: 768 instances. I go to Classify, and I’ve already run J48. This is the result from J48.
I’ve got a decision tree here, quite a complicated decision tree: it’s got 20 leaves and a total of 39 nodes in this tree. It gets 74% accuracy. PART produces a rule set that looks like this. See these rules – ”If plas <= 127 and mass <= 26.4, etc. then tested_negative” (in this dataset, there are two classes, negative and positive). There’s a rule for negative, and a rule for positive, and so on. This rule set has got 13 rules, involving 25 separate tests.
We get 75% accuracy. RIPPER does really well, 76% accuracy, there at the top. It has only 4 rules. Amazing, eh? In fact, going back to the slide, here are the results, and here are the 4 rules. Actually, RIPPER starts out by taking the majority class – in this case tested_negative – and leaving that to the end. So it only produces rules for the other classes, and then leaves the majority class for the default clause like this. So tested_positive is the smaller class, and tested_negative is the larger class. These are the rules it’s come up with. Only 4 rules, 9 tests, and the best performance of all. That’s pretty amazing. PART is quick and quite an elegant algorithm, really.
Repeatedly constructing decision trees and discarding them is less wasteful than it sounds. Incremental reduced-error pruning is a standard technique.
RIPPER does incremental reduced-error pruning followed by a global optimization step: it usually produces fewer rules than PART.
<End Transcript>

<-- 3.8 Quiz -->
Comparing rule-based classifiers
Question 1
For which datasets does J48 give the best results?
Select all the answers you think are correct.
breast-cancer
credit-g
diabetes
glass
ionosphere
iris
segment-challenge
---
Correct answer(s):

<-- 3.8 Quiz -->
Comparing rule-based classifiers
Question 1
For which datasets does J48 give the best results?
Select all the answers you think are correct.
breast-cancer
credit-g
diabetes
glass
ionosphere
iris
segment-challenge
---
Correct answer(s):
breast-cancer
iris

<-- 3.8 Quiz -->
Comparing rule-based classifiers
Question 2
For which datasets does JRip give the best results?
Select all the answers you think are correct.
breast-cancer
credit-g
diabetes
glass
ionosphere
iris
segment-challenge
---
Correct answer(s):
credit-g
diabetes

<-- 3.8 Quiz -->
Comparing rule-based classifiers
Question 3
For which datasets does PART give the best results?
Select all the answers you think are correct.
breast-cancer
credit-g
diabetes
glass
ionosphere
iris
segment-challenge
---
Correct answer(s):
glass
ionosphere
segment-challenge

<-- 3.8 Quiz -->
Comparing rule-based classifiers
Question 4
Now consider statistical significance at the 5% (default) level.
For which datasets are there no significant differences between JRip and ZeroR?
Select all the answers you think are correct.
breast-cancer
credit-g
diabetes
glass
ionosphere
iris
segment-challenge
---
Correct answer(s):
breast-cancer
credit-g

<-- 3.8 Quiz -->
Comparing rule-based classifiers
Question 5
For which dataset is there no significant difference between JRip and OneR?
[Hint: For this question and the following ones you will have to alter the “Test base” appropriately]
breast-cancer
credit-g
diabetes
glass
ionosphere
iris
segment-challenge
---
Correct answer(s):
iris

<-- 3.8 Quiz -->
Comparing rule-based classifiers
Question 6
For which dataset are there significant differences between any of PART, JRip, and J48?
breast-cancer
credit-g
diabetes
glass
ionosphere
iris
segment-challenge
---
Correct answer(s):
segment-challenge

<-- 3.8 Quiz -->
Comparing rule-based classifiers
Question 7
Now change the “Comparison field” to measureNumRules, which gives the number of rules produced by PART and JRip, and the number of leaves produced by J48.
For which datasets does JRip produce the smallest number of rules?
[Hint: For this question and the next two ones you will need to ensure that the “Test base” is set to one of PART, JRip, J48]
Select all the answers you think are correct.
breast-cancer
credit-g
diabetes
glass
ionosphere
iris
segment-challenge
---
Correct answer(s):
breast-cancer
credit-g
diabetes
glass
ionosphere
iris
segment-challenge

<-- 3.8 Quiz -->
Comparing rule-based classifiers
Question 8
For which datasets does PART produce fewer rules than the number of leaves that J48 generates?
Select all the answers you think are correct.
breast-cancer
credit-g
diabetes
glass
ionosphere
iris
segment-challenge
---
Correct answer(s):
credit-g
diabetes
glass
ionosphere
iris
segment-challenge

<-- 3.8 Quiz -->
Comparing rule-based classifiers
Question 9
Now consider significance at the 5% (default) level.
For which datasets does JRip produce significantly fewer rules than PART?
Select all the answers you think are correct.
breast-cancer
credit-g
diabetes
glass
ionosphere
iris
segment-challenge
---
Correct answer(s):
breast-cancer
credit-g
diabetes
glass
ionosphere
segment-challenge

<-- 3.8 Quiz -->
Comparing rule-based classifiers
Question 10
Now that you’ve got everything set up in the Experimenter there’s one final thing that is easy to do, and very instructive; which is to examine the speed of the classifier algorithms you’ve been investigating.
Let’s think about it first.
You have been evaluating 5 methods: ZeroR, OneR, PART, JRip and J48. Which 2 do you think are the fastest for training?
ZeroR and PART
ZeroR and OneR
J48 and OneR
---
Correct answer(s):
ZeroR and OneR
---
Feedback correct:
These methods are by far the simplest

<-- 3.8 Quiz -->
Comparing rule-based classifiers
Question 11
Which 2 do you think are the fastest for testing?
ZeroR and JRip
ZeroR and OneR
J48 and OneR
---
Correct answer(s):
ZeroR and OneR
---
Feedback correct:
Once again, these methods are by far the simplest

<-- 3.8 Quiz -->
Comparing rule-based classifiers
Question 12
Do you think J48 is faster than PART for training?
Yes
No
---
Correct answer(s):
Yes
---
Feedback correct:
J48 produces just one decision tree whereas PART produces several

<-- 3.8 Quiz -->
Comparing rule-based classifiers
Question 13
Do you think JRip is faster than PART for testing?
Yes
No
---
Correct answer(s):
Yes
---
Feedback correct:
JRip produces smaller rule sets, which are quicker to test

<-- 3.8 Quiz -->
Comparing rule-based classifiers
Question 14
In the Experimenter, select UserCPU_Time_training as the “Comparison field”. Set the Mean Precision under “Output Format” to 6 (decimal places), and check Show Average (also under “Output Format”).
Perform the test and look at the average training time taken by these classifiers: ZeroR, OneR, PART, JRip and J48. What order do they come in?
---
Correct answer(s):
ZeroR
oneR
J48
PART
JRIP
---
Feedback correct:
(Note that the times might differ slightly on your computer, so what’s correct for me may not necessarily be correct for you.)

<-- 3.8 Quiz -->
Comparing rule-based classifiers
Question 15
Change the Comparison field to UserCPU_Time_testing. What order do they come in now (fastest to slowest)?
JRip, OneR, J48, PART, ZeroR
J48, ZeroR, OneR, JRip, PART
ZeroR, J48, OneR, JRip, PART
ZeroR, J48, JRip, OneR, PART
J48, ZeroR, JRip, OneR, PART
JRip, OneR, J48, ZeroR, PART
None of the above
---
Correct answer(s):
None of the above
---
Feedback correct:
It’s hard to get this question wrong! I was surprised to find many different permutations of UserCPU_Time_testing ordering when I tried this on different computers. So all answers are marked correct!!!

<-- 3.9 Article -->
What if there's no "class" attribute?
This course so far – like it’s predecessor, Data Mining with Weka – has focused solely on tasks whose aim is to predict the value of a particular attribute called the “class”.
When the class is nominal, this is called classification; when it’s numeric, it’s called regression (why? – don’t ask, it’s kinda crazy). Both tasks are sometimes called “supervised” learning, the idea being that there is a supervisor (or teacher) who dictates what the correct class should be.
But what can you do with a dataset that has no class value? One idea is to seek associations between any of the attributes, or between any set of attributes. Associations are invariably expressed as rules, and this is called “association rule mining”. Another idea is to see if the instances fall into natural groups, a task known as “clustering”. In both cases it’s pretty hard to evaluate the result in objective ways – unlike classification, where the gold standard is to predict the class correctly on fresh data.
This week we’ll examine both of these tasks. By the end you will be able to apply association rule mining to a dataset and seek interesting associations. For any rule you’ll be able to calculate the key parameters of support and confidence. And you’ll have experienced some of the limitations of association rule mining and how difficult it can be to find interesting patterns in data.
You’ll also be experienced in using different clustering methods, and will have learned to be sceptical if the results look too good! And you’ll be able to evaluate clusterings using the classification-by-clustering method.

<-- 3.10 Video -->
Association rules 
Association rule learners find associations between attributes. Between any attributes: there’s no particular class attribute. Rules can predict any attribute, or indeed any combination of attributes. To find them we need a different kind of algorithm. “Support” and “confidence” are two measures of a rule that are used to evaluate them, and rank them. The most popular association rule learner, and the one used in Weka, is called Apriori.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Association rules are about finding associations between attributes. Between any attributes. There’s no particular class attribute. Rules can predict any attribute, or indeed any combination of attributes. For this we need a different kind of algorithm. The one that we use in Weka, the most popular association rule algorithm, is called Apriori. I don’t know if you remember the weather data from Data Mining with Weka. Here’s this little dataset with 14 instances and a few attributes. Well, here are some association rules. “If outlook=overcast, then play=yes.” If you look at that, there are 4 “overcast” instances, and it’s “yes” for all of
them: that rule is 100% correct. “If temperature=cool, then humidity=normal”; that’s also 100% correct. “If outlook=sunny and play=no, then humidity=high.” We don’t have to predict “play” or indeed any particular attribute. If you look at rule #4, “outlook=sunny and play=no,” the first 2 instances satisfy that rule, and there are no other instances that satisfy that rule. So it’s 100% correct, but it only covers 2 instances. There are lots of 100% correct rules for the weather data. I think there are 336 rules that are 100% correct. Somehow we need to discriminate between these rules. The way we’re going to do this is to look at the “support”, the number of instances that satisfy a rule.
The “confidence” is the proportion of instances for which the conclusion holds, and the “support” is the number of instances that satisfy a rule. Here I’ve got the same rules. They all have 100% confidence, but they’ve got different degrees of support, different numbers of instances. We’re looking for high support/high confidence rules, but we don’t really want to specify 100% confidence and look for all of those rules, because, like I said, there are hundreds of them and a lot of them have very low support. Typically what we do is specify a minimum degree of confidence and seek the rules with the greatest support with that minimum confidence. I want to introduce you to the idea of an “itemset”.
An itemset is a set of attribute-value pairs, like “humidity=normal and windy=false and play=yes”. An itemset has got a certain support given a dataset. Here there are 4 instances in the dataset that are in that itemset. We can take that itemset and permute it in 7 different ways to produce rules, all of which have a support of 4. “If humidity=normal and windy=false than play=yes” has a support of 4 and a confidence of 4/4 – that’s 100% – because all of the instances for which humidity=normal and windy=false have play=yes. As we go down this list of rules, we get a lower degree of confidence.
The last rule, for example, doesn’t have anything on the left-hand side: “anything implies humidity=normal, windy=false and play=yes” has a support of 4, but there are 14 instances that satisfy the left-hand side. All of the instances satisfy the left-hand side, so the confidence is 4/14. You can see that as you go down this list of rules, the confidence is decreasing from 100% through 4/6 (67%) down to quite a low value, 4/14. What Apriori does is generate high-support itemsets. Then, given an itemset, it gets all the rules from it, and just takes those with more than a minimum specified degree of confidence.
The strategy is to iteratively reduce the minimum support until the required number of rules is found with a given minimum confidence. That’s it for this lesson. There are far more association rules than classification rules. We need different techniques. The “support” and “confidence” are two important measures. Apriori is the standard algorithm, and I just want to show you that algorithm over here in Weka. In order to use Apriori, I go to the Associate panel. There are a few association rule algorithms, of which by far the most popular is Apriori; that’s the default one. Then I just run that to get association rules.
We want to specify the minimum confidence value and seek rules with the most support, and the details of that are in the next lesson.
<End Transcript>

<-- 3.11 Quiz -->
Association rules
Question 1
Examine the rules that are generated. None of them involve Class=republican. Why is this?
There are no instances classed as republican
The minimum support value is too high
There are no associations between the attributes and Class=republican
The support for the top 10 rules exceeds the number of instances with Class=republican
The class value cannot be used in a rule
---
Correct answer(s):
The support for the top 10 rules exceeds the number of instances with Class=republican
---
Feedback correct:
The support for the top 10 rules (in order) is 219, 198, 211, 202, 247, 200, 208, 203, 204, and 218. However, only 168 instances are classed as republican. Thus Class=republican could never appear in rules with such great support.
---
Feedback incorrect:
168 instances are classed as republican
---
Feedback incorrect:
If you generate, say, 100 rules with the same support value you will find that rules 77, 97 and 100 involve Class=republican.
---
Feedback incorrect:
There are very many such associations. It’s just that none of them fall in the top 10. For example, if you generate 100 rules with the same support value you will find that rules 77, 97 and 100 involve Class=republican.
---
Feedback incorrect:
In association rule mining there does not have to be a class attribute. But if there is one, it is treated just like any other attribute.

<-- 3.11 Quiz -->
Association rules
Question 2
What is the single attribute most strongly associated with democrats?
el-salvador-aid
physician-fee-freeze
adoption-of-the-budget-resolution
aid-to-nicaraguan-contras
education-spending
---
Correct answer(s):
physician-fee-freeze
---
Feedback correct:
It’s mentioned in the top 5 rules, and in 7 of the top 10 rules

<-- 3.11 Quiz -->
Association rules
Question 3
Use OneR to confirm that physician-fee-freeze is the attribute most strongly associated with democrats. What percentage accuracy do you get (evaluated using cross-validation)?
61%
95%
96%
97%
---
Correct answer(s):
96%
---
Feedback correct:
This is OneR’s accuracy for predicting the Class (democrat vs. republican) from physician-fee-freeze

<-- 3.11 Quiz -->
Association rules
Question 4
What is the single attribute most strongly associated with voting for aid to the Nicaraguan Contras?
el-salvador-aid
physician-fee-freeze
Class
adoption-of-the-budget-resolution
education-spending
---
Correct answer(s):
el-salvador-aid
---
Feedback correct:
el-salvador-aid is involved in both rules that predict aid-to-nicaraguan-contras, rules 6 and 7

<-- 3.11 Quiz -->
Association rules
Question 5
Use OneR to confirm that el-salvador-aid is the attribute most strongly associated with aid-to-nicaraguan-contras. What percentage accuracy do you get (evaluated using cross-validation)?
57%
58%
91%
92%
---
Correct answer(s):
91%
---
Feedback correct:
This is OneR’s accuracy for predicting aid-to-nicaraguan-contras from el-salvador-aid

<-- 3.11 Quiz -->
Association rules
Question 6
The third rule generated has confidence 1. But in reality this is a rounded figure. Use a calculator to determine the confidence of that rule to 5 decimal places. What is it?
0.91526
0.99525
0.99526
0.99527
---
Correct answer(s):
0.99526
---
Feedback correct:
This is obtained by dividing the support count for {physician-fee-freeze, aid-to-nicaraguan-contras, Class=democrat} (which is 210) by the support count for {physician-fee-freeze, aid-to-nicaraguan-contras} (which is 211)

<-- 3.11 Quiz -->
Association rules
Question 7
Look at the third rule generated, call it Rule A. Now increase the number of rules generated to 100 and run Apriori again. What number is Rule A now?
3
4
8
10
11
---
Correct answer(s):
8
---
Feedback correct:
In the next Activity we will learn why this rule changes its position. In brief, the reason is that when seeking 10 rules, the minimum support that Apriori considers is 196 instances, but when seeking 100 rules this is reduced to 174 (as shown at the beginning of the Apriori output). This introduces rules with higher confidence than Rule A but smaller support.

<-- 3.11 Quiz -->
Association rules
Question 7
Look at the third rule generated, call it Rule A. Now increase the number of rules generated to 100 and run Apriori again. What number is Rule A now?
3
4
8
10
11
---
Correct answer(s):

<-- 3.12 Video -->
Learning association rules 
Apriori’s strategy for generating association rules is to specify a minimum “confidence” and iteratively reduce “support” until enough rules are found. Doing this efficiently involves an interesting and subtle algorithm. “Market basket analysis” is often considered a prime candidate for association rule mining, and we look at some data that records the contents of shopping baskets in a real New Zealand supermarket.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
We left off last lesson looking at an itemset with 3 items and a support of 4, which means that there are 4 instances in the dataset for which those conditions are true. This itemset can be massaged into 7 possible rules by choosing different things on the left- and right-hand side. I said that the strategy of Apriori is to specify the minimum confidence level, and then iteratively reduce the support until enough rules are found with greater than this confidence. These rules have got support 4 and confidence values ranging from 100%, 4/4, to whatever 4/14 is. For the weather data, Apriori will first generate itemsets with support 14. There aren’t any of those.
If there were, it would find rules in them with greater than the minimum confidence level; the default for Weka is 90% confidence level. Since there weren’t any itemsets with support 14, it would decrease the support to 13 and carry on decreasing it until it had found sufficient rules to satisfy the specified number of rules. Actually, the weather data has 336 rules with confidence of 100%. The reason why it works in this slightly crazy way is that if you started looking at high confidence rules, then you’d find huge numbers of very low support, very high confidence rules. From a large dataset, you’d have massive numbers of 100% confidence rules that weren’t very interesting because they had tiny support.
That’s why it does this. Let’s go over to Weka. I’ve opened the weather data, the 14-instance weather data. I’m going to go to Associate and run Apriori, that’s the default association rule learner. This is the output I get.
Here are the 10 rules; the default number of rules is 10. You can see this is the support of these rules, and it ranges from 4 down to 3 down to 2. These last 2 rules only have 2 instances in the dataset that satisfy those rules. The rules are all 100% confident. Let’s go back to the slide here. We specify the minimum confidence level; the default is 90%. We specify how many rules we want; the default is 10. We express the support as the proportion of the number of instances. Then we run the Apriori algorithm several times. We start at 100% support, and decrease it by 5% each time.
We stop when we get the right number of rules with more than the minimum confidence level, or we would stop when we reached another parameter, lowerBoundMinSupport. So there are quite a lot of parameters here. Let’s take a look at how this works for the weather data. On the output that I just showed you, it said that the minimum support of the top – it specifies the minimum support – is 0.15. 0.15 times 14 instances is 2 – 0.15 is the proportion of the total number of instances. The minimum confidence is 0.9; that was set as the default parameter. It actually performed 17 cycles, reducing the support each time.
Just looking down at the bullet point underneath, the 17 cycles corresponded to having a support of 100%, and then it reduces by 5% each time – 95%, 90%, 85%, and so on. It actually got right down to 15%. When you translate those percentages into a number of instances, it was using 14 instances, 13 instances, then it did it again with 13 instances. It’s a little crazy on this tiny dataset – it’s doing a bit of extra work – but on a large dataset that wouldn’t happen. It got down to 3 instances at the 20% level, and it only found 8 rules with confidence greater than 90% and support 3.
That’s why it was forced to go down to a support of 2.
What are these itemsets? Well, we can look at the itemsets. The itemsets are based on the final support values. Let’s go back to Weka and have a look at the itemsets. Here are the parameters for Apriori.
As I said before, there are quite a few of them. This is the amount by which the support is reduced each time, by 5%. This is when it stops, when it gets to a support of 10%. It’s looking for rules with a confidence greater than 90%. It’s looking for 10 rules. It’s starting as a support of 100%, which is normally what one does. Here we can output the itemsets, so let’s output the itemsets, and run it again. We’ve got the same rules, and here are the itemsets. These are the itemsets with support of at least 2. The way it works is it starts with itemsets with just 1 item, and these are the support for each of these itemsets.
Then it adds new conditions to these to generate 2-item itemsets. This L1 here, these are itemsets with 1 item, and down here, these are itemsets with 2 items. That’s how it generates the itemsets. It’s a little bit convoluted, and it does this for efficiency reasons for large datasets. Here are the itemsets with 3, and here are the itemsets with 4 conditions in them. In fact, there weren’t any itemsets with more than that with this support level. Coming back to the slide, there were 12 1-item sets with support of at least 2. There were 47 2-item sets, 39 3-item sets, 6 4-item sets, and 0 5-item sets, which is where it stopped.
That’s how it goes through this, and for each itemset it converts it into rules and looks for rules with greater than the minimum confidence. Here it ended up with the 10 rules that we saw before. It’s a little bit complicated, but as I say, it’s done for efficiency reasons. There are some other parameters. In the Weka implementation, “car”, class attributes, always produces rules that predict the class attribute. You can filter rules according to a statistical test, a Chi-squared statistical test, but that’s actually unreliable because we’re making a very large number of statistical tests here and significant results will be found just by chance. We’ve talked about confidence, but there are different metrics that can be used for ranking rules.
You can also remove all attributes whose values are all “missing”. Those are extra parameters. You’re going to look at the supermarket data and do some market basket analysis. This data was collected from an actual New Zealand supermarket.
Let’s just go and have a quick look at this: supermarket.arff. Here it is.
You can see there are departments here: there’s baking needs, and coupons, and tea, and biscuits – very popular in New Zealand – frozen foods, razor blades, gardening aids, spices. A large number of attributes, 217 attributes, and 4,600 instances in this dataset. There are 1 million attribute values if you multiply those numbers together. In this dataset, missing values are used to indicate that the basket did not contain that item. In fact, 92% of the values are missing. That means that the average basket contains 220 attributes times 8%, that’s only 18 items – the average number of items in your supermarket basket. The most popular items are bread-and-cake, vegetables, frozen foods, and biscuits. That’s Apriori.
It makes multiple passes through the data generating 1-item sets, 2-item sets, and so on, with more than the minimum support. It turns each one into rules and checks their confidence. It’s fast and efficient, providing that the data fits into main memory. Weka invokes the algorithm several times, gradually reducing the support each time until sufficient high-confidence rules have been found. There are many parameters that control this iteration.
<End Transcript>

<-- 3.13 Article -->
Try some market basket analysis
Try this (instead of a quiz).
Market basket analysis aims to discover interesting purchasing patterns in large datasets of transactional records. Typically these are the contents of individual shoppers’ baskets in a supermarket, recorded at the checkout. Interesting patterns could be exploited in the store, such as special offers and product layout.
As Ian mentioned in the video, the “supermarket” dataset (supermarket.arff) is a real world transaction data set from a small NZ supermarket. Each instance represents a customer transaction – products purchased and the departments involved. The data contains 4,500 instances and 220 attributes. Each attribute is binary and either has a value (t for true) or no value (“?” for missing).
The attributes are aggregated to the department level, so, for example, “bread and cake” covers several different products, and a value of t indicates that the customer’s shopping cart contained at least one product from that department. (Unfortunately, not all departments are named.)
There is a nominal class attribute called total that indicates whether the transaction was less than $100 (low) or greater than $100 (high). However, we are not aiming to create a predictive model for total. Instead, we are interested in what items were purchased together. The aim is to find useful patterns in the data that may or may not be related to the predicted attribute.
Load the file supermarket.arff into the Explorer and use Apriori to mine this supermarket checkout data for associations. See if you can discover anything interesting.
(The point of this exercise is to show you how difficult it is to find any interesting patterns in this kind of data!)

<-- 3.14 Article -->
Here's what I did
The top 10 rules involve total = high and predict bread-and-cake, supported by 723 transactions.
  They all have a consequent of “bread and cake”
  They all indicate a high total transaction amount
  “biscuits” and “frozen foods” appear in many of them.
You have to be careful about interpreting association rules. They are merely associations, not necessarily causal relations. If we are interested in total, for example, should we try to convince people who already buy biscuits, frozen foods and fruit to buy bread and cake as well? – because according to the above rule this combination tends to be associated with a high total transaction amount. This is flawed reasoning: the product combination does not cause a high total. Those 723 transactions probably include a vast assortment of other items, in addition to those mentioned in the rule.
However, an interesting exercise might be to model the path through the store required to collect associated items and see whether changes to that path (shorter, longer, displayed offers, etc) have an effect on transaction amount or basket size.
I continued as follows.
  Remove the total attribute. The top 10 rules still all predict bread-and-cake, so remove bread-and-cake
  No rules found, so reduce lowerBoundMinSupport to 0.05
  The top 10 rules now all predict vegetables
    The most interesting rule is
     beef=t fruit=t potatoes=t 287 ==> vegetables=t 273 conf: 0.95
  Remove vegetables. Now everything predicts frozen foods, so remove it
  Only one rule resulted, so I reduced lowerBoundMinSupport further, to 0.025
  Now the top 10 rules predict biscuits, so remove it
    Now the top 10 rules predict baking needs, with one exception:
     laundry needs=t wrapping=t dental needs=t prepared meals=t 132 ==> tissues-paper prd=t 125 conf:(0.95)
  Remove baking needs. Now the top 10 rules predict tissues-paper prd, so remove it
  The top rules now predict sauces-gravy-pkle, margarine, fruit
Here are the resulting rules:
1. canned vegetables=t puddings-deserts=t party snack foods=t cheese=t
      fruit=t 126 ==> sauces-gravy-pkle=t 116 conf:(0.92)
2. canned vegetables=t puddings-deserts=t party snack foods=t cheese=t 
      margarine=t 134 ==> sauces-gravy-pkle=t 123 conf:(0.92)
3. juice-sat-cord-ms=t canned vegetables=t breakfast food=t 
      sauces-gravy-pkle=t jams-spreads=t cheese=t 131 ==> margarine=t 120 
      conf:(0.92)
4. juice-sat-cord-ms=t canned fruit=t canned vegetables=t milk-cream=t 
      department137=t 141 ==> fruit=t 129 conf:(0.91)
5. canned fruit=t canned vegetables=t sauces-gravy-pkle=t jams-spreads=t 
      cheese=t 129 ==> margarine=t 118 conf:(0.91)
6. canned vegetables=t confectionary=t party snack foods=t wrapping=t 
      cheese=t 128 ==> sauces-gravy-pkle=t 117 conf:(0.91)
7. juice-sat-cord-ms=t canned vegetables=t sauces-gravy-pkle=t 
      jams-spreads=t party snack foods=t cheese=t 148 ==> margarine=t 135 
      conf:(0.91)
8. juice-sat-cord-ms=t canned vegetables=t puddings-deserts=t 
      party snack foods=t cheese=t 133 ==> sauces-gravy-pkle=t 121 
      conf:(0.91)
9. canned vegetables=t breakfast food=t sauces-gravy-pkle=t jams-spreads=t 
      party snack foods=t cheese=t 133 ==> margarine=t 121 conf:(0.91)
10. juice-sat-cord-ms=t canned fruit=t canned vegetables=t jams-spreads=t 
      cheese=t 128 ==> margarine=t 116 conf:(0.91)
At this point I became bored and gave up. I’m not very interested in supermarkets. And here in New Zealand I would be very unlikely to buy canned vegetables, so none of these rules would apply to me.
The following Discussion step invites you to share what you did with this data, along with your thoughts on market basket analysis in general.

<-- 3.15 Discussion -->
Market basket analysis
Do you have any thoughts on market basket analysis in general, or the supermarket dataset in particular, that you’d like to share with others?
Now’s your chance!
I’m certainly not an expert on association rule mining, so you might find better rules, and better interpretations, than me.

<-- 3.16 Video -->
Representing clusters 
With clustering, there’s no “class” attribute: we’re just trying to divide the instances into natural groups or “clusters”. There are different ways of representing clusters. Are they disjoint, or can they overlap? Is cluster membership precisely determined, or probabilistic? Perhaps a tree structure in which clusters are repeatedly refined into smaller ones is appropriate? Different algorithms produce different representations. In any case, it’s hard to evaluate clustering, because, lacking a class value, you can’t compute the accuracy of any particular clustering.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello again! We’re going to talk about clustering, and I’m going to show you some different clustering methods.
With clustering, there’s no “class” attribute: we’re just trying to divide the instances into natural groups, or “clusters”. For instance, imagine the iris dataset that we looked at in the last course. Imagine deleting the class attribute. Of course (you might remember) there are 3 kinds of iris, and in the iris dataset there
are 50 of each kind: iris setosas, iris versicolors, and iris virginicas. The dataset gives the petal length, and petal width, and so on.
If you deleted the “class” attribute, the question is: could you recover the 3 classes by clustering the data? You’ll be trying that in the activity [quiz] after this lesson. The different kinds of clustering algorithms produce different sorts of representations of clusters. One way of thinking about clusters is to imagine disjoint sets. We take the instance space and divide it into sets such that each part of the instance space is in just one cluster. Or the clusters might overlap, as shown in the second picture. You might have overlapping clusters. If you have overlapping clusters, you might have probabilistic assignment of instances. So a, b, c, d, e, are instances and there are 3 clusters here.
Instance a has 0.4 probability of belonging to cluster 1, and 0.1 probability for cluster 2, and 0.5 probability for 3. Fourthly, we might have a hierarchical clustering method. Here the instances are along the bottom and a and g get clustered together at the bottom level. In fact, you can see the clusters at the bottom level. Then these clusters join together at the next level up, and so on, until at the very top level all the dataset is just one big cluster. It’s called a dendrogram, that kind of tree.
The first algorithm we’re going to look at is called KMeans: it does iterative distance-based clustering.
First of all you specify the desired number of clusters: we call that k. Then the algorithm chooses k points at random as cluster centers. It assigns all the instances in the dataset to their closest cluster center. Then it takes each cluster and calculates the centroid of the instances in it – that’s the mean of all of the instances. These centroids are new cluster centers. It goes back to the beginning and carries on until the clusters centers don’t change. At some point when you re-calculate the cluster centers you get just the same numbers you had before. This algorithm minimizes the total squared distance from instances to their cluster centers. Unfortunately, it’s a local minimum, not a global minimum.
You get different results with different random number seeds.
We’re going to look at it in Weka: we’re going to look at KMeans clustering. I’ve opened the weather dataset, and on the Cluster panel I’m going to open SimpleKMeans.
It’s got some parameters here: the number of clusters, we’ve set this for 2 clusters; we can have different distance functions; and the random number seed here. Let’s just run it. Here we get 2 clusters. One’s got 9 instances and the other’s got 5 instances. The total squared error is 16.2 – that’s what we’re trying to minimize. The thing is that if we run this with a different random number seed, say 11, then we’re going to get different clusters. There are 6 instances in one cluster and 8 in another cluster. Here the total squared error is 13.6. If we were to do it again with another seed, let’s say 12, we get a different clustering again.
Going back to the slide, you can see that for each different random number seed we get a different clustering, and that doesn’t seem like a very good thing. Although maybe it’s the dataset. Maybe with a proper dataset we might get better results, more consistent results. But in KMeans, it’s always dependent on the initial assignment of the cluster centers, the initial choice of cluster centers. XMeans, also in Weka, is an extended version of KMeans. It selects the number of clusters itself, whereas with KMeans you’ve got to specify that. For XMeans, you can specify a minimum and a maximum for this number. It uses kD-trees, which are sophisticated data structures, to make it operate pretty quickly.
Unfortunately, though, XMeans cannot handle nominal attributes. Let’s look at another method. EM is a probabilistic clustering method. It stands for Expectation Maximization. I’m going to go to the cluster panel and choose EM. There are some parameters here. We’ve got here the number of clusters. That’s set to –1; that means EM will try to determine the number of clusters itself. I’m going to set that to 2. Then I’m going to run EM. Here I get 2 clusters. In fact, these are the clusters. For each of the attribute values, I get kind of a probability that the “outlook” is going to be “sunny” and “overcast” and “rainy”, in each of the clusters.
We get the probability by dividing this by the total here. Given those probabilities, if we had a new instance we could calculate for it the probabilities for each cluster. Actually, EM uses as an overall quality measure, a thing called the “log likelihood”. Back to the slide, we’ve got two clusters with these prior probabilities. Within each cluster, we’ve got the probability of each value for a nominal attribute; and for numeric attributes we’ve got the mean and standard deviation. Let’s look at another, final clustering method. This is a hierarchical clustering method called Cobweb. Back in Weka, let me just run Cobweb. It’s got some rather magic parameters here. I’m going to choose 0.3.
It’s a bit of a black art, actually, using Cobweb. I’m going to run this. I’ll get a tree, which I can visualize – on the right-click method I can visualize this tree. Going back to the slide, this is the tree that we get for the weather data, with 10 clusters. You can see these clusters at the bottom level, and then these clusters at the level above, and one cluster at the very top. That’s clustering. In clustering, there’s no class value. There are different representations of clusters, and different algorithms produce different kinds of representation. KMeans is the simplest, standard clustering method. It’s an iterative, distance-based method. It can take different distance metrics.
We were using the Euclidean distance, but you can select different distances metrics in KMeans and XMeans. It’s really hard to evaluate clustering, and we’re going to be looking at that in the next lesson.
<End Transcript>

<-- 3.17 Quiz -->
Clustering the Iris data
Question 1
How many clusters are generated?
1
2
3
4
5
---
Correct answer(s):
2
---
Feedback correct:
This is the default value for SimpleKMeans.

<-- 3.17 Quiz -->
Clustering the Iris data
Question 2
Configure SimpleKMeans to generate 3 clusters and run again. How many instances are there in each cluster?
33
50
100
150
---
Correct answer(s):
50

<-- 3.17 Quiz -->
Clustering the Iris data
Question 3
The result looks good – suspiciously good. Why do you think this is?
The class values are evenly distributed.
The class attribute is included in the data that is being clustered.
The number of clusters is optimal.
---
Correct answer(s):
The class attribute is included in the data that is being clustered.
---
Feedback correct:
The class value splits the instances into three natural clusters, corresponding to the three types of iris.

<-- 3.17 Quiz -->
Clustering the Iris data
Question 4
Remove the class attribute (use “Ignore attributes” on the Cluster panel) and run SimpleKMeans again. How many instances are there in the clusters (in the order in which Weka shows them)?
39, 61, 50
50, 61, 39
61, 39, 50
61, 50, 39
---
Correct answer(s):
61, 50, 39

<-- 3.17 Quiz -->
Clustering the Iris data
Question 5
Call the clusters generated by SimpleKMeans KM0, KM1, KM2, and make a note of the coordinates of the three cluster centers for future reference (to 1 decimal place).
Now run a second clusterer, XMeans. This is not in the Weka core but in a package called “XMeans”, which you will need to install first. (If you need help, refer to the instructions for adding PRISM earlier this week.)
How many clusters are generated?
1
2
3
4
5
---
Correct answer(s):
2

<-- 3.17 Quiz -->
Clustering the Iris data
Question 6
Constrain XMeans to generate 3 clusters and run again. How many instances are there in the clusters (in the order in which Weka shows them)?
48, 50, 52
50, 48, 52
52, 48, 50
52, 50, 48
---
Correct answer(s):
52, 48, 50

<-- 3.17 Quiz -->
Clustering the Iris data
Question 7
Call the clusters XM0, XM1, XM2, and make a note of the coordinates of the three cluster centers for future reference (again to 1 decimal place).
Now run a third clusterer, EM. How many clusters are generated?
1
2
3
4
5
---
Correct answer(s):
5

<-- 3.17 Quiz -->
Clustering the Iris data
Question 8
Constrain EM to generate 3 clusters and run again. How many instances are there in the clusters (in the order in which Weka shows them)?
48, 50, 64
52, 50, 48
64, 50, 36
64, 52, 36
---
Correct answer(s):
64, 50, 36
---
Feedback incorrect:
You may find
2.12 Activity: Comparing AUCs 
useful.

<-- 3.17 Quiz -->
Clustering the Iris data
Question 9
Call the clusters EM0, EM1, EM2, and make a note of the coordinates of the three cluster centers (again to 1 decimal place).
Although the number of instances in each cluster is not exactly the same, there is a pretty good correspondence between the cluster centers for each of the three algorithms. What is the correspondence?
KM0 = XM0 = EM1
KM1 = XM2 = EM2
KM2 = XM1 = EM0
KM0 = XM0 = EM0
KM1 = XM1 = EM1
KM2 = XM2 = EM2
KM0 = XM0 = EM0
KM1 = XM1 = EM2
KM2 = XM2 = EM1
KM0 = XM1 = EM0
KM1 = XM2 = EM1
KM2 = XM0 = EM2
---
Correct answer(s):
KM0 = XM1 = EM0
KM1 = XM2 = EM1
KM2 = XM0 = EM2

<-- 3.17 Quiz -->
Clustering the Iris data
Question 10
Run the Cobweb clusterer. How many leaf clusters are generated?
1
2
3
4
5
---
Correct answer(s):
2
---
Feedback incorrect:
You may find
1.19 Prepare for the quiz
useful.

<-- 3.17 Quiz -->
Clustering the Iris data
Question 11
Cobweb has two parameters, acuity and cutoff. With the default cutoof value, which settings for acuity generate 3 leaf clusters?
Select all the answers you think are correct.
0.5
0.6
0.7
0.8
0.9
1.0
---
Correct answer(s):

<-- 3.17 Quiz -->
Clustering the Iris data
Question 11
Cobweb has two parameters, acuity and cutoff. With the default cutoof value, which settings for acuity generate 3 leaf clusters?
Select all the answers you think are correct.
0.5
0.6
0.7
0.8
0.9
1.0
---
Correct answer(s):
0.6
0.7
---
Feedback correct:
Setting acuity and cutoff for Cobweb is a black art!
---
Feedback incorrect:
Setting acuity and cutoff for Cobweb is a black art!

<-- 3.18 Video -->
Evaluating clusters 
Different clustering algorithms use different metrics for optimization internally, which makes the results hard to evaluate and compare. Weka allows you to visualize clusters, so you can evaluate them by eye-balling. More quantitative evaluation is possible if, behind the scenes, each instance has a class value that’s not used during clustering. That makes classification via clustering possible, and the resulting classifier can then be evaluated in standard ways, such as cross-validation.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
In the last lesson we looked at some different clustering algorithms, and each of them had a different metric. SimpleKMeans talked about the total squared distance of each instance from its cluster center. That’s not necessarily a good way of evaluating clustering, and it certainly makes it difficult to compare the results of different clustering algorithms. One thing we can do in Weka is to visualize the clusters. Over here in Weka I’ve got the iris data open. I’ve got here the SimpleKMeans method with 3 clusters selected, and I’m going to run that.
On the right-click menu, I’m going to visualize the cluster assignments. Here they are. This would make most sense if we plot the cluster against the instance number.
Remember the iris data: the first 50 instances are one kind of iris, and the next 50 are another, and the third 50 are another. Well, this looks too good to be true. Here the first 50 are in one cluster, the second 50 are in another cluster, and the third 50 are in another cluster. And in data mining, if things look too good to be true, they probably are. The problem here, when you think about it, is that one of the attributes is the “class”, and it’s not really fair to include the class when we’re doing the clustering. On the clustering panel we can ignore attributes. I’m going to ignore the “class” attribute and try again.
Now I’ve got 61 instances in one cluster and 50 in another and 39 in another. If I visualize the cluster assignments and choose the cluster here, I get a different picture. You can see that the first cluster looks pretty good, but there are some errors here, some green things have crept into this second thing. For the last 50 items of the dataset, which all belong to one class of iris, we’ve got a whole bunch of stuff coming in here from another [class]. That’s not looking so good. How do you tell which instances are in which cluster? To do that, there’s a filter called “AddCluster”. It’s an unsupervised attribute filter called AddCluster. In this filter, we can specify a clusterer.
Here we specified SimpleKMeans, and I’ll choose 3 clusters again. I’m going to apply the filter, and that’s going to add a new attribute. Let’s do this. You can see that we’ve got a new attribute. It’s called “cluster”, attribute 6. If we edit this dataset, we can have a look at the values for the last attribute and compare them with the class. This is an unsupervised filter, so the class was not used when running the filter. The clustering is done just on the basis of the first four attributes. You can see that the iris-setosas are all in cluster 2. The next lot of irises, versicolors, are mostly in cluster 1 – there are a couple of cluster 3’s here.
The third lot, the iris-virginicas, are mostly in cluster 3, but there are quite a lot of cluster 1’s. That’s just exactly what we saw when we visualized the cluster assignments before. Coming back to the slide, we’ve looked at the Visualize cluster assignments on the Cluster panel. We’ve learned how to ignore attributes. Typically the class attribute is a good one to ignore if you’ve got a dataset with a class. Then we’ve looked at a filter, the AddCluster unsupervised attribute filter. We looked at the result of that and how you can add a new attribute which gives a cluster number, and then look at which instances have got which cluster by clicking the Edit button.
A way of evaluation in Weka is called the “classes-to-clusters evaluation”. I’m going to go back to the iris data and do a classes-to-clusters evaluation. (Let me get rid of this.) I’m going to undo the filter we just did to get the original iris data back. I’m going to go to my Cluster panel, click “Classes to clusters evaluation”, and run that. Now I see I’ve got my 3 classes. There are 3 clusters, and you can see how many of each class were assigned to which cluster. You can see there are 17 incorrectly clustered instances. We’ll have a look at that in a minute, but first let me go and use the EM algorithm and see how that does.
Again, I’m going to specify 3 clusters, and I’m going to run that. I get a similar kind of thing here. Back on the slide, this is the result I saw for SimpleKMeans with 3 clusters. You can see that the majority in cluster 0 is this 47 here. That’s versicolor. So we’re going to assign versicolor to cluster 0. The majority in cluster 1, that’s the second column, are the setosas – that 50 there in the second column, the column labeled 1. The final column, there’s a 36 there, so the majority class is virginica. That’s where we get the 17 incorrectly clustered instances from. EM does quite a lot better here.
We only get 14 incorrectly clustered instances, or 9% of the dataset. That’s a classes-to-clusters evaluation. There’s a meta-classifier called ClassificationViaClustering. It works by ignoring the classes, clustering the data, assigning to each cluster its most frequent class, and that’s a classifier. It’s very similar to what we just did, but we can evaluate it like we evaluate classifiers. Let’s get back to Weka. I’m going to go to Classify, and in my meta list I’m going to choose ClassificationViaClustering. I’m going to stick to SimpleKMeans with 3 clusters. Now if I evaluate that on the training set, that’s exactly what we just did on the clustering panel. Let me start that. Here I get exactly the same matrix as I just looked at.
As you can see, there are 17 errors here. That’s evaluating on the training set. Of course, there are the 17 errors up there. We know we shouldn’t be evaluating on the training set.
We’re going to use cross-validation, which is going to do the usual thing: take 90%, form a clustering, form a classification based on that clustering, and then see how well that does on the held-out 10% of the dataset.
In this case, I get slightly worse results, as I would expect. I’ve got 19 errors, or an 84% success rate. That’s ClassificationViaClustering. Of course, I could choose different clusterers and build classifiers based on them. It’s a very good way of comparing clusterers. It’s hard to evaluate clustering. SimpleKMeans, for instance, uses within-cluster sum of squared errors, but really clustering should be evaluated with respect to a particular application.
Visualization helps: it helps you to see what’s happening to your data. The AddCluster filter allows you to see which instances are in each cluster, which is often useful to see. The classes-to-clusters evaluation gives you a way of looking at the clusters, but, in effect, it uses the entire dataset. To look at the incorrectly assigned instances based on a classification made from the entire dataset risks overfitting; you should never evaluate on the training set. Classification via clustering uses the same kind of technique to produce a classifier that can then be evaluated in different ways, for example, 10-fold cross-validation, which is what we just did.
<End Transcript>

<-- 3.19 Quiz -->
Comparing clusterers using classification-by-clustering
Question 1
Are there any significant differences (at the default 5% level) between SimpleKMeans and EM?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
No v’s or *’s appear in the Experimenter output when you perform the default test on the result of this experiment

<-- 3.19 Quiz -->
Comparing clusterers using classification-by-clustering
Question 2
For which datasets does SimpleKMeans outperform EM?
Select all the answers you think are correct.
breast-cancer
credit-g
diabetes
ionosphere
schizo
vote
---
Correct answer(s):
breast-cancer
diabetes
schizo

<-- 3.19 Quiz -->
Comparing clusterers using classification-by-clustering
Question 3
Of these datasets, XMeans can only process diabetes and ionosphere, because these are the only ones whose attributes are all numeric. Delete the other 4 datasets and add XMeans to the comparison, again constraining the number of clusters to 2.
Are there any significant differences (at the default 5% level) between XMeans and the other two clusterers?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
No v’s or *’s appear in the Experimenter output when you perform the default test on the result of this experiment

<-- 3.19 Quiz -->
Comparing clusterers using classification-by-clustering
Question 4
For which datasets does XMeans outperform SimpleKMeans?
Select all the answers you think are correct.
diabetes
ionosphere
---
Correct answer(s):

<-- 3.19 Quiz -->
Comparing clusterers using classification-by-clustering
Question 4
For which datasets does XMeans outperform SimpleKMeans?
Select all the answers you think are correct.
diabetes
ionosphere
---
Correct answer(s):
diabetes
ionosphere

<-- 3.19 Quiz -->
Comparing clusterers using classification-by-clustering
Question 5
For which datasets is XMeans the best of all?
Select all the answers you think are correct.
diabetes
ionosphere
---
Correct answer(s):
diabetes

<-- 3.20 Discussion -->
Reflect on this week's Big Questions
The Big Questions this week are, “Is it better to generate rules or trees?” and “What if there’s no class attribute?”
We promised that by the end you’d be able to explain the important differences between rules and trees as knowledge representation methods. You’d know how to read an equivalent set of rules from a decision tree, and explain why it may well be excessively redundant. And you’d be able to use two rule-generating methods in Weka, and explain – at a high level – how they work.
We also promised that you’d have some ideas on what to do with a dataset with no class attribute. You’d be able to apply association rule mining to seek interesting associations, and calculate the key parameters of support and confidence for any rule. You’d also be able to use different clustering methods, and be sceptical if the results look too good! And you’d be able to evaluate clusterings using the classification-by-clustering method.
A lot of promises! Can you do these things? Any of them? All of them? And explain them to a colleague?
Well, how did you get on?

<-- 3.21 Article -->
Index
(A full index to the course appears at the end of Week 1.)
      Topic
      Step
      Datasets
      Breast-cancer
      3.8, 3.19
      Contact-lenses
      3.5
      Credit-g
      3.8, 3.19
      Diabetes
      3.7, 3.8, 3.19
      Glass
      3.8
      Ionosphere
      3.8, 3.19
      Iris
      3.8, 3.16, 3.17, 3.18
      Segment-challenge
      3.8
      Schizo
      3.19
      Supermarket
      3.12, 3.13, 3.14
      Vote
      3.11, 3.19
      Weather
      3.2, 3.5, 3.10, 3.12
      Classifiers
      J48
      3.5, 3.8
      JRip
      3.7, 3.8
      OneR
      3.8, 3.11
      PART
      3.7, 3.8
      PRISM
      3.2, 3.5
      ZeroR
      3.8
      Metalearners
      ClassificationViaClustering
      3.18
      Other learning schemes
      Apriori
      3.10, 3.11, 3.12, 3.13, 3.14
      Cobweb
      3.16, 3.17
      EM
      3.16, 3.17, 3.19
      SimpleKMeans
      3.16, 3.17, 3.18, 3.19
      XMeans
      3.16, 3.17, 3.19
      Filters
      AddCluster
      3.18
      Packages
      ClassificationViaClustering
      3.19
      SimpleEducationalLearningSchemes
      3.3
      XMeans
      3.17
      Plus …
      Decision list
      3.5
      Experimenter interface
      3.8, 3.19
      Missing values
      3.13

<-- 4.0 Todo -->
Selecting attributes and counting the cost 
How about selecting key attributes before applying a classifier?
This week's first Big Question!
4.1
How about selecting key attributes before applying a classifier?
article
"Wrapper" attribute selection 
Fewer attributes often yield better performance!
The "wrapper" method of attribute selection involves both an attribute evaluator and a search method.
A classifier, wrapped inside a cross-validation loop, is used for evaluation.
4.2
"Wrapper" attribute selection 
video (09:45)
4.3
Using a wrapper to select attributes
quiz
The Attribute Selected Classifier 
Experimenting with a dataset to select attributes and applying a classifier to the result is cheating!
– even when evaluating by cross-validation.
The AttributeSelectedClassifier selects attributes based on the training set only.
4.4
The Attribute Selected Classifier
video (07:49)
4.5
More benefits of cheating
quiz
4.6
What's all this about cheating?
discussion
Scheme-independent selection 
The "wrapper" method for evaluating attributes is slow.
"Scheme-independent" methods that do not depend on a particular classifier can be faster.
However, searching is still involved whenever you evaluate subsets of attributes.
4.7
Scheme-independent selection 
video (06:40)
4.8
Attribute selection for text classification
quiz
Attribute selection using ranking 
Evaluating attributes individually is much faster than evaluating subsets.
Single-attribute methods, often based on particular machine learning methods, can eliminate irrelevant attributes – but not redundant ones.
4.9
Attribute selection using ranking
video (06:26)
4.10
More attribute selection for text classification
quiz
What happens when different errors have different costs?
This week's second Big Question!
4.11
What happens when different errors have different costs?
article
4.12
Examples of different error costs
discussion
Counting the cost 
If different errors have different costs, the "classification rate" is inappropriate.
Cost-sensitive evaluation takes account of cost when measuring performance.
Cost-sensitive classification takes account of cost during learning.
4.13
Counting the cost
video (06:43)
4.14
Cost-sensitive classification
quiz
Cost-sensitive classification 
A classifier can be made cost-sensitive by re-calculating internal probability thresholds to adjust its output; alternatively the classifier itself can be reimplemented to take account of the cost matrix.
4.15
Cost-sensitive classification 
video (09:49)
4.16
Comparing cost-sensitive techniques
quiz
4.17
Cost-sensitive classification conclusions
article
4.18
Reflect on this week's Big Questions
discussion
4.19
Index
article

<-- 4.1 Article -->
How about selecting key attributes before applying a classifier?
Removing attributes from a dataset before applying a classifier often results in better performance.
It’s ironic that removing information can improve performance! Isn’t the whole idea of data mining to bring as much data as possible to bear on the problem?
Well, that’s true enough. But individual classification algorithms, though they might try to select the most appropriate attributes to use at any given stage, are not necessarily as good at it as a specially designed attribute selection algorithm, applied before the classifier is invoked. For example, “nearest neighbor” or instance-based algorithms (described in Week 3 of Data Mining with Weka) classify test instances on the basis of the nearest training instance. But attributes irrelevant to the decision will distort the distance measure by adding an irrelevant, perhaps random, dimension to the instance space. Decision tree algorithms try to choose the most informative attribute to split on at each node of the tree, but at lower levels there are not many instances to go on and an irrelevant, noisy, attribute may seem, by chance, to be the best choice.
At the end of this week you will be able to explain – and apply – different approaches to attribute selection. The idea is to select a subset of attributes that will work well for the problem at hand. How can you measure whether they would work well? And to find the best subset seems to require searching through all possible subsets, which is an onerous task. You will soon be able to describe – and use – alternative search strategies that reduce the computation (but do not necessarily arrive at the very best solution).
A particularly important revelation is that if you apply an attribute selection method and then apply a classification algorithm to the result, the outcome – even when evaluated using cross-validation – is not necessarily an accurate assessment of performance on fresh data. This is because the entire data set is used when choosing the best attribute subset. That’s cheating! Only the training data should be used. (If you’re on the ball, you might recognize a parallel with supervised discretization.) But how on earth can you achieve this with Weka? You’re about to find out …

<-- 4.2 Video -->
"Wrapper" attribute selection 
Fewer attributes often yield better performance! In a laborious manual process, you can start with the full attribute set and remove the best attribute by selectively trying all possibilities, and carry on doing that. Weka’s Select Attributes panel accomplishes this automatically.  The “wrapper” method wraps a classifier in a cross-validation loop: it searches through the attribute space and uses the classifier to find a good attribute set. Searching can be forwards, backwards, or bidirectional, starting from any subset.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
In this lesson we’re going to talk about attribute selection using the wrapper method. D’you remember way back in Data Mining with Weka, in the first class, you looked at glass.arff, you ran J48, you removed some attributes and – much to your surprise – you sometimes got better results with fewer attributes? Well, that was a laborious manual process, where you started with the full attribute set and removed the best attribute by selectively trying all possibilities; then you carried on doing that. You probably remember the pain involved. Well, of course there’s a better way, and that’s what the Select Attributes panel does.
We’re going to go to Weka, and I’ve opened the glass data set – there it is: 214 instances. I’m going to go to the Select Attributes panel. We’re talking here about the wrapper method of attribute selection, and that involves wrapping up a classifier. We’re going to wrap up J48, which is exactly what you did back then all those weeks ago. I’m going to use 10-fold cross-validation, which actually is what you did – although in Class 1 of Data Mining with Weka you’d never heard of cross-validation. That looks pretty good to me.
I’m going to select a threshold of –1: I’ll explain that later on. Then we have a search method. We’re going to use the BestFirst search, but we’re going to search backwards – I’ll talk about that later on. And we’re going to have a search termination ... yes, we’re going to leave it at that. OK. Let’s just run it. Now it’s running, doing all those cross-validations. And, lo and behold, it’s got the same attribute subset as you got before, and it’s got a “Merit of best subset” of 74%. Going back to the slide here, that’s really the same thing as we got before. Same subset, and the “merit” is the same as the accuracy.
It’s a little bit of a coincidence that we got the same results, because Weka doesn’t do exactly the same thing, you know, the setting of the random number generator and so on. But anyway, we did get the same result here in this situation.
A good question is: how many subsets we had to evaluate? How many attributes we had to evaluate? How much experimentation we had to do? I’m going to go back here, and I’m going to set the searchTermination to 1 – and again I will explain that in a minute – and run it again. And here it tells me that it’s evaluated 36 subsets. Back on the slide, you can count these subsets. It took the complete attribute set and then it tried removing all of the 9 attributes, one by one. That’s 9 more evaluations. Then it removed another attribute, 8 evaluations, and another one and another one, which gave it the final attribute subset.
But to check that it was the final attribute subset and you couldn’t do better by removing another attribute, it had to do a further 5 evaluations. And if you add up all of those, you get 36 subsets evaluated. The wrapper method involves an evaluation method and a search method. Let’s talk about search. We were doing backwards searching. We started with all 9 attributes, and selected one to remove, and so on and so forth until we decided to stop; the searchTermination criteria. It would be equally viable to do forwards search, starting with a 0-attribute subset and adding the best attribute each time until you decided to stop. Or you could do bi-directional search.
You could start with some random attribute subset – actually, Weka allows you to specify what attribute subset to start with – and then either add or subtract an attribute depending on which gives the most performance improvement.
Or you could do exhaustive search: in this case there are 512 possible subsets of 9 attributes and you could simply try them all. The searchTermination criterion is interesting.
When we did this manually, we stopped as soon as the results started to get worse: we got a local maximum in the search space. But you might do better by plowing on through that minimum that you get, going a little bit further to see if perhaps you might reach an even higher peak further on. If you set the searchTermination criterion to something greater than 1, then Weka will try a little bit harder, go a little bit further, before deciding to abandon the search. Now, I’m not going to show you all these different searches, but here are some results. It’s a pretty [lengthy] process. I showed you Backwards search, and we got that first subset at a 0.72 evaluation.
And then we set the searchTermination up to 5 which gives us a chance of powering on past a local maximum, finding an even bigger maximum in the search space, and that gives us a better evaluation. Or with Forwards search, you get that 3-attribute subset RI, Al and Ca. If you search on a little bit further instead of terminating the search prematurely, you can get a better subset with better accuracy. And Bi-directional search will give that 3-attribute subset, and again you can improve that by setting the searchTermination criterion to search a little bit further.
Note that we are always finding a local optimum, but setting the searchTermination criterion to more than 1 gives you a chance of traversing a valley in the search space to find a better local optimum. I turns out that, on this dataset, “Al” is the single best attribute to use (OneR will confirm that for you), so all Forward search results will include “Al”. Curiously, “Al” is the best single attribute to drop. So if you start with a full set, the best one to drop is “Al”. This sounds pretty strange, and I must admit it is pretty unusual. But nevertheless it’s true, and it’s certainly not impossible. Let’s go back to Weka here. I’m going to set cross-validation and see what happens.
What it’s doing now is doing the attribute evaluation 10 separate times. It’s showing us here how many times this attribute, RI, appeared in the final attribute subset. In this case, it appeared in 9 out of the 10 attribute subsets.
Coming back to the slide: in how many folds does this attribute appear in the final subset? You can see that RI and Mg and Ba appear in all 10 of the folds, and Al, Si, K and Fe appear in not too many, 2 or 3 of the folds. This gives you an indication of the stability of the attribute selection method. For this dataset it’s not really very stable, as we’ve seen by getting all those different subsets when we try different parameters of the wrapper method. If we do forward search, of course, we will definitely choose Al, so this was done with Backwards search. The gory details of the Wrapper method.
In general, Weka implementations follow descriptions in the research literature, so these parameters came from the research literature. It tries to do a 5-fold cross-validation by default, not a 10-fold cross-validation, but it doesn’t necessarily do all 5 folds. It does at least 2 and up to 5 runs, and stops when the standard deviation is less than a user-specified threshold. Setting a negative threshold, which is what we did, forces a single cross-validation each time. The BestFirst search method is the default, and the searchTermination defaults to 5 for traversing valleys. The Wrapper method uses cross-validation to select the best attribute to add or drop at each stage. If we go back to Weka, there’s another attribute evaluator, which is called the ClassifierSubsetEvaluator.
That allows us to specify a classifier and also a hold-out file, so here we would use the hold-out file to evaluate each subset in turn. That’s attribute selection using the Wrapper method.
We use a classifier to find a good attribute set: we used J48. We wrap the classifier in a cross-validation loop.
There are two components here: the attribute evaluator, which evaluates a subset of attributes; and the search method, which searches through the attribute space. Searching can be forwards, backwards, or bidirectional starting from any subset.
It’s computationally intensive: m^2 subsets need to be evaluated for m attributes, and there’s an exhaustive method which evaluates 2^m subsets. Greedy search always finds a local optimum in the search space, and you can traverse valleys by increasing the searchTermination parameter.
<End Transcript>

<-- 4.3 Quiz -->
Using a wrapper to select attributes
Question 1
What would the final attribute subset contain if you used ZeroR with forward search?
Nothing
Na
Si
K
All attributes
---
Correct answer(s):
Nothing
---
Feedback correct:
All attribute subsets yield the same performance, and the preference is for the smallest subset, so the result is a set with no attributes (the “empty” set)

<-- 4.3 Quiz -->
Using a wrapper to select attributes
Question 2
What would the final attribute subset contain if you used ZeroR with backward search?
Nothing
RI
Fe
Al
All attributes
---
Correct answer(s):
Nothing
---
Feedback correct:
Same as for the previous question. In this case the order of searching is irrelevant.

<-- 4.3 Quiz -->
Using a wrapper to select attributes
Question 3
For this question and the next you need to know that OneR chooses Al as the attribute for this dataset.
What would the final attribute subset contain if you used OneR with forward search?
Nothing
Al
All attributes
---
Correct answer(s):
Al
---
Feedback correct:
OneR chooses Al for this dataset, and exactly the same result would be obtained for any larger subset of attributes. So preferring the smallest subset leads to the 1-attribute subset containing Al alone.

<-- 4.3 Quiz -->
Using a wrapper to select attributes
Question 4
What would final attribute subset contain if you used OneR with backward search?
None
Al
All attributes
---
Correct answer(s):
Al
---
Feedback correct:
OneR chooses Al for this dataset, and any subset of attributes that does not include Al will yield a worse result. Again, the subset containing Al alone is preferred over larger subsets that also contain Al.

<-- 4.3 Quiz -->
Using a wrapper to select attributes
Question 5
Use Weka to confirm your intuition for the above questions.
Now use the Classify panel to run IBk (with default parameters) on the glass.arff dataset, evaluating using 10-fold cross-validation. What is the percentage of correctly classified instances?
71%
72%
75%
78%
80%
---
Correct answer(s):
71%

<-- 4.3 Quiz -->
Using a wrapper to select attributes
Question 6
What is the percentage of correctly classified instances when the attribute set is {RI, Na, Mg, Ca, Ba}?
(This is the set that Ian found at the beginning of the video Wrapper attribute selection, using wrapper-based attribute selection with J48 in the wrapper.)
72%
74%
76%
78%
80%
---
Correct answer(s):
78%

<-- 4.3 Quiz -->
Using a wrapper to select attributes
Question 6
What is the percentage of correctly classified instances when the attribute set is {RI, Na, Mg, Ca, Ba}?
(This is the set that Ian found at the beginning of the video Wrapper attribute selection, using wrapper-based attribute selection with J48 in the wrapper.)
72%
74%
76%
78%
80%
---
Correct answer(s):

<-- 4.3 Quiz -->
Using a wrapper to select attributes
Question 7
Use the Select attributes panel to find the subset selected by WrapperSubsetEval using IBk and BestFirst search (default parameters throughout). There are 6 attributes in this subset. What are they? (Type them in order.)
---
Correct answer(s):
RI
Mg
Al
K
Ca
Ba

<-- 4.3 Quiz -->
Using a wrapper to select attributes
Question 8
What percentage of correctly classified instances does IBk yield for that attribute set?
71%
72%
75%
78%
80%
---
Correct answer(s):
78%

<-- 4.3 Quiz -->
Using a wrapper to select attributes
Question 9
For this question you will first need to install Weka’s attributeSelectionSearchMethods package. (That’s a nuisance? – hey, it only takes about 2 minutes!)
Restore the original dataset and choose the “Exhaustive search” search method on the Select attributes panel. This searches all 512 possible subsets of the 9 attributes.
What attribute subset is selected by WrapperSubsetEval using IBk (default parameters)?
---
Correct answer(s):
RI
Na
Mg
K
Ca
Ba

<-- 4.3 Quiz -->
Using a wrapper to select attributes
Question 10
What percentage of correctly classified instances does IBk yield for that attribute set?
71%
73%
75%
77%
79%
---
Correct answer(s):
79%

<-- 4.4 Video -->
The Attribute Selected Classifier
Experimenting with a dataset to select attributes and applying a classifier to the result is cheating, if  performance is evaluated using cross-validation, because the entire dataset is used to determine the attribute subset. You mustn’t use the test data when setting discretization boundaries! But with cross-validation you don’t really have an opportunity to use the training data only. Enter the FilteredClassifier, which solves the problem. (Does that ring a bell? You saw it before, in Week 2.)
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
We’re going to open a dataset, glass.arff, apply J48, and see what we get. Then we’re going to use Wrapper attribute selection with J48, and we’re going to see what attributes that gives us. Then we’re going to use just those attributes and run classification with J48 again on the new dataset. Let’s go over to Weka and try it. I’ve got the glass data open here. I’m going to classify it with J48 with default parameters, and I get 67%. Then I’m going to go to Attribute selection. I’m going to use the Wrapper method, and within the Wrapper method I’m going to choose J48. I’m going to leave everything else at its default value and see what attributes I get.
And we get the attribute set {RI, Mg, Al, K, Ba}. So I’m going to go back. RI, Mg, Al, K, Ba; delete the rest; and now I’ve got just that set {RI, Mg, Al, K, Ba}.
I’m going to classify it again with J48, and I get better accuracy: 71%. (I’m just going to back and undo that filter.) Back to the slide here. I got improved accuracy. If I did the same thing with IBk, I’d get 71% for IBk on the glass data set and 78% if I used just the attributes selected by wrapper attribute selection using IBk.
The question is: is this cheating?
And the answer is: yes, it certainly is. The reason is, because we’re using the entire data set to decide on the attribute subset. We should really just use the training data to decide on all of those things and then test the final result on the test set. This is just what the AttributeSelectedClassifier does. Remember the FilteredClassifier we used for supervised discretization? Well, the AttributeSelectedClassifier is the analogous thing for attribute selection. It selects attributes based on the training data only, even if we are within a cross-validation. Then it trains the classifier, again on the training data only. And then it evaluates the whole thing on the test data. I’m going to use that to wrap J48 and see what I get.
It’s a little bit complex to set up. I’m going to get the AttributeSelectedClassifier from meta. I’m going to use the J48 classifier. I’m going to use the wrapper subset evaluator, and within that I get to choose a classifier to wrap for attribute selection.
I can choose any classifier – I don’t have to choose J48 again – but I will: I’m going to use J48 both for attribute selection and for classification. Leave everything else at its default value, and run it again. It’s finished now, and I get an accuracy of 72%. Back on the slide. This is not cheating. Actually, it’s slightly surprising that I get a higher accuracy (72%) when I’m not cheating than I did, the 71% that I got when I was cheating. But, you know, we should really use the Experimenter to get reliable results; this is just the result of one run here.
If we were to do the same thing with IBk, I got a dramatic improvement in IBk by using the correct attributes, 78% – by cheating, that is. If I then use the AttributeSelectedClassifier, well, of course I can decide what I’m going to wrap. If I were using IBk, then I’d probably want to select attributes using IBk. That would give me the figure of 71% at the bottom right. Of course, I can use a different classifier to do the attribute selection than the one I’m using for classification. The 69% figure is when I wrap IBk, do attribute selection using IBk, and then classify using J48. The 74% is when I do attribute selection using J48 and then classify using IBk.
And it’s slightly surprising – you would expect that you would get a better attribute subset by using the classifier that you’re going to be using for classification, so it’s slightly surprising to see that 74% coming in larger than the 71% figure. But, you know, surprising things happen, and if we did a more extensive run with the Experimenter we probably wouldn’t find that. I’m going to check the effectiveness of the AttributeSelectedClassifier for getting rid of redundant attributes. I’m going to open the diabetes dataset and I’m going to use the AttributeSelectedClassifier with Naive Bayes. Remember with Naive Bayes, when you add redundant attributes the performance of Naive Bayes gets worse.
So I’m going to add redundant attributes, copies of attributes, and then I’m going to use the AttributeSelectedClassifier and see if the performance still gets worse. I’m hoping it doesn’t. The AttributeSelectedClassifier should get rid of those redundant copied attributes. I’m going to open diabetes, and I’m going to use the AttributeSelectedClassifier. I’ll just going to do this one more time, because configuration takes a bit of thought. I’m going to use as my classifier Naive Bayes. And I’m going to use the WrapperSubsetEvaluator. Within that, as the classifier to wrap, I’m going to choose Naive Bayes again. I don’t have to, but it’s probably better to use the same classifier. I’m going to leave everything else at its default value, and let’s run that.
Here I get 75/76% accuracy, 75.7%. Back on the slide then. If I just run Naive Bayes on the diabetes dataset, I would get 76.3%. Using the attribute selection in the proper way – that is, not cheating – with the AttributeSelectedClassifier, I get 75.7%. It’s a little disappointing that attribute selection didn’t help much on this dataset. But let’s now copy the attributes. I’m going to copy the first attribute. Naive Bayes give me 75.7%, and the Attribute [Selected] Classifier also gives me 75.7%. If I add a bunch more copies of that attribute, 9 further copies, then the performance of Naive Bayes deteriorates to 68.9%, whereas the AttributeSelectedClassifier stays the same, because it’s resistant to these redundant attributes.
And if I add further copies, then Naive Bayes will slowly get worse and worse, whereas the AttributeSelectedClassifier continues at its standard level of 75.7%. The conclusion is that attribute selection does a good job of removing redundant attributes. In this lesson, we’ve looked at the AttributeSelectedClassifier, which selects attributes based on the training set only, which is the right way to do it.
<End Transcript>

<-- 4.5 Quiz -->
More benefits of cheating
Question 1
Use the Experimenter to evaluate J48 on this reduced data file, with ten 10-fold cross-validations (the default). Note that this is cheating. What classification accuracy is obtained?
71%
73%
75%
77%
79%
---
Correct answer(s):
75%

<-- 4.5 Quiz -->
More benefits of cheating
Question 2
The proper (non-cheating) way is to use the original glass dataset and configure the AttributeSelectedClassifier to use J48 as a classifier and WrapperSubsetEval, configured with J48, as the evaluator.
Set this up with the Experimenter. It’s a fairly complex process: keep it as simple as possible by using default options for everything else (including BestFirst as the search method).
What classification accuracy is obtained?
70%
72%
74%
76%
78%
---
Correct answer(s):
70%

<-- 4.5 Quiz -->
More benefits of cheating
Question 3
Does crime pay in this case?
Yes
No
---
Correct answer(s):
Yes
---
Feedback correct:
Too right! Criminals get 75%; honest people get only 70%.

<-- 4.5 Quiz -->
More benefits of cheating
Question 4
Repeat the whole performance using Naive Bayes instead of J48.
First use the Explorer’s Select attributes panel to determine the subset of attributes of the glass dataset that WrapperSubsetEval selects using Naive Bayes as the classifier and default parameters for everything else (including BestFirst as the search method).
What are the attributes that Naive Bayes selects?
Select all the answers you think are correct.
RI
Na
Mg
Al
Si
K
Ca
Ba
Fe
---
Correct answer(s):
RI
Al

<-- 4.5 Quiz -->
More benefits of cheating
Question 5
Create a reduced data file with just these attributes, along with Type, and use the Experimenter to evaluate Naive Bayes on it, with ten 10-fold cross-validations (the default). What classification accuracy is obtained?
50%
55%
58%
59%
---
Correct answer(s):
59%

<-- 4.5 Quiz -->
More benefits of cheating
Question 6
That’s cheating, of course.
The proper way is to configure the AttributeSelectedClassifier to use Naive Bayes as the classifier and WrapperSubsetEval, again with Naive Bayes, as the evaluator (use default options for everything else).
Set this up with the Experimenter, using the original glass dataset. What classification accuracy is obtained?
50%
55%
58%
59%
---
Correct answer(s):
58%

<-- 4.5 Quiz -->
More benefits of cheating
Question 7
Does crime pay in this case?
Yes
No
---
Correct answer(s):
Yes
---
Feedback correct:
Though it doesn’t pay much: criminals get 59% whereas the rest of us get 58%.

<-- 4.5 Quiz -->
More benefits of cheating
Question 8
Repeat the whole performance with IBk instead of Naive Bayes. Using the Explorer’s Select attributes panel as before, what are the attributes that IBk selects?
Select all the answers you think are correct.
RI
Na
Mg
Al
Si
K
Ca
Ba
Fe
---
Correct answer(s):
RI
Mg
Al
K
Ca
Ba

<-- 4.5 Quiz -->
More benefits of cheating
Question 9
Use the Experimenter to evaluate IBk on a reduced data file with just these attributes, with ten 10-fold cross-validations (the default). What classification accuracy is obtained?
Error
63%
78%
81%
---
Correct answer(s):
78%
---
Feedback incorrect:
Make sure that you include the attribute “type”.

<-- 4.5 Quiz -->
More benefits of cheating
Question 10
Use the Experimenter to do it the proper way, configuring the AttributeSelectedClassifier to use IBk as the classifier and WrapperSubsetEval, again with IBk, as the evaluator (use default options for everything else).
Set this up with the Experimenter, using the original glass dataset.
What classification accuracy is obtained?
36%
69%
75%
81%
---
Correct answer(s):
75%

<-- 4.5 Quiz -->
More benefits of cheating
Question 11
Does crime pay in this case?
Yes
No
---
Correct answer(s):
Yes
---
Feedback correct:
You bet! 78% vs 75%.

<-- 4.5 Quiz -->
More benefits of cheating
Question 12
Using a classifier on a reduced dataset whose attributes were selected by a different classifier is also cheating, because again the data in the test set is used to determine the attribute set. However, in this case the crime is far less likely to pay off.
Use the Experimenter to determine the accuracy of J48 on the version of the dataset reduced to include just the attributes selected by Naive Bayes, that is, {RI, Al}. What is it?
60%
62%
64%
66%
---
Correct answer(s):
62%

<-- 4.5 Quiz -->
More benefits of cheating
Question 13
Is this greater than the accuracy of J48 on the original dataset?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
J48 yields 68% on the original dataset, evaluated using the Experimenter with default parameters. The cheaters lose!

<-- 4.5 Quiz -->
More benefits of cheating
Question 14
Use the Experimenter to determine the accuracy of Naive Bayes on the version of the dataset reduced to include just the attributes selected by J48, that is, {RI, Na, Mg, Ca, Ba}. What is it?
45%
59%
62%
63%
---
Correct answer(s):
45%

<-- 4.5 Quiz -->
More benefits of cheating
Question 14
Use the Experimenter to determine the accuracy of Naive Bayes on the version of the dataset reduced to include just the attributes selected by J48, that is, {RI, Na, Mg, Ca, Ba}. What is it?
45%
59%
62%
63%
---
Correct answer(s):

<-- 4.5 Quiz -->
More benefits of cheating
Question 15
Is this greater than the accuracy of Naive Bayes on the original dataset?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
Naive Bayes yields 49% on the original dataset, evaluated using the Experimenter with default parameters. The cheaters lose again!

<-- 4.6 Discussion -->
What's all this about cheating?
Confession time!
I called it “cheating” mainly to get your attention.
Here we’re talking about attribute selection, but the same phenomenon occurred back in Week 2 when we talked about supervised discretization. With supervised discretization you mustn’t use the test set to help determine discretization boundaries; with attribute selection you mustn’t use it to decide which attributes to select. If you do, I’ve called that “cheating”.
The dictionary definition of cheating is “acting dishonestly or unfairly in order to gain an advantage, especially in a game or examination”.
Is using the test set to help set discretization boundaries or select attributes really “cheating”? What do you think? What’s the real danger?

<-- 4.7 Video -->
Scheme-independent selection 
Attribute selection methods that do not involve a classifier can be faster than the wrapper method. They can use the same kind of searching, but evaluate each subset using a heuristic instead of applying a particular classifier. In this video we look at a scheme-independent attribute selection method that is nearly as good as the wrapper method, and is much faster.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello again! Welcome back to New Zealand.
In this lesson, we’re going to look at a new class of attribute selection methods: scheme-Independent attribute selection. The Wrapper method that we looked at before is straightforward, simple, and direct – but it’s really slow. So here are a couple of alternatives. We could use a single-attribute evaluator, evaluate the attributes one by one independently, and then rank them and base our attribute selection on that. That allows us to eliminate irrelevant attributes, and we’ll be looking at that in the next lesson. A second alternative is to combine an attribute subset evaluator with a search method. And that allows us to eliminate redundant attributes as well as irrelevant ones, so it’s potentially much more powerful.
Now we’ve already looked at search methods, and we’ve looked at one kind of attribute subset evaluator, the wrapper method. That is a way, a scheme-dependent way, of evaluating an attribute subset. Now we’re going to look at scheme-independent ways of evaluating attribute subsets. In fact, we’re going to look at a method called CfsSubsetEval. It considers an attribute subset to be good if the attributes it contains are highly correlated with the class attribute and not strongly correlated with one another. It comes up with a measure of “goodness” of an attribute subset. This is a measure applied to a subset.
We sum the correlation between the attribute and the class over all of the attributes in the subset; then we divide that by the correlations of each attribute with each other attribute, summed over all pairs of attributes (we take the square root of that). For correlation, the CfsSubsetEval method uses an entropy-based metric called the “symmetric uncertainty”. It’s pretty straightforward, but I’m not going to talk about that. Let’s try it. Let’s compare CfsSubsetEval with Wrapper selection on the ionosphere data. We’re going to look first at Naive Bayes. Coming over to Weka here, I’ve got the ionosphere data open, and I’m going to classify that with Naive Bayes, standard Naive Bayes. When I do that, I get 82–83%. All right.
Now let’s do attribute selection and, of course, we’re going to use the AttributeSelectedClassifier to ensure that we’re not cheating. That’s a meta classifier, the AttributeSelectedClassifier. Within that, remember, we can select a classifier – we’re going to choose Naive Bayes – and we’re also going to choose a subset evaluator – we’re going to use the default, CfsSubsetEval. And for the search method, I’ll just use the default search method. Let’s run that. Now we get 88.6% ... 89%, which is a lot better, so attribute selection has really helped here. Let’s try attribute selection using the Wrapper method. I’m going to use the same learning scheme, Naive Bayes, but here I’m going to choose the Wrapper method.
For that, of course, I’ve got to specify a machine-learning method to use to wrap, and we’re going to wrap Naive Bayes. I’m going to run that – everything else is default – and it’s going to take a while. Here we go. It’s finished now; it took quite a long time. We got 91% accuracy. Back on the slide. In the NaiveBayes column, we got 83% without attribute selection. Attribute selection helped quite a lot, with CfsSubsetEval, which is very fast – and it was even better with the very slow Wrapper method. When I did IBk, I got 86% for plain IBk, 89% for CfsSubsetEval.
And for the wrapper, I wrapped IBk – in each of these things, I wrapped the corresponding classifier, the one that we’re using for classification – and I got 89%. The two attribute selection methods were the same. J48 was already extremely good without any attribute selection. I got 92% for the very fast method, and in fact, I got slight worse results (90%) for the much slower wrapper selection. A little bit surprising that wrapper selection does worse than CfsSubsetEval for J48. These are just based on one run, of course. The conclusion is that CfsSubsetEval is nearly as good as the Wrapper method, and much faster. There are a number of attribute subset evaluators in Weka. There are a couple of scheme-dependent methods.
The WrapperSubsetEval uses internal cross-validation, and I think in a previous lesson we mentioned briefly the ClassifierSubsetEval, which is like the Wrapper method but instead of using cross-validation it uses a separate held-out test set. Those are scheme-dependent. And then the scheme-independent methods, there are a few of those. We’ve looked at CfsSubsetEval, and there’s another one called the ConsistencySubsetEval, which measures consistency in class values of the training set with respect to the attributes. If I go over to Weka here and have a look at the different methods of attribute selection. There’s CfsSubsetEval. I talked about ClassifierSubsetEval, that’s a scheme-dependent method. ConsistencySubsetEval, that’s the one we were just talking about, and I can look at that and get some more information.
It evaluates the worth of a subset by consistency, and to really understand that method you need to go and look at the paper where it’s referenced. As you can see, there are quite a lot of different methods for attribute subset evaluation, and the list includes meta-evaluators, which incorporate other operations. I’m not going to talk about that here. In conclusion, attribute subset selection involves a subset evaluation measure and a search method. Some measures are scheme-dependent, like the Wrapper method, which is very slow, and others are scheme-independent, like CfsSubsetEval, which, as we found, is quite fast. Even faster is to use a single-attribute evaluator using ranking, and we are going to talk about that in the next lesson.
<End Transcript>

<-- 4.8 Quiz -->
Attribute selection for text classification
Question 1
In the Explorer, load ReutersCorn-train and apply the StringToWordVector filter (with default parameters). In the Classify panel, apply NaiveBayes. Note that Weka assumes that the last attribute (zone) is the class, so be sure to change this to class-att.
Check two of the boxes below to indicate the accuracy obtained by NaiveBayes and NaiveBayesMultinomial (using 10-fold cross-validation).
Select all the answers you think are correct.
Naive Bayes: 89%
Naive Bayes: 90%
Naive Bayes: 91%
Naive Bayes: 92%
Naive Bayes Multinomial: 96%
Naive Bayes Multinomial: 97%
Naive Bayes Multinomial: 98%
Naive Bayes Multinomial: 99%
---
Correct answer(s):
Naive Bayes: 90%
Naive Bayes Multinomial: 98%

<-- 4.8 Quiz -->
Attribute selection for text classification
Question 2
In the Select attributes panel, use CfsSubsetEval with BestFirst search (default parameters). Again, be sure to change the class from zone to class-att.
How many attributes are selected?
8
16
256
1024
---
Correct answer(s):
16

<-- 4.8 Quiz -->
Attribute selection for text classification
Question 3
In the Classify panel, apply the AttributeSelectedClassifier with CfsSubsetEval and BestFirst search (default parameters).
Check two of the boxes below to indicate the accuracy obtained by NaiveBayes and NaiveBayesMultinomial.
Select all the answers you think are correct.
Naive Bayes: 96.5%
Naive Bayes: 97.3%
Naive Bayes: 98.1%
Naive Bayes: 98.9%
Naive Bayes Multinomial: 96.2%
Naive Bayes Multinomial: 97.6%
Naive Bayes Multinomial: 98.5%
Naive Bayes Multinomial: 99.3%
---
Correct answer(s):
Naive Bayes: 98.9%
Naive Bayes Multinomial: 99.3%

<-- 4.8 Quiz -->
Attribute selection for text classification
Question 4
In each case the error rate on the minority class is the same. How many errors are there (out of the 45 instances in that class)?
7
9
11
20
---
Correct answer(s):
7

<-- 4.8 Quiz -->
Attribute selection for text classification
Question 5
Return to the Select attributes panel and examine the attributes that were selected by CfsSubsetEval. Three of them are the same word, expressed in different combinations of upper- and lower-case letters.
What is that word?
Taiwan
Wheat
Corn
International
---
Correct answer(s):
Corn

<-- 4.8 Quiz -->
Attribute selection for text classification
Question 6
That makes one suspect that it would be a good idea to convert everything into lower-case before performing attribute selection (this is called “case-folding”). Return to the Preprocess panel and configure the StringToWordVector filter to do that; then repeat the attribute selection process on the Select attributes panel (be sure to change the class to class-att again).
How many attributes are selected?
10
16
24
34
---
Correct answer(s):
16

<-- 4.8 Quiz -->
Attribute selection for text classification
Question 7
It’s just a coincidence that the same number of attributes (16) are selected in Q.6 as in Q.2. With case-folding, the reduced attribute set contains extra words that seem relevant, e.g. assets, decision, grain, meal, sorghum.
In the Classify panel, apply the AttributeSelectedClassifier again, with CfsSubsetEval and BestFirst search (default parameters).
Check two of the boxes below to indicate the accuracy obtained by NaiveBayes and MultinomialNaiveBayes.
Select all the answers you think are correct.
Naive Bayes: 96%
Naive Bayes: 97%
Naive Bayes: 98%
Naive Bayes: 99%
Multinomial Naive Bayes: 96%
Multinomial Naive Bayes: 97%
Multinomial Naive Bayes: 98%
Multinomial Naive Bayes: 99%
---
Correct answer(s):
Naive Bayes: 98%
Multinomial Naive Bayes: 99%

<-- 4.8 Quiz -->
Attribute selection for text classification
Question 8
In each case, the error rate on the minority class is the same. How many errors are there?
0
1
2
4
---
Correct answer(s):
1

<-- 4.8 Quiz -->
Attribute selection for text classification
Question 9
It’s disappointing that the performance of NaiveBayes drops for this new attribute set, from 99% (actually, 98.9%) without case-folding for Q.3 to 98% (actually, 98.1%) with case-folding for Q.7.
However, the performance of NaiveBayesMultinomial with case-folding is outstanding: 9 errors out of 1509 (0.6%) for the majority class, and 1 error out of 45 (2%) for the minority class.
When investigating document classification with Multinomial Naive Bayes in Week 2 (Step 2.10), we fiddled around with StringToWordVector parameters and finally achieved a weighted-average ROC Area of 0.977, with outputWordCounts and lowerCaseTokens true, minTermFreq 5, wordsToKeep 800, and the Snowball stemmer. (Perhaps you should repeat that experiment to refresh your memory of what you had to do. Recall that ReutersCorn-test.arff was used for evaluation, instead of cross-validation.)
Is the ROC area we have just achieved with Multinomial Naive Bayes and attribute selection greater than this previously-best value of 0.977?
Yes
No
---
Correct answer(s):
Yes
---
Feedback correct:
It is 0.998, which is considerably better than 0.977

<-- 4.8 Quiz -->
Attribute selection for text classification
Question 10
In the earlier activity, performance was evaluated on a separate test set rather than using cross-validation. Suppose you created a reduced training set with a smaller number of attributes, as selected above, and evaluated performance on the test set, also reduced in the same way.
Would it be cheating to apply classifiers like NaiveBayes and NaiveBayesMultinomial to this problem directly, instead of using the AttributeSelectedClassifier?
Yes
No
---
Correct answer(s):

<-- 4.8 Quiz -->
Attribute selection for text classification
Question 10
In the earlier activity, performance was evaluated on a separate test set rather than using cross-validation. Suppose you created a reduced training set with a smaller number of attributes, as selected above, and evaluated performance on the test set, also reduced in the same way.
Would it be cheating to apply classifiers like NaiveBayes and NaiveBayesMultinomial to this problem directly, instead of using the AttributeSelectedClassifier?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
It would be fine, because the test set has not been used in any way during the attribute selection process.

<-- 4.9 Video -->
Attribute selection using ranking
The attribute selection methods we have examined so far strive to eliminate both irrelevant attributes and redundant ones. A simpler idea is to rank the effectiveness of each individual attribute, and choose the top few to use for classification, discarding the rest. This is lightning fast because it does not involve searching at all, but can only eliminate irrelevant attributes, not redundant ones. And the results are very sensitive to the number of attributes that are retained.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello again! One final lesson on attribute selection. You’re probably getting a bit fed up with attribute selection by now, but you know it’s really important. It’s one of the things that can really improve the performance of machine learning methods, and more importantly, it really improves the understandability. You know, you select out some attributes – it’s easy to explain to other people what you’ve done to get such good performance on their data set. Attribute selection is pretty important. We’re going to look in this lesson at fast attribute selection using ranking. Remember before in the last lesson we looked at attribute subset selection, which involves a subset evaluation measure and a search method, and we were looking for rapid subset evaluation methods.
The Wrapper method is very slow, and we were looking for faster alternatives. But, of course, searching is slow. So we’re not doing any searching now. We’re going to use a single-attribute evaluator, that doesn’t evaluate a subset, it evaluates each attribute individually. This can help eliminate irrelevant attributes, but it can’t remove redundant attributes, because it’s only looking at individual attributes, one at a time. You need to choose the ranking search method whenever you select a single-attribute evaluator. The ranking search method doesn’t really search, it just sorts them into rank order of the evaluation. We’ve seen several metrics for evaluating attributes before. We looked in the last course at OneR, ages ago. Remember OneR? It’s effectively a method of ranking attributes.
In Weka, there are attribute selection methods based on all of these. The OneR attribute evaluator. C4.5, what we know as J48 in Weka, uses information gain, so there’s an information gain attribute evaluator. Actually, it uses gain ratio, slightly more complex than information gain, and there’s also a gain ratio attribute evaluator. In the last lesson we saw the CfsSubsetEvaluation method, and that uses symmetric uncertainty, so there is a symmetric uncertainty attribute evaluator in Weka. The ranker search method is very simple. It just sorts attributes according to their evaluation, and you can specify the number of attributes to retain.
The default is to retain them all, or you can ask it to discard attributes whose evaluation falls below a certain threshold, or you can specify a certain set of attributes that you want to ignore. Let’s have a look. Let’s compare GainRatioAttributeEval with the other methods we looked at in the last lesson, on the ionosphere data. The gray part of this, the “No attribute selection” and “CfsSubsetEval” and “Wrapper”, those results we got before in the last lesson. We’re just going to look at the GainRatioAttributeEval. I’m going to go to Weka. I’ve got my ionosphere dataset.
Of course, I’m going to use the AttributeSelectedClassifier to get a fair evaluation: meta > AttributeSelectedClassifier. Here I’m going to specify – let’s just use Naive Bayes to start off with. I’m going to use the GainRatioAttributeEval.
If I just run that, it’s not going to work: the attribute evaluators must use the Ranker search method. Sorry about that, I should have specified here the Ranker search method. There are a couple of parameters.
The number to select: –1 means select them all; it’s not really very useful to select them all. I’m going to select 7, the best 7 attributes. We could have a set to ignore. This threshold here, this bizarre number, is actually minus infinity in Java, so that’s it why it’s such a strange number. That’s all I need to do. I’m going to run that, and I get 89 … 90% accuracy. Let’s go back to the slide and compare this. Last time with Naive Bayes I got 83% accuracy, and then 89% with CfsSubsetEvaluation, 91% with the Wrapper selection method, and with this new method GainRatioAttributeEval, a single-attribute evaluator, I get 90%. Fantastic performance for a method that’s lightning fast.
For IBk, the performance is really not very good. It’s just the same as IBk without any attribute selection. For J48, it’s the same as J48 without any attribute selection. Single-attribute selection is lightning fast but very sensitive to the number of attributes. I chose 7 here because it turned out to be a good number for this problem. There are a lot of single-attribute evaluators in Weka. We talked about the first four a minute ago. There’s one based on the chi-squared test, one based on support vector machines, one instance-based evaluator, principal components transform, and latent semantic analysis. The workings of these are all explained in the papers that are referenced in the More button for that attribute evaluator.
There are also meta-evaluators, which incorporate other operations. That’s it. We’ve seen that attribute subset selection involves searching, which is bound to be slow no matter how quickly you can evaluate the subsets, so instead we can use single-attribute evaluation. It involves ranking, which is really fast. It’s hard to specify a suitable cut-off, you need to do experimentation. It doesn’t cope with redundant attributes. For example, if you have copies of an attribute, then they will be repeatedly selected, because attributes are evaluated individually. Many single-attribute evaluators are based on machine-learning methods we’ve already looked at.
<End Transcript>

<-- 4.10 Quiz -->
More attribute selection for text classification
Question 1
Load ReutersCorn-train.arff and apply the StringToWordVector filter with case-folding (i.e., set lowerCaseTokens to true). In the Select attributes panel, use GainRatioAttributeEval with the Ranker search method. Don’t forget to change the class attribute to class-att.
What are the top 6 attributes, in order?
---
Correct answer(s):
corn
maize
sorghum
upholds
countervailing
cwt

<-- 4.10 Quiz -->
More attribute selection for text classification
Question 2
In the Classify panel, apply the AttributeSelectedClassifier with NaiveBayes, and GainRatioAttributeEval with Ranker (default parameters).
What overall accuracy does NaiveBayes achieve?
90%
91%
92%
93%
---
Correct answer(s):
91%

<-- 4.10 Quiz -->
More attribute selection for text classification
Question 3
What is the number of errors on the minority and majority classes respectively?
Select all the answers you think are correct.
Errors on the minority class: 3
Errors on the minority class: 5
Errors on the minority class: 40
Errors on the majority class: 132
Errors on the majority class: 137
Errors on the majority class: 140
---
Correct answer(s):
Errors on the minority class: 3
Errors on the majority class: 140

<-- 4.10 Quiz -->
More attribute selection for text classification
Question 4
The NaiveBayes accuracy you have just achieved is the same as without any attribute selection because the Ranker selects them all by default (numToSelect = –1).
Change the Ranker to select just one attribute. What is the
  Overall accuracy
  Number of errors on the minority class
  Number of errors on the majority class?
Select all the answers you think are correct.
Overall accuracy: 89.7%
Overall accuracy: 98.8%
Overall accuracy: 99.7%
Errors on the minority class: 1
Errors on the minority class: 3
Errors on the minority class: 14
Errors on the minority class: 45
Errors on the majority class: 0
Errors on the majority class: 4
Errors on the majority class: 35
---
Correct answer(s):
Overall accuracy: 98.8%
Errors on the minority class: 14
Errors on the majority class: 4

<-- 4.10 Quiz -->
More attribute selection for text classification
Question 5
Change the Ranker to select 2 attributes. What is the
  Overall accuracy
  Number of errors on the minority class
  Number of errors on the majority class?
Select all the answers you think are correct.
Overall accuracy: 89.7%
Overall accuracy: 98.8%
Overall accuracy: 99.7%
Errors on the minority class: 1
Errors on the minority class: 3
Errors on the minority class: 14
Errors on the minority class: 45
Errors on the majority class: 0
Errors on the majority class: 4
Errors on the majority class: 35
---
Correct answer(s):
Overall accuracy: 99.7%
Errors on the minority class: 1
Errors on the majority class: 4

<-- 4.10 Quiz -->
More attribute selection for text classification
Question 6
Change the Ranker to select 5 attributes. What is the
  Overall accuracy
  Number of errors on the minority class
  Number of errors on the majority class?
Select all the answers you think are correct.
Overall accuracy: 89.7%
Overall accuracy: 98.8%
Overall accuracy: 99.7%
Errors on the minority class: 1
Errors on the minority class: 3
Errors on the minority class: 14
Errors on the minority class: 45
Errors on the majority class: 0
Errors on the majority class: 4
Errors on the majority class: 35
---
Correct answer(s):
Overall accuracy: 99.7%
Errors on the minority class: 1
Errors on the majority class: 4

<-- 4.10 Quiz -->
More attribute selection for text classification
Question 7
Change the Ranker to select 10 attributes. What is the
  Overall accuracy
  Number of errors on the minority class
  Number of errors on the majority class?
Select all the answers you think are correct.
Overall accuracy: 89.7%
Overall accuracy: 98.8%
Overall accuracy: 99.7%
Errors on the minority class: 1
Errors on the minority class: 3
Errors on the minority class: 14
Errors on the minority class: 45
Errors on the majority class: 0
Errors on the majority class: 4
Errors on the majority class: 35
---
Correct answer(s):
Overall accuracy: 99.7%
Errors on the minority class: 1
Errors on the majority class: 4

<-- 4.10 Quiz -->
More attribute selection for text classification
Question 8
Change the Ranker to select 15 attributes. What is the
  Overall accuracy
  Number of errors on the minority class
  Number of errors on the majority class?
Select all the answers you think are correct.
Overall accuracy:  98.8%
Overall accuracy: 99.5%
Overall accuracy: 99.7%
Errors on the minority class: 1
Errors on the minority class: 3
Errors on the minority class: 14
Errors on the minority class: 45
Errors on the majority class: 0
Errors on the majority class: 4
Errors on the majority class: 35
---
Correct answer(s):
Overall accuracy: 99.5%
Errors on the minority class: 3
Errors on the majority class: 4

<-- 4.10 Quiz -->
More attribute selection for text classification
Question 9
These results are astonishing! With just 2 carefully selected attributes, Naive Bayes yields unprecedented performance. Unfortunately, Multinomial Naive Bayes does not fare so well.
Repeat the above with NaiveBayesMultinomial, using the Ranker to select 2 attributes. What is the
  Overall accuracy
  Number of errors on the minority class
  Number of errors on the majority class?
Select all the answers you think are correct.
Overall accuracy: 97.1%
Overall accuracy: 99.5%
Overall accuracy: 99.7%
Errors on the minority class: 1
Errors on the minority class: 3
Errors on the minority class: 14
Errors on the minority class: 45
Errors on the majority class: 0
Errors on the majority class: 4
Errors on the majority class: 35
---
Correct answer(s):
Overall accuracy: 97.1%
Errors on the minority class: 45
Errors on the majority class: 0

<-- 4.10 Quiz -->
More attribute selection for text classification
Question 10
Multinomial Naive Bayes yields the same result with 5, 10, or 15 attributes. Its performance is appalling – it incorrectly classifies all 45 instances in the minority class.
Return to Naive Bayes with 2 attributes, the best result by far that we have found for this problem. Can you expect this result to be achieved on independent test data?
Definitely
Perhaps
---
Correct answer(s):
Perhaps
---
Feedback correct:
Not necessarily, because although we have used proper methodology throughout, in the end we selected the best method of several, based on their performance – which effectively uses the evaluation data for selection.

<-- 4.10 Quiz -->
More attribute selection for text classification
Question 11
Previously we used the ReutersCorn-test.arff dataset for evaluation, and so far we haven’t used it in any way during our investigation. Thus it’s an entirely independent test set. Let’s use it to test our new best method. It’s a bit of a nuisance, but very instructive, so persevere for a little longer.
The current dataset is ReutersCorn-train.arff, filtered by StringToWordVector with LowerCaseTokens set. Return to the Preprocess panel and delete all but the first two attributes, corn and maize, plus the class. (Click the button that selects all attributes; then find the three you want and deselect them. Delete all the rest.) Save the resulting dataset.
Load ReutersCorn-test.arff, process it in the same way, and save it.
Apply Naive Bayes to the training set and test it on the test set. What is the
  Overall accuracy
  Number of errors on the minority class
  Number of errors on the majority class?
Select all the answers you think are correct.
Overall accuracy: 99.3%
Overall accuracy: 99.5%
Overall accuracy: 99.7%
Errors on the minority class: 0
Errors on the minority class: 3
Errors on the minority class: 14
Errors on the minority class: 45
Errors on the majority class: 0
Errors on the majority class: 4
Errors on the majority class: 35
---
Correct answer(s):
Overall accuracy: 99.3%
Errors on the minority class: 0
Errors on the majority class: 4

<-- 4.10 Quiz -->
More attribute selection for text classification
Question 12
These are astonishingly good! - despite our qualms that the earlier figures may be biased.
Previously (with Multinomial Naive Bayes), we achieved a best ROC area of 0.977. What is it now?
0.977
0.979
0.987
0.997
---
Correct answer(s):
0.997
---
Feedback correct:
In fact, the error rates previously were 1 and 61 for the minority and majority classes respectively, compared with 0 and 4 now.
Congratulations! You have achieved a phenomenal result using the simplest of methods.

<-- 4.10 Quiz -->
More attribute selection for text classification
Question 12
These are astonishingly good! - despite our qualms that the earlier figures may be biased.
Previously (with Multinomial Naive Bayes), we achieved a best ROC area of 0.977. What is it now?
0.977
0.979
0.987
0.997
---
Correct answer(s):

<-- 4.11 Article -->
What happens when different errors have different costs?
In this course (and its predecessor, Data Mining with Weka), we’ve been obsessed with classification accuracy – the percentage of correct predictions on test data – as the measure of success.
But is it really the right thing to measure?
We touched on this when looking at document classification in Week 2, where we learned how to use “threshold curves” to show different tradeoffs between error types. But it’s not just document classification. In reality, when you look at the larger picture surrounding any deployment of machine learning, different errors almost always have different costs. Even with the weather: if today I undertake a certain activity because suitable weather is predicted, and that prediction is incorrect, I might suffer, even die. This – though thankfully not the dying part – happened to me just the other day, when I was sailing offshore in predicted 20 knot winds and was hit by an unforecasted 50 knot gust. (It was exciting, and dangerous.) On the other hand, if unsuitable weather had been incorrectly predicted, I might just have spent the day playing music instead of sailing, in which case little would have been lost.
At the end of this week you will be able to take different error costs into account when doing machine learning. Of course, we won’t be able to help you ascertain what the costs are. But if you know the different costs of incorrect predictions, you’ll be able to take this into account when measuring performance. You’ll also know how the output of an ordinary classifier can be post-processed to take account of different error costs, and – an alternative method – how to build a classifier that takes account of the costs internally.

<-- 4.12 Discussion -->
Examples of different error costs
Different error costs occur when the cost of mis-classifying an instance depends on its actual class and the class to which it has erroneously been assigned. More specifically, with two classes A and B, the cost of mis-classifying an instance that is actually an A as a B is different to the cost of mis-classifying an instance that is actually a B as an A.
For example, suppose you have a suspected life-threatening disease for which a particular expensive drug will probably save your life. The classes are (A) you actually have the disease, and (B) you don’t. What is the cost if the correct class is A but the doctor erroneously misclassifies you as B and therefore doesn’t prescribe the drug? Death! And what if the correct class is B but the doctor erroneously misclassifies you as A and therefore does prescribe the drug? Unnecessary expenditure. These are different costs!
We invite you to come up with other data mining/machine learning scenarios that have different error costs. But don’t stick to medicine … choose a different application area.
Or, if you like, try instead to come up with a real-life application scenario where the error costs are the same!

<-- 4.13 Video -->
Counting the cost
So far we’ve taken the classification rate – computed on a test set, or holdout, or cross-validation – as the measure of a classifier’s success. We’re trying to maximize the classification rate, that is, minimize the number of errors. But in real life, different kinds of error often have different costs. If the costs are known, they can be taken into account when evaluating a classifier’s performance. Error costs can also taken into account when using a learning method to create a classifier – regardless of which learning method is used – to get a classifier that minimizes the cost rather than the error rate.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello again. You know, the trouble with life is that sometimes everything just comes down to money. In this lesson and the next we’re going to look at counting the cost in data mining applications. What is success? Well, that’s a pretty good question, I suppose. In data mining terms, we’ve looked at the classification rate, measured on a test set, or holdout, or cross-validation. But essentially we’re trying to minimize the number of errors, or maximize the classification rate. In real life, different kinds of errors might have different costs, and minimizing the total errors might be inappropriate. Now we looked at the ROC curve in Class 2, and that shows you the different tradeoffs between the different error costs.
But it’s not really appropriate if you actually know the error costs: then, we want to pick a particular point on this ROC curve. We’re going to look at the credit rating dataset, credit-g.arff. It’s worse to class a customer as “good” when they’re “bad” then it is to class a customer as “bad” when they’re “good”. (In this dataset, the class value is “good” or “bad”.) The idea is that if you class someone as “good” when they’re “bad” and you give them a loan, then he’s going to run away with all your money, whereas if you make an error the other way round then you might have an opportunity to rectify it later on.
To tell you the truth, I know nothing about the credit rating industry, but let’s just suppose that’s the case. Furthermore, let’s suppose that the cost ratio is 5 to 1. I’ve got the credit dataset open here, and I’m going to run J48. What I get is an error rate of 29.5%, a success rate of 70–71%. Down here is the confusion matrix. I’ve copied those over here on to this slide. You can see that the cost here, the number of errors, is effectively the 183 plus 112, those off-diagonal elements of the confusion matrix. If errors cost the same amount, that’s a fair reflection of the cost of this confusion matrix.
However, if the cost matrix is different, then we need to do a different kind of evaluation. On the Classify panel, we can do a cost-sensitive evaluation. Let me go and do that for you. In the More options menu, we’re going to do a cost-sensitive evaluation. I need to set a cost matrix. This interface is a little weird. I want a 2×2 matrix; I’m going to resize this. Here we’re got a cost of 1 for both kinds of error, but I want a cost of 5 for this kind of error. Just close that and then run this again. Now I’ve got the same result, the same confusion matrix, but I’ve got some more figures here.
I’ve got a total cost of 1027 and an average cost of 1.027. (There are 1000 instances in this dataset.) Coming back to the slide, the cost here is computed by taking the 183 in the lower left and multiplying it by 5 – because that’s the cost of errors down there – and the 112 times 1, adding those up, and I get 1027. If I take the baseline, let’s go and have a look at ZeroR. I’m going to run ZeroR on this. Here it is. Here I get a cost of 1500. I get this confusion matrix. Over here on the slide, there’s the confusion matrix.
And although I’ve only got 300 errors here, they’re expensive errors, they each cost $5, so I’ve got a cost of 1500. This is classifying everything as “good”, because there are more “good” instances than “bad” in this dataset. If I were to classify everything as “bad” the total cost would only be 700. That’s actually better than either J48 or ZeroR. Obviously we ought to be taking the cost matrix into account when we’re doing the classification, and that’s exactly what the CostSensitiveClassifier does. We’re going to take the CostSensitiveClassifier, select J48, define a cost matrix, and see what happens. It’s meta > CostSensitiveClassifier, which is here.
I can define a classifier: I’m going to choose J48, which is here. I need to specify my cost matrix. I want it 2×2; I’ll need to resize that. I want to put a 5 down here. Cool. I’m just going to run it.
Now I get a worse classification error. We’ve only got 60–61% accuracy, but we’ve got a smaller cost, 658. And we’ve got a different confusion matrix. Back here on the slide you can see that. The old confusion matrix looked like this, and the new confusion matrix is the one [below]. You can see that the number 183 of expensive errors has been reduced to 66. That brings the cost down, the average cost, to 0.66 per instance instead of 1.027, despite the fact that we now have a worse classification rate. Let’s look at what ZeroR does with the CostSensitiveClassifier. It’s kind of interesting because we’re going to get a different rule.
Instead of classifying everything as “good”, we’re going to classify everything as “bad”. We’re going to make 700 mistakes, but they’re cheap mistakes. It’s only going to cost us $700. That’s what we’ve learned today. Is classification accuracy the best measure? Very likely it isn’t, because in real life different kinds of errors usually do have different costs. If you don’t know the costs, you might just want to look at the tradeoff between the error costs – different parts of the space – and the ROC curve is appropriate for that.
But if you do know the costs – the cost matrix – then you can do cost-sensitive evaluation to find the total cost on the test set of a particular learned model; or you can do cost-sensitive classification, that is, take the costs into account when producing the classifier.
The CostSensitiveClassifier does this: it makes any classifier cost-sensitive. How does it do this? Very good question. We’re going to find out in the next lesson.
<End Transcript>

<-- 4.14 Quiz -->
Cost-sensitive classification
Question 1
In the Explorer, load breast-cancer.arff and apply Naive Bayes (evaluate using 10-fold cross-validation throughout this Quiz). How many recurrence-events are misclassified as no-recurrence-events?
33
37
48
168
---
Correct answer(s):
48

<-- 4.14 Quiz -->
Cost-sensitive classification
Question 2
Assume that misclassifying a recurrence-event as a no-recurrence-event (and therefore terminating treatment prematurely) is twice as costly as the reverse (and therefore continuing treatment unnecessarily).
Check Cost-sensitive evaluation in the More options menu and specify a cost matrix that reflects this asymmetry in misclassification costs by doubling the cost of one error type.
Apply Naive Bayes again, and check that the evaluation cost matrix appears near the beginning of the output. What is output as the “total cost”?
81
114
129
162
---
Correct answer(s):
129
---
Feedback correct:
129 is 2x48 + 1x33
---
Feedback incorrect:
Perhaps you put “2” in the upper right-hand corner of the cost matrix? That is not the correct place.

<-- 4.14 Quiz -->
Cost-sensitive classification
Question 3
Now apply the CostSensitiveClassifier metalearner with Naive Bayes, defining the cost matrix appropriately (but leaving the other parameters at their default values).
How many recurrence-events are misclassified as no-recurrence-events now?
33
36
49
54
---
Correct answer(s):
36

<-- 4.14 Quiz -->
Cost-sensitive classification
Question 4
What is output as the “total cost”?
90
126
144
180
---
Correct answer(s):
126
---
Feedback correct:
126 is 2x36 + 1x54
Using cost-sensitive classification the cost decreases (from 129 without cost-sensitive classification to 126 with cost-sensitive classification) even though the total number of errors has increased (from 81 to 90 incorrectly classified instances).
The reason is that the number of critical misclassifications – the more costly ones – has decreased (from 48 to 36).

<-- 4.14 Quiz -->
Cost-sensitive classification
Question 5
Repeat the whole procedure with J48 instead of Naive Bayes.
When you change from ordinary to cost-sensitive classification, does the total number of errors increase?
Yes
No
---
Correct answer(s):
Yes
---
Feedback correct:
It increases from 70 (which is 62+8) to 96 (which is 55+41) incorrectly classified instances

<-- 4.14 Quiz -->
Cost-sensitive classification
Question 6
Using cost-sensitive classification, is the number of critical misclassifications – the more costly ones – smaller with J48 than with Naive Bayes?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
For J48 there are 55 critical misclassifications, compared with 36 for Naive Bayes

<-- 4.14 Quiz -->
Cost-sensitive classification
Question 7
Again using cost-sensitive classification, is the total cost smaller with J48 than with Naive Bayes?
Yes
No
---
Correct answer(s):
No
---
Feedback correct:
For J48 the total cost is 151, compared with 126 for Naive Bayes.

<-- 4.14 Quiz -->
Cost-sensitive classification
Question 8
Still using J48, alter the cost matrix to treat misclassifying a recurrence-event as a no-recurrence-event as five times as costly as the reverse (instead of twice as costly as before), and repeat the experiment.
What is the total number of errors now?
24
121
141
165
---
Correct answer(s):
165
---
Feedback correct:
141 no-recurrence events are classified as recurrence-events, and 24 recurrence-events are classified as no-recurrence-events
---
Feedback incorrect:
24 recurrence-events are classified as no-recurrence-events, but the question asks for the total number of errors
---
Feedback incorrect:
141 no-recurrence events are classified as recurrence-events, but the question asks for the total number of errors

<-- 4.14 Quiz -->
Cost-sensitive classification
Question 9
What is the number of critical (i.e., more costly) misclassifications?
24
60
61
141
---
Correct answer(s):
24

<-- 4.14 Quiz -->
Cost-sensitive classification
Question 10
What is the total cost now?
(Note: you must specify the new cost matrix in the More options menu, as well as in the CostSensitiveClassifier.)
165
189
261
729
---
Correct answer(s):
261
---
Feedback correct:
261 is 5x24 + 1x141
---
Feedback incorrect:
189 is 2x24 + 1x141. Perhaps you didn’t change the evaluation cost matrix in the More options menu?

<-- 4.14 Quiz -->
Cost-sensitive classification
Question 11
Remember that old rule, always check your results against a baseline classifier? With this cost matrix, cost-sensitive classification with ZeroR does even better!
What is the total cost for ZeroR (with all the same settings)?
5
201
206
1005
---
Correct answer(s):

<-- 4.14 Quiz -->
Cost-sensitive classification
Question 11
Remember that old rule, always check your results against a baseline classifier? With this cost matrix, cost-sensitive classification with ZeroR does even better!
What is the total cost for ZeroR (with all the same settings)?
5
201
206
1005
---
Correct answer(s):
201
---
Feedback correct:
ZeroR classifies everything as a recurrence-event. With a multiplier of 5 for the cost of misclassifying a recurrence-event as a no-recurrence-event, this is a very good idea!

<-- 4.15 Video -->
Cost-sensitive classification 
There are two different ways to make a classifier cost-sensitive. One is to create the classifier in the usual way, striving to minimize the number of errors rather than their cost – but then adjust its output to reflect the different costs by recalculating the probability threshold used to make classification decisions. This can be done even for methods that don’t use probabilities explicitly. The second is to learn a different classifier, one that takes the costs into account internally rather than by post-processing the output. This can be done by re-weighting the instances in a way that reflects the error costs.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
There are two ways of making a classifier cost-sensitive. The terminology is a little bit confusing. The first method is going to be called “cost-sensitive classification” and the second method is going to be called “cost-sensitive learning”. For cost-sensitive classification, what we do is adjust a classifier’s output by re-calculating the probability threshold. I’ve opened the german_credit dataset, with 1000 instances. I’m going to classify this with Naive Bayes. I get this matrix here. If I set Output predictions, which I’ve set, then I can see in the output the actual predictions for the 1000 instances. I’ve written those down here – not all 1000, I’ve just taken every 50.
I’ve got 20 results here: the actual class of the instance, the predicted class of the instance, and Naive Bayes’ probability that the instance is a “good” one rather than a “bad” one. And I’ve sorted this list by the probability column. In fact, the effect of Naive Bayes is, it looks to see if the “good” probability is bigger than the “bad” probability, which is the same as saying, “is the good probability bigger than 0.5?” It’s like drawing a horizontal line at 0.5, between instance number 750 and instance number 800. Everything above that line is going to be classified as “good”, and everything below the line is going to be classified as “bad”.
Going back to that “classified as” matrix, the confusion matrix, 605 plus 151, that’s 756 instances that are going to be classified as “good”. 95 plus 149, that’s [244], are going to be classified as “bad”. Then, within those, if we were to look at the matrix with the actual classes, and we counted the number of “bad” ones above the line, we find 151 “bad” ones above the line – those are misclassifications – and 95 “good” ones below the line, which are misclassifications. We don’t actual have to use a threshold of 0.5. This is exactly the same table of the actual and predicted, but I’ve changed the threshold to 0.833.
That gives me the classification matrix that’s shown here, and a total cost – using the cost matrix we were talking about in the last [lesson], where one kind of error costs 5 times the cost of the other kind of error – we get a total cost here of 517, versus 850 for the threshold of 0.5 on the previous slide. You can see that if you count up the numbers above the line, then there are 501 of them (448+53), of which 53 are “bad”. Then count up the number below the line and look at the number of “good” ones there; those are the errors.
In general, it’s not hard to show that, given a general cost matrix 0, λ, μ, 0, you minimize the expected cost by classifying an instance as “good”, setting the threshold at μ/( λ + μ), which is where we got the 0.833 from for this problem. That’s what you do for Naive Bayes, but what about methods that don’t produce probabilities? Well, they almost all do produce probabilities. Let’s look at J48. Imagine J48 with minNumObj set to 100. I’ve done this to force a small tree. I won’t do it for you, but I’d get the tree shown here. If I look at the tree, the leaves of the tree have effectively got probabilities.
The leftmost leaf at the bottom is predicting “good”, and there are 37 exceptions, 37 “bad” instances. The “good” probability for this leaf is 1 – 37/108 (the total number of instances that reach that leaf), which is 0.657. You’ll find that [number] in the list of probabilities in the table on the right. The next leaf is predicting “bad”, and there are 68 out of 166 exceptions. So the “good” probability for that leaf is 0.410, and you’ll see that number in the list in the table on the right. And so on. We can get probabilities from J48, and from other methods as well. Let’s do this. To do this in Weka, we use the CostSensitiveClassifier with “minimizeExpectedCost = true”.
I’ve got the credit dataset open. If I just run J48 with that cost matrix, I get a cost of 1027. Over in Weka, I’m going to select the CostSensitiveClassifier, Meta > CostSensitiveClassifier. I’m going to configure that to have the appropriate cost matrix. I need to put in the cost matrix here, a 2 by 2 cost matrix, and I want the one we’ve been using all along, with a 5 there. Then I want to set minimizeExpectedCost to true. That gives us cost-sensitive classification. If I run that with J48 (did I select J48? No; I should have selected J48 here). Now, if I run that with J48, I get this little matrix here, and a total cost of 770.
In fact, back on the slide, that’s the middle section of the slide, the cost of 770 with the confusion matrix that’s shown. Actually, J48 isn’t very good at producing probabilities, and it’s advantageous to use bagging. We talked about bagging in Data Mining with Weka J48 produces a restricted set of probabilities, but using the bagging technique enriches the set of probabilities produced. If you just used bagged J48 – I won’t do this for you, but if you used that as the classifier – then you’d get a lower cost, a better confusion matrix, with a cost of 603. Or 0.603, because there are 1000 instances. That was what we’re calling “cost-sensitive classification”, where you adjust the probability threshold.
The second method we’re going to call “cost-sensitive learning”, where, instead of adjusting the output of the classifier, the probability threshold, we’re going to learn a different classifier. Here’s a way to think about that. Suppose we created a new dataset by replicating some instances in the old dataset. To simulate the cost matrix we’ve been talking about, suppose we added 4 copies of every “bad” instance. The new dataset would have 700 “good” instances and 1500 “bad” instances. And rerun, say, J48. When you think about it, that will give errors on the “bad” instances effectively a weight of 5 to 1 more expensive than errors on the “good” instances. In practice, we won’t actually copy the instances, we’ll re-weight them internally in Weka.
The way to do this is to use the same classifier, CostSensitiveClassifier, but set minimizeExpectedCost to false. We had it true before, now we’re going to set it to false, which is the default. We’re going to try that with Naive Bayes and J48. Here we are. Let’s use J48 first. We’re going to set minimizeExpectedCost to false, and run that. Now we get a total cost of 658 with this confusion matrix.
That corresponds to the middle line on this slide: J48 has a cost of 658. If we were to use Naive Bayes, we’d get a cost of 530; and if we used bagged J48, we’d get a cost of 581. In general, these are a little bit better – certainly for J48, the results of cost-sensitive learning are a little better than the results of cost-sensitive classification that we looked at before. Here’s what we’ve learned. Cost-sensitive classification adjusts a classifier’s output to optimize a given cost matrix. Cost-sensitive learning, on the other hand, learns a new classifier to optimize with respect to a given cost matrix, effectively by duplicating – or, really, internally re-weighting – the instances in accordance with the cost matrix.
Both of these are done with the Weka classifier CostSensitiveClassifier; [it] implements both of those with a switch to choose which one to use. And there are ways in Weka to store and load the cost matrix automatically.
<End Transcript>

<-- 4.16 Quiz -->
Comparing cost-sensitive techniques
Question 1
What is the total weighted cost for plain Naive Bayes?
25
40
86
124
---
Correct answer(s):
86
---
Feedback correct:
5x15.29 + 1x9.55 = 86

<-- 4.16 Quiz -->
Comparing cost-sensitive techniques
Question 2
What is the total weighted cost for cost-sensitive learning (CostSensitiveClassifier with Naive Bayes and default parameters)?
31
53
133
156
---
Correct answer(s):
53
---
Feedback correct:
5x5.58 + 1x25.54 = 53.44

<-- 4.16 Quiz -->
Comparing cost-sensitive techniques
Question 3
What is the total weighted cost for cost-sensitive classification (CostSensitiveClassifier with Naive Bayes and minimizeExpectedCost = true)?
31
50
53
155
---
Correct answer(s):
53
---
Feedback correct:
5x5.53 + 1x25.53 = 53.18.
Cost-sensitive learning (Q.2) and cost-sensitive classification (this question) give pretty well the same result with Naive Bayes. Both do much better than plain Naive Bayes (Q.1).

<-- 4.16 Quiz -->
Comparing cost-sensitive techniques
Question 4
What is the total weighted cost for plain J48?
47
100
101
144
---
Correct answer(s):
101
---
Feedback correct:
5x18.07 + 1x10.68 = 101.03

<-- 4.16 Quiz -->
Comparing cost-sensitive techniques
Question 5
What is the total weighted cost for J48 with cost-sensitive learning?
41
47
63
66
---
Correct answer(s):
66
---
Feedback correct:
5x6.39 + 1x34.53 = 66.48

<-- 4.16 Quiz -->
Comparing cost-sensitive techniques
Question 6
What is the total weighted cost for J48 with cost-sensitive classification?
34
78
124
168
---
Correct answer(s):
78
---
Feedback correct:
5x11.02 + 1x22.64 = 77.74
For J48, cost-sensitive learning (Q.5) gives a better result than cost-sensitive classification (this question). Both do much better than plain J48 (Q.4).

<-- 4.16 Quiz -->
Comparing cost-sensitive techniques
Question 7
What is the total weighted cost for J48 with cost-sensitive classification with bagging?
39
58
60
172
---
Correct answer(s):

<-- 4.16 Quiz -->
Comparing cost-sensitive techniques
Question 7
What is the total weighted cost for J48 with cost-sensitive classification with bagging?
39
58
60
172
---
Correct answer(s):
60
---
Feedback correct:
5x5.23 + 1x33.84 = 59.99
Bagging improves the result of cost-sensitive classification using J48.

<-- 4.17 Article -->
Cost-sensitive classification conclusions
Even if you don’t do the exercise, you should look at the numbers and note that they support the following general conclusions for the credit-g dataset with a particular cost matrix.
    Naive Bayes is generally better than J48 on this problem.
    Using cost-sensitive classification or cost-sensitive learning always improves the result.
    For Naive Bayes, cost-sensitive learning gives about the same result as cost-sensitive classification.
    For J48, cost-sensitive learning gives a better result than cost-sensitive classification.
    Bagging improves the result of cost-sensitive classification using J48.
(Note incidentally that the Experimenter differs from the Explorer in that it calculates the four elements of the confusion matrix, Num_false_positives etc, separately for each cross-validation fold and averages them, whereas the Explorer sums them over the entire dataset. Thus, since we are using 10-fold cross-validation in this activity, the numbers, and the costs, produced by the Experimenter are 10% of those produced by the Explorer. The Experimenter does this because a separate confusion matrix is needed for each fold in order to compute the standard deviation of each element and to do significance tests between corresponding elements produced by different classifiers.)

<-- 4.18 Discussion -->
Reflect on this week's Big Questions
The Big Questions this week are, “How about selecting key attributes before applying a classifier?” and “What happens when different errors have different costs?”
We promised that by the end you’d be able to explain – and apply – different ways of selecting a subset of attributes that work well for the problem at hand. You’d be able to use different measures to evaluate that subset. And you’d be able to adopt different search strategies to reduce the computation by avoiding the need to test all possible subsets. You’d also be able to do attribute selection and classification within the cross-validation operation, so that attribute selection was based on the training set for each fold, not the entire dataset.
We also promised that you’d be able to take different error costs into account when doing machine learning. If you knew the different costs of incorrect predictions, you’d be able to take them into account when measuring performance. You’d also know how to post-process the output of an ordinary classifier to take account of different error costs, and – an alternative method – how to build a classifier that takes costs into account internally.
And you’d be able to do all these things in Weka.
And, hopefully, explain them to a colleague!
So: can you? Tell your fellow learners how you get on.

<-- 4.19 Article -->
Index
(A full index to the course appears at the end of Week 1.)
      Topic
      Step
      Datasets
      Breast-cancer
      4.14
      Credit-g
      4.13, 4.15, 4.16
      Diabetes
      4.4
      Glass
      4.2, 4.3, 4.5
      Ionosphere
      4.7, 4.9
      Reuters-Corn-train/test
      4.8, 4.10
      Classifiers
      IBk
      4.3, 4.5, 4.7, 4.9
      J48
      4.2, 4.4, 4.5, 4.9, 4.13, 4.14, 4.15, 4.16
      NaiveBayes
      4.4, 4.5, 4.7, 4.8, 4.9, 4.10, 4.14, 4.15, 4.16
      NaiveBayesMultinomial
      4.8, 4.10
      OneR
      4.3
      ZeroR
      4.13, 4.14
      Metalearners
      AttributeSelectedClassifier
      4.4, 4.5, 4.7, 4.9, 4.10
      Bagging
      4.16
      CostSensitiveClassifier
      4.13, 4.14, 4.15, 4.16
      FilteredClassifier
      4.4
      Filters
      StringToWordVector
      4.8, 4.10
      Packages
      AttributeSelectionSearchMethods
      4.3
      Plus …
      Cost-sensitive evaluation
      4.13, 4.14
      Experimenter interface
      4.5, 4.16
      ROC curve (AUC)
      4.8, 4.10, 4.13
      Select attributes panel
      4.2, 4.3, 4.4, 4.5, 4.8, 4.10

<-- 5.0 Todo -->
Neural networks, learning curves, and performance optimization
What are "neural networks" and how can I use them?
This week's first Big Question!
5.1
What are "neural networks" and how can I use them?
article
Simple neural networks 
The "Perceptron" is the simplest form of neural network.
The basic perceptron implements a linear decision boundary
... though modern improvements allow more complex boundaries.
5.2
Simple neural networks 
video (07:44)
5.3
Investigating the Voted Perceptron
quiz
Multilayer perceptrons 
Multilayer perceptrons are networks of perceptrons, with an input layer, an output layer, and (perhaps many) "hidden layers". They can implement arbitrary decision boundaries, but have practical limitations.
5.4
Multilayer perceptrons
video (09:09)
5.5
Investigating multilayer perceptrons
quiz
5.6
Performance of the multilayer perceptron
article
5.7
 The deep learning renaissance
article
5.8
Share stories on deep learning
discussion
How much training data do I need? And how do I optimize all those parameters? 
This week's second Big Question! (two questions, really)
5.9
How much training data do I need? And how do I optimize all those parameters? 
article
Learning curves 
Find out how much data you need by plotting a learning curve using the "resample" filter (which allows sampling with or without replacement). You can avoid sampling the test set by using the FilteredClassifier.
5.10
Learning curves 
video (06:24)
5.11
Learning curves
quiz
Performance optimization 
Weka has several "wrapper" metalearners that optimize parameters for best performance:
CVParameterSelection, GridSearch, and ThresholdSelector.
You should avoid optimizing parameters manually: you're bound to overfit!
5.12
Performance optimization 
video (08:55)
5.13
Optimizing IBk
quiz
ARFF and XRFF 
The ARFF format can encode sparse data, weighted instances, and relational attributes.
Some Weka filters and classifiers take advantage of sparsity to reduce space and increase speed.
There's an XML version of ARFF, called XRFF.
5.14
ARFF and XRFF 
video (05:42)
5.15
Advanced ARFF
quiz
There's no magic in data mining
There's no magic in data mining – no universal "best" method. It's an experimental science. You've learned a lot – but there's plenty more! Data mining is a powerful technology: please use it wisely.
5.16
Summary
video (05:54)
5.17
How to cheat: A case study
quiz
5.18
Corruption and cheating
discussion
5.19
Reflect on this week's Big Questions
discussion
Farewell
It's time to say goodbye again.
5.20
Post-course assessment
test
This is a test step, it helps you verify your understanding. If you want to earn a Certificate of Achievement on this course you need to complete this test and any others, scoring an average of 70% or above.
To take tests you need to upgrade this course.  It costs $94, which also gets you:
Unlimited access to the course for as long as it exists on FutureLearn, so you can learn at your own pace
A Certificate of Achievement when you’re eligible, to prove what you’ve learned
Upgrade
Find out more
5.21
Farewell
article
5.22
Index
article

<-- 5.1 Article -->
What are "neural networks" and how can I use them?
Neural networks are a computational approach based on a large collection of primitive units connected together in a simple, regular, way.
Some people call the units “neurons,” and claim that this loosely models the way a biological brain solves problems with large clusters of biological neurons connected by axons. If so, the key word is loosely! Drawing an analogy with that amazing organ inside your head provides an element of sex appeal that is entirely unjustified, in my opinion.
The individual units of so-called “neural networks” – I prefer the term “Perceptron”, the historical name under which these things were introduced in the 1960s – are disarmingly simple. And the connections are also disarmingly simple.
You’ll be asking: Exactly what do these units do? How do the connections work? Where does the learning come in? What decision boundaries can neural networks (aka “multilayer perceptrons”) create? How do you design these things? And can you do it in Weka?
By the end of the week you’ll know. You’ll be able to create your own neural network structures, and make them learn. And you’ll experience the role that patience plays in the process.

<-- 5.2 Video -->
Simple neural networks 
Neural network learning methods invite an analogy to the brain that is seductive but entirely misleading! The simplest form of neural network, called a “Perceptron”, implements a linear decision boundary. It operates iteratively, and the result depends on the order in which the instances are presented, but there is a theorem that proves that under certain circumstances it will converge rather than cycle indefinitely. The Perceptron has been the subject of much controversy during its 60-year life span.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello! Well, we’ve come to Class 5, the last class of More Data Mining with Weka. Congratulations on having got this far. In this class, we’re going to look at some miscellaneous things. We’ll have a couple of lessons on neural networks and the Multilayer Perceptron. Then we’ll take a quick look at learning curves and performance optimization in Weka. Then we’ll come back and have another look at the ARFF file format. You’ve been listening to me talking for quite a long time now, and I just wonder if you might be interested in finding out a little bit more about me. If so, if you go to the web and search for “A stroll through the gardens of computer science” in quotes.
So you’ve got to get it exactly right: “A stroll through the gardens of computer science”.
You’ll get just one result, or, I got just one result: News from New Zealand. This, in fact, is an interview with me, an extended interview. It starts on the next page. A dialogue with me. You’ll learn where I came from and what I’ve done and what I’ve been doing and what I’ve been thinking of and some of my biases. That might be interesting or not. It’s up to you. Let’s get back to the lesson. We’re going to talk about simple neural networks. Now, a lot of people love neural networks. I’m not one of them. I think it’s a brilliant term, “neural network”, because it conjures up the image on the left of some really cool brain-like mechanism.
Actually, you should think of the rather grungy picture on the right, a linear sum. We’ll talk about that in a minute. The very name is suggestive of intelligence. However, the reality, I think, is not. In this lesson, we’re going to talk about the simplest neural network, the Perceptron. It’s a simple learning method that determines the class in a two-class dataset using a linear combination of attributes. For test instance a – that is, with attributes a1, a2, a3 – then we take the sum w0 plus w1a1 plus w2a2 and so on over all the attributes. We’ll express that as a sigma from j=0. We’re implicitly defining a0 as 1 here just to make the notation look nice.
If the result, x, is greater than zero, then we’re going to say that instance belongs to class 1; otherwise, we’re going to say it belongs to class 2. This, of course, works most naturally with numeric attributes. Where do the weights come from? That’s the big question. We have to learn them. Here’s the algorithm. We start by setting all weights to zero until all the instances in the training data are classified correctly. We continue for each instance in the training data. If it’s classified correctly, then we do nothing.
If it’s classified incorrectly, then, if it belongs to the first class we’ll add it to the weight vector, and if it belongs to the second class, we’ll subtract it from the weight vector. There’s a theorem that if you continue to do this (the Perceptron Convergence Theorem), it will converge if you cycle repeatedly through the training data, perhaps many times. It will converge providing the problem is linearly separable, that is, there exists a straight line that separates the two classes, class 1 and class 2. Actually, we talked about linear decision boundaries before when we talked about Support Vector Machines.
They were also restricted to linear boundaries, but they can get more complex boundaries using the “Kernel trick”, which I mentioned but did not explain back then in Data Mining with Weka. And I’m not going to explain it now, but I’m just going to tell you that the Perceptron can use the same trick to get non-linear boundaries. The Weka implementation is called the Voted Perceptron, a slightly different algorithm. It stores all of the weight vectors, all versions of the weight vector, and lets them vote on test examples. Their importance, the weight vectors are themselves weighted according to the length of time that they survived before the weights got changed.
You know, we’re going to use a weight vector, keep classifying training instances, and when the system makes a mistake, then we’re going to change the weight vector. The survival time is some kind of indication of how successful that version of the weight vector is. This is claimed to have many of the advantages of Support Vector Machines, but it’s faster, simpler, and nearly as good. We’ll take a look. I’m going to look at the ionosphere dataset. I’ve got it open here in Weka. I’m going to go to Classify, and the VotedPerceptron is in the functions category.
If I select that – there’s a bunch of options, but we won’t worry about that – and just run it using cross-validation, I get 86%. If I were to choose SMO, then I would get 89%. Back to the slide. For the German credit data, we also get slightly better performance with SMO. For the breast cancer dataset, they are almost exactly the same, and for the diabetes, again SMO is a little bit better. It’s certainly true that the VotedPerceptron is faster, maybe 2 times, 5 times, perhaps up to 10 times depending on the dataset. The Perceptron’s got a long history. It was first published in 1957, the basic Perceptron algorithm. It was derived from theories about how the brain works.
It’s an acronym for “a perceiving and recognizing automaton”, and a guy called
Rosenblatt published a book in 1958 called “Principles of neurodynamics: Perceptrons and the theory of brain mechanisms”. Very suddenly, in 1970, it went out of fashion with a book by two well-known computer scientists, called “Perceptrons”, and they showed that there were some simple things that Perceptrons simply couldn’t do. They proved theorems about what Perceptrons could and couldn’t do. This is the cover of their famous book that basically took Perceptrons off the map. Until 1986, when they came back rebranded “connectionism”, the movement was the “connectionist movement”, and a couple of guys wrote another book “Parallel distributed processing”. Some people claim that artificial neural networks mirror brain function, just like Richard Rosenblatt did back in the 50’s.
The main form of Perceptron the connectionists use is a Multilayer Perceptron, which is capable of drawing nonlinear decision boundaries using an algorithm called the backpropagation algorithm that we’ll look at in the next lesson. Here’s the summary. The basic Perceptron algorithm implements a linear decision boundary. It’s very reminiscent of classification by regression. It works with numeric attributes. It’s an iterative algorithm, and it depends on the order in which it encounters the training instances, the result depends on the order. Actually, many years ago, in 1971, I described a simple improvement to the Perceptron in my Master’s thesis, but I’m still not very impressed with the Perceptron stuff; sorry about that.
Recently, there have been some improvements: the use of the Kernel trick to get more complex boundaries, and this Voted Perceptron strategy with multiple weight vectors and voting.
<End Transcript>

<-- 5.3 Quiz -->
Investigating the Voted Perceptron
Question 1
Load iris.arff into the Weka Explorer. How does VotedPerceptron perform on this dataset?
Worse than 75% accuracy
Better than 75% accuracy
It doesn’t run
---
Correct answer(s):
It doesn’t run
---
Feedback correct:
VotedPerceptron can’t handle this data

<-- 5.3 Quiz -->
Investigating the Voted Perceptron
Question 2
What’s the reason for the answer to the previous question?
There are few class values
There are more than 2 class values
There are missing values
---
Correct answer(s):
There are more than 2 class values
---
Feedback correct:
VotedPerceptron can’t handle problems with more than 2 class values.
Weka doesn’t show the reason because it simply refuses to run VotedPerceptron on this data, but you can trick it into trying to run it by using a meta-classifier (e.g. AttributeSelectedClassifier) and specifying VotedPerceptron. Then Weka will tell you that this classifier can’t handle a multi-valued nominal class.

<-- 5.3 Quiz -->
Investigating the Voted Perceptron
Question 3
Delete all Iris-setosa instances and try again.
You can delete the instances either by using the RemoveWithValues attribute filter or by multiply selecting these instances in the Edit window and right-clicking to delete. In either case you will have to save the file and change the header by editing it. Alternatively, if you are lazy (like me), you can avoid editing the header by saving as CSV and reloading. (Why does this work?)
What accuracy does VotedPerceptron achieve on this 2-class Iris data?
75%
89%
98%
100%
---
Correct answer(s):
89%

<-- 5.3 Quiz -->
Investigating the Voted Perceptron
Question 4
How does this compare with SMO, J48 and Naive Bayes?
VotedPerceptron performs better than some
VotedPerceptron performs better than all three
VotedPerceptron performs worse than all three
---
Correct answer(s):
VotedPerceptron performs worse than all three
---
Feedback correct:
Bummer!

<-- 5.3 Quiz -->
Investigating the Voted Perceptron
Question 5
Let’s check whether VotedPerceptron is affected by the order of presentation of instances.
Randomize the order (hint: check for a suitable filter) and try again.
Does this make it generally better or generally worse?
Generally better
Generally worse
---
Correct answer(s):
Generally worse
---
Feedback correct:
I got 77%, 79%, 89% and 72% after successive applications of the Randomize instance filter with the default parameter, compared to 89% before randomization.

<-- 5.3 Quiz -->
Investigating the Voted Perceptron
Question 6
Use the Experimenter to compare VotedPerceptron with SMO on breast-cancer.arff, credit-g.arff, diabetes.arff, and ionosphere.arff (all 2-class datasets).
For which datasets does VotedPerceptron significantly outperform SMO?
Select all the answers you think are correct.
breast-cancer
credit-g
diabetes
ionosphere
none
---
Correct answer(s):
none
---
Feedback correct:
Bummer!

<-- 5.3 Quiz -->
Investigating the Voted Perceptron
Question 7
For which datasets does SMO significantly outperform VotedPerceptron?
Select all the answers you think are correct.
breast-cancer
credit-g
diabetes
ionosphere
none
---
Correct answer(s):
credit-g
diabetes

<-- 5.3 Quiz -->
Investigating the Voted Perceptron
Question 8
Disregarding significance, for which datasets does VotedPerceptron outperform SMO?
Select all the answers you think are correct.
breast-cancer
credit-g
diabetes
ionosphere
none
---
Correct answer(s):
breast-cancer

<-- 5.3 Quiz -->
Investigating the Voted Perceptron
Question 9
Disregarding significance, for which datasets does SMO outperform VotedPerceptron?
Select all the answers you think are correct.
breast-cancer
credit-g
diabetes
ionosphere
none
---
Correct answer(s):
credit-g
diabetes
ionosphere

<-- 5.3 Quiz -->
Investigating the Voted Perceptron
Question 10
Change the “Comparison field” to UserCPU_Time_training.
Is VotedPerceptron significantly faster than SMO?
Yes
No
---
Correct answer(s):
Yes
---
Feedback correct:
On my machine it’s significantly faster on all 4 datasets, although on other machines the speed difference on the diabetes dataset may not be significant

<-- 5.3 Quiz -->
Investigating the Voted Perceptron
Question 11
Let’s finish by turning to our old workhorse, J48, on the same 4 datasets (breast-cancer.arff, credit-g.arff, diabetes.arff, and ionosphere.arff), again using the Experimenter to do the comparison.
Does VotedPerceptron outperform J48?
Sometimes
Always
Never
---
Correct answer(s):
Never
---
Feedback correct:
Bummer!

<-- 5.3 Quiz -->
Investigating the Voted Perceptron
Question 12
Is VotedPerceptron always faster to train than J48?
About the same
Yes
No
---
Correct answer(s):
About the same
---
Feedback correct:
On my computer, VotedPerceptron is significantly faster in terms of UserCPU_Time_training on breast-cancer, diabetes and ionosphere; significantly slower on credit-g. However, other computers report no significant differences.

<-- 5.3 Quiz -->
Investigating the Voted Perceptron
Question 12
Is VotedPerceptron always faster to train than J48?
About the same
Yes
No
---
Correct answer(s):

<-- 5.4 Video -->
Multilayer perceptrons
Multilayer perceptrons are networks of perceptrons, networks of linear classifiers. In fact, they can implement arbitrary decision boundaries using “hidden layers”. Weka has a graphical interface that lets you create your own network structure with as many perceptrons and connections as you like. A quick test showed that a multilayer perceptron with one hidden layer gave better results than other methods on two out of six data sets – not too bad. But it was 10–2000 times slower than other methods, which is a bit of a disadvantage.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
In the last lesson, we looked at the basic Perceptron algorithm, and now we’re going to look at the Multilayer Perceptron. Multilayer Perceptrons are simply networks of Perceptrons, networks of linear classifiers. They have an input layer, some hidden layers perhaps, and an output layer. If we just look at the picture on the lower left, the green nodes are input nodes. This is actually for the numeric weather data. Although you probably can’t read the labels, the top one is “outlook=sunny”; underneath is “outlook=overcast”; then “outlook=rainy”; and then we have “temperature”, “humidity” and “windy” for the nodes. This is the numeric weather data, so “outlook” is the only nominal variable, and that’s been made into three binary attributes.
These two [yellow] nodes are the output nodes for output is “play” and “don’t play”, respectively. Each of those two yellow nodes performs a weighted sum, and each of the connections has a weight. If we look at the more complicated picture to the right, we’ve got some red nodes here. These are three hidden layers with different numbers of neurons/nodes in each of these three hidden layers. Each node performs a weighted sum of its inputs and thresholds the result, just like in the regular, basic Perceptron. But in the basic Perceptron, you looked to see whether the result was greater than zero or less than zero. In Multilayer Perceptrons, instead of using that hard-edged function, people use what’s called a “sigmoid” function.
I’ve drawn a few sigmoid functions on the slide up in the top right. You can see that as they become more extreme, they approach the step function, which corresponds to the hard-edged threshold used in the basic Perceptron. But here we’re going to use a smooth, continuous sigmoid function. Actually, there is a theoretical property that the network will converge if the sigmoid function is differentiable. That’s kind of important. Anyway, that’s by the by. These nodes are often called “neurons”, the red nodes and the yellow nodes. These are not to be confused with the neurons that you have in your head. The big questions are how many layers, and how many nodes in each?
We know for the input layer, we’re going to have one for each attribute, and the attributes are numeric or binary. For the output layer, we’re going to have one for each class. How many hidden layers? Well, that’s up to you. If you have zero hidden layers, that’s the standard Perceptron algorithm. That’s suitable if the data is linearly separable.
There are theoretical results: with one hidden layer, that’s suitable for a single, convex region of the decision space; two hidden layers are enough to generate arbitrary decision boundaries. However, people don’t necessarily use two hidden layers, because that really increases the number of connections – that’s the number of weights that would have to be learned.
The next big question is: how big should the layers be? They are usually chosen somewhere between the input and output layers. A common heuristic, Weka’s heuristic, is to use the mean value of the [number of] input and output layers. What are these weights? Well, they’re learned. They’re learned from the training set. They are learned by iteratively minimizing the error using the steepest descent method, and the gradient is determined using a backpropagation algorithm. We’re not going to talk about the backpropagation here. The change in weight is computed by multiplying the gradient by a constant called the “learning rate” and adding the previous change in weight multiplied by another parameter called “momentum”.
So Wnext (the next weight vector) is W + ΔW, where ΔW is minus the learning_rate times the gradient (minus because we want to go downhill) plus momentum times the previous change in the weight parameter. Multilayer Perceptrons can get excellent results, but they often involve a lot of experimentation with the number and size of the hidden layers and the value of the learning rate and momentum parameters. Let’s take a look in Weka. I’m going to use the numeric weather data. Over here, I’ve got it open. I’m going to go to Classify and find MultilayerPerceptron in the functions category. Here it is, and let’s just run it. We get 79%. I want to show you the network we used.
Let me just switch on GUI, the graphical user interface. Now when I run it, I get a picture of the network. That is Weka’s default network. These are the input nodes that we looked at before, the green ones. Weka has chosen 4 neurons in the hidden layer. That’s the average of the number of input and output layers. There are 2 output neurons.
Going back to the slide: when I tried IBk, I also get 79% on this data set. J48 and so on do worse. However, it’s just a toy problem, so those results aren’t really indicative. On real problems Multilayer Perceptrons often do quite well, but they’re slow.
There are a number of parameters: the number of hidden layers and the size of the hidden layers; the learning rate and momentum. The algorithm makes multiple passes through the data, and training continues until the error on the validation set consistently increases – that is, if we start going uphill – or the training time is exceeded, the maximum number of epochs allowed. Going back to Weka, I’m going to configure this to use 5 neurons, 10 neurons, and 20 neurons in 3 hidden layers. Look at this! You can see the three hidden layers with 5, 10, and 20 neurons – an awful lot of weights here. We’ve got the learning rate, so we can change the momentum.
We’ve got the maximum number of epochs. We can just run that. Also, in Weka, you can create your own network structure. You can add new nodes, add connections, and delete nodes and so on. I’m going to go back to Weka, and I’m just going to use the default number of hidden layers. I’ve now got my 4 neurons in the 1 hidden layer. I’m going to add another hidden layer. If I click empty space, I create a neuron. It’s yellow, which means it’s selected. I’m going to deselect it by clicking empty space, and create another couple. With this one here, I’m going to connect it up to this.
If I click these, they connect the selected neuron – that is, the yellow one – to the one I click. Then I can deselect it and select this one and make connections here. You can see it’s pretty quick to add connections. I’ve added another hidden layer. Well, I need to do some things with the output here, but you can get the idea from this. We can click to select a node and right-click an empty space to deselect. We can create and delete nodes by clicking in empty space to create and right-clicking to delete. We can create and delete connections, and we can set parameters in this interface too. Are they any good?
Well, I tried the Experimenter with 6 datasets, and I used 9 algorithms. MultilayerPerceptron gave me the best results on 2 of the 6 datasets.
The other wins were: SMO won on another 2 datasets; J48 and IBk won on 1 dataset each. When I say “win”, I mean beat all the other methods. MultilayerPerceptron was not too bad, but in fact it was between 10 and 2000 times slower than other methods, which is a bit of a disadvantage. Here’s the summary. Multilayer Perceptrons can implement arbitrary decision boundaries given two or more hidden layers, providing you’ve got enough neurons in the hidden layers, and providing they’re trained properly. Training is done by backpropagation, which is an iterative algorithm based on gradient descent. In practice, you get quite good performance, but Multilayer Perceptrons are extremely slow. I’m still not a fan of Multilayer Perceptrons; I’m sorry about that.
They might be a lot more impressive on more complex datasets; I don’t know. But for me, configuring Multilayer Perceptrons involves too much messing around.
<End Transcript>

<-- 5.5 Quiz -->
Investigating multilayer perceptrons
Question 1
What happens if you select Cross-validation as the Test option, instead of Use training set?
The neural network won’t run
You have to dismiss it 11 times
---
Correct answer(s):
You have to dismiss it 11 times
---
Feedback correct:
Once for each cross-validation fold, and once when it is trained on the entire dataset. It’s a nuisance.

<-- 5.5 Quiz -->
Investigating multilayer perceptrons
Question 2
The default neural network configuration, which you have just seen, has one hidden layer whose size is the average of the input and output layers – that’s what specifying “a” as the hiddenLayers parameter does.
Experiment with the interface to learn how to create a network with two hidden layers of this same size. What do you need to specify as the hiddenLayers parameter?
2
a,2
a,a
---
Correct answer(s):
a,a

<-- 5.5 Quiz -->
Investigating multilayer perceptrons
Question 3
Experiment with the interface to learn how to create a network with no hidden layers.
What do you need to specify for hiddenLayers?
0
a,0
Nothing. Leave it blank
---
Correct answer(s):
0
---
Feedback incorrect:
If you leave the parameter blank, Weka will put in the last value that was used

<-- 5.5 Quiz -->
Investigating multilayer perceptrons
Question 4
Now for the actual experiment.
In the Experimenter, use the datasets breast-cancer.arff, credit-g.arff, diabetes.arff, glass.arff, ionosphere.arff and iris.arff and three versions of MultilayerPerceptron: with no hidden layers, one hidden layer, and two hidden layers. Use the default number of nodes in each hidden layer, namely the average size of the input and output layers, by specifying “a”.
You will need to be patient: this took almost 1.5 hours on my computer. However, you will learn just as much if you restrict the datasets to iris alone (which took 20 secs), or to iris and ionosphere (which took 10 mins), provided you examine the answers and general conclusions given in the article that follows this quiz.
For which datasets does one or two hidden layers significantly outperform no hidden layer?
Select all the answers you think are correct.
breast-cancer
credit-g
diabetes
glass
ionosphere
iris
none
---
Correct answer(s):
ionosphere
---
Feedback correct:
For the ionosphere dataset both one and two hidden layers are significantly better than none

<-- 5.5 Quiz -->
Investigating multilayer perceptrons
Question 5
For which datasets does two hidden layers significantly outperform one hidden layer?
Select all the answers you think are correct.
breast-cancer
credit-g
diabetes
glass
ionosphere
iris
none
---
Correct answer(s):
none
---
Feedback correct:
Disappointing, eh?

<-- 5.5 Quiz -->
Investigating multilayer perceptrons
Question 6
Ignoring significance, for which datasets are zero hidden layers best?
Select all the answers you think are correct.
breast-cancer
credit-g
diabetes
glass
ionosphere
iris
none
---
Correct answer(s):
breast-cancer
credit-g
diabetes

<-- 5.5 Quiz -->
Investigating multilayer perceptrons
Question 7
Ignoring significance, for which datasets are one hidden layer best?
Select all the answers you think are correct.
breast-cancer
credit-g
diabetes
glass
ionosphere
iris
none
---
Correct answer(s):
glass
iris

<-- 5.5 Quiz -->
Investigating multilayer perceptrons
Question 8
Ignoring significance, for which datasets are two hidden layers best?
Select all the answers you think are correct.
breast-cancer
credit-g
diabetes
glass
ionosphere
iris
none
---
Correct answer(s):
ionosphere

<-- 5.5 Quiz -->
Investigating multilayer perceptrons
Question 8
Ignoring significance, for which datasets are two hidden layers best?
Select all the answers you think are correct.
breast-cancer
credit-g
diabetes
glass
ionosphere
iris
none
---
Correct answer(s):

<-- 5.6 Article -->
Performance of the multilayer perceptron
Let’s review the performance of multilayer perceptrons in the preceding quiz.
First, 2 hidden layers never significantly outperform 1 hidden layer.
Ignoring significance:
    2 hidden layers are best for 1 dataset
    1 hidden layer is best for 2 datasets
    0 hidden layers are best for 3 datasets.
It’s interesting to look at the time taken for training, UserCPU_Time_training. Overall, 2 hidden layers take up to twice as long as 1 hidden layer, which takes between 2 and 12 times as long as 0 hidden layers.
On 4 of the 6 datasets, all three versions of MultilayerPerceptron (0, 1, and 2 hidden layers) are outperformed by much faster methods:
    roundly outperformed by J48 on breast-cancer (74% correct vs 67% correct for the default configuration of MultilayerPerceptron)
    outperformed by SMO on credit-g and diabetes (75% vs 72%, and 77% vs 75% respectively)
    outperformed by IBk on glass (70% vs 67%).
MultilayerPerceptron (default configuration with 1 hidden layer) has two marginal successes:
    on iris, it’s a shade better than its nearest competitor, SMO (97% vs 96%)
    on ionosphere, it’s a shade better than its nearest competitor, AdaBoostM1 (91.1% vs 90.9%).

<-- 5.7 Article -->
 The deep learning renaissance
In the last couple of videos I’ve been a bit negative about perceptrons and multilayer perceptrons – and the preceding quiz hasn’t exactly made a good case for them!
But …
In recent years, so-called “deep learning” approaches to machine learning have been successfully applied to computer vision, natural language processing, and speech recognition. Deep learning involves multilayer networks with many layers. Ideally, the lower layers learn about low-level features (like lines and edges in an image), intermediate layers learn how to put them together (like arms and legs, noses and tails, wheels and faces), and upper layers learn how to combine these into objects (is it a car? a cat? a cow?). The key is to use “high-capacity” models – that is, ones with many parameters – with several layers. Multilayer neural networks are a prime example.
A critical ingredient is the use of overwhelmingly larger quantities of data than has heretofore been possible. In former times, data mining researchers paid scant attention to images and speech signals. Of course, the world abounds with signal data – but it is generally unlabeled. Recently, large collections of labeled data have been created, stimulating the application of deep learning techniques to tasks such as image recognition, face verification, speech and language models.
Images
The Large-scale visual recognition challenge is to classify images obtained from sources such as Flickr into 1000 possible object category classes. The training set contains 1.2M images, which were hand-labeled(!) based on the presence or absence of an object belonging to these categories. A random subset of 50,000 images (50 per class) was used as the validation set and 100,000 images (100 per class) as the test set. The test images are hidden to participants in the challenge and available only to the judges.
The percentage of times the target label is not one of the 5 highest-probability predictions is called the “top-5 error”. Classical machine learning methods struggle to get below 25% – far worse than human performance, which has been measured at 5.1% for the same test set. The 2015 challenge was won by a team from Microsoft Research Asia who achieved 3.6%, substantially outperforming humans. They used a multilayer architecture with 152 layers! (Don’t try setting this up in Weka.)
Faces
An important special case of object recognition, face recognition has been the subject of intense research for decades—and deep learning networks have transformed the field. If you have used photo apps that identify faces, you have probably been surprised (dismayed?) at how good they are. Controlled experiments on face verification, where the question is whether two mugshots belong to the same person or not, have yielded better-than-human performance.
More data gives even better results. Every time one of its 2 billion users uploads a photo to Facebook and tags someone, that person is helping the facial recognition algorithm – which, according to the company, is able to accurately identify a person 98% of the time. (According to the FBI, its own facial recognition technology identifies the correct person in a list of the top 50 people only 85% of the time.)
It is worth noting that this raises controversial ethical issues (apart from the fact that every user works for Facebook, for free). Federal governments deploy face verification technology in the fight against international terrorism; airports use it to reduce lineups at immigration. The application of face recognition in widespread video surveillance has a profound effect on the balance between security and privacy, and other civil liberties. At the individual level, stalkers exploit end-user web services for face recognition.
Language
Researchers at Google create language models based on single-hidden-layer networks trained with vast amounts of data – 30 billion words. One model trains a neural network to predict upcoming words given their context. Another predicts nearby words within a certain distance before and after the source word. Here the number of “classes” equals the vocabulary size, which ranges from 10^5 to 10^9 terms, so the output is decomposed into a binary tree – for a V-word vocabulary it is then necessary to evaluate only log2(V) rather than V output nodes.
Many research groups are mining massive quantities of text data in order to learn as much as possible from scratch, replacing features that have previously been hand engineered by ones that are learned automatically. Large neural networks are being applied to tasks ranging from sentiment classification and translation to dialog and question answering. Google uses deep learning techniques to learn how to translate languages from scratch, based on voluminous data.
Deep learning with Weka
It’s not feasible to do these things with Weka’s multilayer perceptron classifier. But there is
… good news ☺
The wekaDeeplearning4j package provides Weka classifiers and filters that invoke the popular Deeplearning4J Java library. It allows you to do advanced image recognition with deep networks involving “convolutional” layers, and experiment with language models like those mentioned above. It even allows you to deploy your network on specialist hardware, namely graphics processing units (GPUs), which can yield execution speeds that are orders of magnitude faster than standard implementations.
… and bad news ☹
Although no programming is needed to use the classifiers and filters, they have many configuration options. You need some understanding of deep learning, which involves specialist (difficult!) mathematics, and a great deal of trial and error in order to get good results. To introduce the subject of deep learning and show how to do it with Weka would take a whole course. Deep learning with Weka? Maybe.
Want to learn more?
I hate to self-advertise, but a recent book by myself and friends includes copious material on deep learning. Although pretty mathematical, by the standards of the field it’s a fairly accessible explanation of this area.
Data mining: Practical machine learning tools and techniques (4th edition), by Ian Witten, Eibe Frank, Mark Hall, and Christopher Pal. (Click here to order from Amazon.)
Note: this book is not required reading for this course.

<-- 5.8 Discussion -->
Share stories on deep learning
You hear a lot about amazing successes – and failures – of deep learning neural network techniques.
For example, adversarial examples are synthetic examples constructed by modifying real examples slightly in order to make a classifier believe they belong to the wrong class with high confidence – though whether they might make a difference in practice is controversial. You might want to think about that next time you step into a self-driving car.
Over to you – do you have any experiences or information to share with your fellow students about successes – and failures – of deep learning techniques?

<-- 5.9 Article -->
How much training data do I need? And how do I optimize all those parameters? 
Two questions loom large when embarking on a data mining project. First, how much training data is enough? And second, given that data mining algorithms generally have parameters, how do you find suitable values for them, having chosen the algorithm itself?
Good questions.
And Weka can help. By the end of the week you will be able to examine how performance improves as the volume of training data increases. You’d expect it to improve rapidly at first, subject to random fluctuations, and then continue to improve – but at a steadily decreasing rate – thereafter. That should help you determine how much data is enough to achieve your goals. As for the second question, you should avoid optimizing parameters manually: you’re bound to overfit! Instead, by the end of the week you will be able to use “wrapper” metalearners that Weka provides to optimize parameters for best performance.
And a final question, in this closing miscellany of things you need to know: what other things can be specified in the ARFF file format in which datasets are represented?

<-- 5.10 Video -->
Learning curves 
How much data do you need? There is no easy answer; it depends on many features of the problem and dataset. One way to estimate it is to plot a learning curve, and this can be done using the “resample” filter – along with the FilteredClassifier, to avoid resampling the test set (which is undesirable). The performance figures you obtain are estimates, and you can improve their reliability by repeating the experiment several times.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hello again! In data mining, people are always asking “how much data do I need?” We’re going to show you how you can address that question in this lesson using learning curves.
The advice on evaluation from Data Mining with Weka was: if you’ve got a large, separate test set, then just go ahead and use the test set. If you’ve got lots of data, use the holdout method. Otherwise, use 10-fold cross-validation – it’s the best way of getting the most reliable performance estimate out of a limited amount of data. You might repeat it 10 times or more, like the Experimenter does. But how much data is a lot?
Well, that’s a good question, and there is no answer: it depends. Supposing you’ve got 1000 instances. That sounds like quite a lot. If you’ve got a 2-class dataset with 500 of each class, then maybe that’s pretty good. If you’ve got a 10-class dataset with 1000 instances and the classes are unevenly distributed – so maybe for some classes there are only 10 or 15 instances – well, that doesn’t sound so good. Although perhaps you don’t care about those small classes. It depends on the number of attributes. Again, with your 1000-instance dataset, that sounds like a lot, but if you have 1000 attributes, that might not be such a lot of instances. It depends on the structure of the domain.
Are you looking for complicated decision boundaries? It depends on the kind of model, the sort of decision boundaries it makes. If you’ve got a machine learning technique that looks for linear decision boundaries, then they’re pretty simple. You might not need so much data as you would for ones that look for more convoluted linear boundaries, or for decision trees, perhaps. It’s an impossible question to answer. The only way to look at it really is to look at it empirically using learning curves. I’ve shown a plot here of a learning curve. As the size of the training data increases, the performance gets better and better, but of course, it asymptotes off.
The point where it starts to asymptote off is probably enough training data to get a reliable estimate. Let’s talk about how to plot a learning curve in Weka. We’re going to sample the data. When you do sampling, we’re going to choose a sample, and you need to understand the difference between sampling with replacement and sampling without replacement. When you sample, it’s really a question of whether you move or copy the data. If you sample “with replacement”, then it’s like you take it out of the original dataset and put it into the sample dataset, and then replace it back in the original dataset. You don’t really take it out.
You copy it from the sample data, the instance from the original dataset to the sample dataset. “Without replacement” means you move it. You can’t see it again; you can’t sample it twice. If you sample with replacement, then instances might occur more than once in the sample dataset. If you sample without replacement, then they can’t. That’s the first thing. We’re going to sample the training set, but not the test set. We want to find out how performance changes as the size of the training set increases. But the test set determines the reliability of our estimate – we don’t want to make that artificially smaller. We always want to use the same size test set.
We can do that in Weka by using the FilteredClassifier. There’s a Resample filter, and if we wrap that up in the FilteredClassifier, that means that the filtering will apply to the training data and not to the test data. I’m going to do that with the glass dataset. I’ve opened the glass dataset here. I’m going to go to Classify. In meta, I’m going to find the FilteredClassifier.
Then I’m going to check – I’m going to use J48 as the classifier. For the filter, I’m going to use the Resample filter.
It’s an unsupervised instance filter: we’re resampling instances. There it is. Here are the parameters. We can sample with or without replacement, and I would like to sample with no replacement, so I want to make that true. I want a 50% sample. I can go ahead and run that. I’m doing 10-fold cross-validation, sampling the training set, using a 50% sample of the training set and leaving the test set untouched. I get 65% performance. Back to the slide.
Here is the 50% level: 65% performance.
I did this for other sample sizes, which enabled me to plot this learning curve empirically: the performance against the percentage of training data I’m using. I’ve shown the ZeroR performance there, for reference. The line’s a bit jagged, and to get a smoother line I’d want to do it several times with cross-validation. If I do 10 repetitions of J48, then I get this line here. (I did this with the Experimenter. It’s very easy to do.) Then I did 1000 repetitions. I get this red line here, this smooth line. You can look at this line and make your own judgment as to how much training data you need to get pretty close to the ultimate accuracy of J48 on this dataset.
It looks like providing you have about maybe 50-60% of the training data, you’re going to be fairly close to the final accuracy. That’s it for learning curves. The question is how much data is enough? The answer is we don’t know! So you can plot a learning curve. We looked at resampling with and without replacement, but we didn’t want to sample the test set, because that would just decrease the reliability of evaluation. We used the FilteredClassifier. Obviously, the performance figures you get are only estimates, and you can improve the reliability of those estimates by repeating the test several times.
<End Transcript>

<-- 5.11 Quiz -->
Learning curves
Question 1
Before we begin, if you train on a certain percentage of a dataset using the FilteredClassifier with the Resample filter (as Ian did in the “Learning curves” video), would you expect better results if sampling was done with replacement or without replacement?
With replacement
Without replacement
---
Correct answer(s):
Without replacement
---
Feedback correct:
Sampling without replacement yields a greater variety of different instances, which will probably generate a more accurate classifier.

<-- 5.11 Quiz -->
Learning curves
Question 2
Open the classifier-performance.csv file in a spreadsheet program.
It gives the performance on 1000 repetitions of 10-fold cross-validation of 3 algorithms (Naive Bayes, J48, and IBk) on 5 datasets (breast-cancer.arff, glass.arff, credit-g.arff, diabetes.arff, ionosphere.arff), training on 100%, 90%, …, 20%, 10% of the instances (sampling without replacement).
Just to be sure that you understand how it was generated, the figures for a 50% sample of the breast-cancer dataset are omitted from the CSV file. What should they be, for Naive Bayes, J48, and IBk respectively?
72.42, 71.10, 70.22
71.34, 70.27, 69.78
72.13, 70.83, 70.84
---
Correct answer(s):
72.13, 70.83, 70.84
---
Feedback incorrect:
Are you sure you set the number of repetitions to 1000 in the Experimenter? – this is the result for 10 repetitions (the default)
---
Feedback incorrect:
Are you sure you are filtering without replacement (noReplacement = true)? – this is the result for sampling with replacement

<-- 5.11 Quiz -->
Learning curves
Question 3
Complete the classifier-performance spreadsheet by including the three new figures you have calculated. The remainder of this quiz involves examining this spreadsheet; you do not need to use Weka any more.
Suppose we are interested in the number of training instances (not the percentage of the dataset) required to achieve 99% of the “ultimate” performance of each classifier, defined as the performance obtained using 100% of the training data.
    Add a new column (M) to the spreadsheet that calculates 99% of that ultimate performance (for the first spreadsheet row that’s 72.08, or 99% of 72.81).
    Add a further column (N) that shows what percentage of the dataset is needed to better that performance (for the first spreadsheet row that’s 50%, because that yields a performance of 72.13, which is just better than 72.08). I did this part manually; it took me a few minutes.
    Add a column (O), again manually, that shows the number of instances in the dataset, which I found using the Explorer (286 for the first row).
    Add a column (P) that shows the number of instances corresponding to the percentage in column N (143 for the first row).
Now use your spreadsheet to answer the following questions.
Which dataset seems to be the “hardest”, in the sense that Naive Bayes, J48, and IBk require the greatest number of training instances in order to attain 99% of their ultimate performance?
breast-cancer
credit-g
diabetes
glass
ionosphere
---
Correct answer(s):
credit-g
---
Feedback correct:
Naive Bayes, J48, and IBk require 600, 500, and 900 instances respectively to reach 99% of their ultimate performance on credit-g – more instances than for any other dataset (except diabetes, but only in the case of J48)
---
Feedback incorrect:
J48 does require the largest number of instances (691) to reach 99% of its ultimate performance on diabetes, but the other two methods do not require nearly as many instances as they do for one of the other datasets.

<-- 5.11 Quiz -->
Learning curves
Question 4
Is there evidence that different methods require different amounts of data to reach 99% of their ultimate performance?
Yes
No
---
Correct answer(s):
Yes
---
Feedback correct:
IBk always requires the most data, except on the breast-cancer and diabetes datasets where J48 requires even more. Naive Bayes always requires the least data, except on the credit-g dataset where J48 requires even less.

<-- 5.11 Quiz -->
Learning curves
Question 5
What is the order of the methods in terms of the volume of data that is generally required to reach 99% of ultimate performance?
---
Correct answer(s):
IBK
J48
NaiveBayes

<-- 5.11 Quiz -->
Learning curves
Question 6
In a further column (Q) of the spreadsheet, calculate for each dataset the average number of instances needed by the three methods to achieve 99% of their ultimate performance.
Also note on the spreadsheet which datasets have two classes and which have more than two.
Does the data support the contention that the more class values there are, the more data is needed?
Definitely
Not really
---
Correct answer(s):
Not really

<-- 5.11 Quiz -->
Learning curves
Question 7
Note on your spreadsheet which datasets have 10 or fewer attributes and which have more than 10.
Does the data support the contention that the more attributes there are, the more data is needed?
Definitely
Not really
---
Correct answer(s):

<-- 5.11 Quiz -->
Learning curves
Question 7
Note on your spreadsheet which datasets have 10 or fewer attributes and which have more than 10.
Does the data support the contention that the more attributes there are, the more data is needed?
Definitely
Not really
---
Correct answer(s):
Not really
---
Feedback correct:
We conclude that it’s pretty hard to predict the amount of data required by a classifier to achieve close to ultimate performance – without actually plotting a learning curve.

<-- 5.12 Video -->
Performance optimization 
Machine learning methods often involve several parameters, which should be optimized for best performance. Optimizing them manually is tedious, and also dangerous, because you risk overfitting the data (unless you hold out some data for final testing). Weka contains three “wrapper” metalearners that optimize parameters for best performance using internal cross-validation.  CVParameterSelection selects the best value for a parameter; GridSearch optimizes two parameters by searching a 2D grid; and the ThresholdSelector selects a probability threshold.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
In data mining, you often want to optimize parameters for some situation, and I’m going to show you some methods in Weka that allow you to do that. These are “wrapper” meta-learners. There are three of them. Do you remember the AttributeSelectedClassifier with WrapperSubsetEval? The way it worked was to select an attribute subset based on how well a classifier performed, and it evaluated that using cross-validation. These do the same kind of thing. CVParameterSelection selects the best value for a parameter. Again, it uses cross-validation.
It can optimize various parameters: the accuracy or the root mean-squared error. GridSearch optimizes two parameters by searching a 2-dimensional grid. The ThresholdSelector selects a probability threshold and you can optimize various parameters with that. Let’s take a look first at CVParameterSelection. Over in Weka, I’ve got the diabetes dataset open, and I’m looking at J48.
Now, do you remember J48 has got these two parameters: “C” and “M”? We can optimize those. Let’s just run it. In plain mode, we get 73.8%. Now, we can optimize these parameters. Coming back to the slide. We can use CVParameterSelection. The way we express our optimization is to write a loop. The “C” parameter is going to go from 0.1 to 1 in [10 steps]. That will take it right up to 1.0. Actually, if you were to try this, you would find it would fail, because if C is set to 1, then J48 can’t cope with that. Instead, we’re going to use C goes from 0.1 to 0.9 in 9 steps.
To find out about this syntax, you need to use the More button. Let’s go back to Weka and do that.
I’m going to choose CVParameterSelection: it’s a meta-classifier. I’m going to wrap up J48.
My string is the C parameter is going from 0.1 to 0.9 in 9 steps.
I need to Add that. That’s it here. I’ll leave this and then go back and have another look. It still says the same thing. This is what it’s doing. This is the list you want. You can have several lines in this list. If I just go ahead and do that, then it will optimize that parameter. It will take quite a long time. I’m going to stop it now. I’m going to be disappointed, because actually, I’m going to get worse results. It’ll choose a value of C as 0.1 instead of the default of 0.2, and it’s going to get slightly worse results, only 73.4%. I’m going to get better luck with minNumObj, the other parameter, which is called M.
Let’s go back here. We’re going to go back and reconfigure CVParameterSelection.
I’m going to add another optimization: M goes from 1 to 10 in steps of 10. I’m going to Add that; it’s first – and then I’m going to do C. So I’m going to loop around M and get the best value for M, and then I’m going to loop around C and get the best value for C with that best value for M. I’m not going to do this; it takes a long time. But let me tell you the results. It gets 74.3% with C as 0.2 and M as 10. Actually, it gets a much simpler tree. We get a very slightly better result than with plain J48, and we get a simpler tree. That’s a worthwhile optimization.
The next method is GridSearch. You can do CVParameterSelection with multiple parameters, and it will optimize the first parameter and then the other parameter. GridSearch optimizes the two parameters together. It allows you to explore not just for a classifier, but the best parameter combinations for a filter and a classifier. You can optimize various things. It’s very flexible, but pretty complicated to set up. Let’s take a quick look at GridSearch. You would need to study this to actually use it. This is the configuration panel. You can see it’s pretty complex. We’re doing “x” and “y”. x is actually going to be the filter. We can optimize a number of components in the filter, the x property.
y is going to be the classifier, and we’re going to optimize the ridge parameter of the classifier. That’s in this default configuration. We’re using linear regression, which has got a ridge parameter. This is the parameter we’re optimizing. For the filter, we’re using partial least squares, and that’s got a parameter called numComponent. That’s what we’re going to be optimizing. That’s the default configuration. In order to change this configuration, then you’d need to look at the More button and think about this quite a bit. The third thing I want to look at is a threshold selector. Do you remember in the last class, we looked at probability thresholds, and we found that Naive Bayes uses a probability threshold of 0.5?
We fiddled around with that to optimize a cost matrix. That’s exactly the kind of thing that ThresholdSelector can optimize. In fact, in this case, it’s unlikely to do better than Naive Bayes, but we can do different things. I’m going to use the credit dataset and Naive Bayes. I’ve got them here, the credit database and Naive Bayes. I can just run that, and I’ll get 75.4% accuracy. Now, I can use the threshold selector. Let’s look at the ThresholdSelector. it’s a meta classifier, of course. I’m going to configure that with Naive Bayes. There are various things I can do.
The designated class: I’m going to designate the first class value. In this dataset, the class values are “good” and “bad”. The first class is the “good” class. Let me optimize the accuracy and see what happens. I get exactly the same 75.4% that I got before. We can actually optimize a number of different measures here, in fact, these measures, the TP_Rate and FP_Rate and so on. Back on the slide, there are some new terms here, the F-measure, Precision, and the Recall. Remember the confusion matrix? The TP is there, so that’s True Positive. True Negative (TN) is in the lower right-hand corner of the confusion matrix. The TP_Rate is TP divided by TP plus FN. We’ve talked about those before.
We haven’t talked about Precision, Recall, and F-measure, which are commonly used measures in the area of information retrieval. Those are defined there on the slide for you. Going back to Weka, let’s optimize something simple, like the number of true positives. Look – we’ve got 700 of them here, isn’t that fantastic? A very high number of true positives. Or we could change the classifier to optimize the number of true negatives. Here we get 295, a very high number of true negatives. The threshold value’s actually given here up at the top. You can see it’s chosen almost 1 here. It’s tuning on one third of the data is how it’s evaluating this.
We can optimize other things – Precision, Recall, and F-measure – as well as the accuracy. That’s it.
The moral is: don’t optimize parameters manually. If you do, you’ll overfit, because you’ll use the whole dataset in cross-validation. That’s cheating! We’re going to use wrapper methods using internal cross-validation. We’ve looked at CVParameterSelection, GridSearch, and ThresholdSelection.
<End Transcript>

<-- 5.13 Quiz -->
Optimizing IBk
Question 1
In the Explorer, determine the optimal parameter value for IBk between 1 and 10 for the breast-cancer.arff dataset using the CVParameterSelection meta-learning scheme with the CVParameter string set to “K 1 10 10”. What is it?
1
2
4
9
---
Correct answer(s):
4

<-- 5.13 Quiz -->
Optimizing IBk
Question 2
Set up the Experimenter to compare IBk with its default parameter of 1 with the optimal value between 1 and 10 found using the CVParameterSelection meta-learning scheme.
Use these 4 datasets: credit-g.arff, diabetes.arff, glass.arff, and ionosphere.arff. Note: the optimal parameter value will vary from one dataset to another, so be sure to set up the Experimenter to determine it individually for each dataset.
For which datasets does this optimization step produce a better result?
Select all the answers you think are correct.
credit-g
diabetes
glass
ionosphere
none
---
Correct answer(s):
credit-g
diabetes
ionosphere

<-- 5.13 Quiz -->
Optimizing IBk
Question 3
For which datasets is the result significantly better (5% level)?
Select all the answers you think are correct.
credit-g
diabetes
glass
ionosphere
none
---
Correct answer(s):
ionosphere

<-- 5.13 Quiz -->
Optimizing IBk
Question 4
For which datasets is the result significantly worse?
Select all the answers you think are correct.
credit-g
diabetes
glass
ionosphere
none
---
Correct answer(s):

<-- 5.13 Quiz -->
Optimizing IBk
Question 4
For which datasets is the result significantly worse?
Select all the answers you think are correct.
credit-g
diabetes
glass
ionosphere
none
---
Correct answer(s):
none

<-- 5.14 Video -->
ARFF and XRFF 
Remember the ARFF format? – we’ve been using it all along. But it’s more powerful than you have seen. For example, it can encode sparse data, which often greatly reduces the size of dataset files – and some filters and classifiers use sparse data directly, which can make them very fast (for example, StringToWordVector and Multinomial Naive Bayes, which you have used already). ARFF can represent weighted instances, data attributes, and relational attributes. Furthermore, there is an XML version called XRFF, which the Explorer can read and write.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
I just wanted to go right back to the beginning and talk about the ARFF format a little bit more. Remember, an ARFF file starts out with an @relation to name the relation, and then some “@” attribute statements, one for each attribute. It declares them to be nominal, in which case it gives the values, or numeric. Integer or real, it’s the same thing – they’re all numeric for Weka. There are also string attributes. Then there’s an @data line, and following that, for each instance there’s one data line. We use question mark for missing values. And of course there are comment lines, beginning with %. Well, you know all that, but there are a few more things that you don’t know.
First of all, we can have sparse ARFF files. There’s a sparse format, and filters NonSparseToSparse and SparseToNonSparse. Here’s an example of the weather data, first of all in the regular format. Both the sparse and the regular format have the same header. On the left is the regular format, on the right is the sparse format. In the first instance, which is “sunny, hot, high, false, no”, well, in the sparse format, if the attribute has the first value, then that’s considered the default. “Sunny”, “hot”, and “high” are all default. So the first instance in the sparse format is 3, attribute number 3 (we count from 0). Attribute number 3 is “false” and 4 is “no”.
In the second instance, “sunny, hot, high, true, no”; “sunny”, “hot”, “high”, and “true” are all default. Those are all the first possible values as declared in the ARFF header, so we don’t need to specify those. We just specify that the 4th attribute – numbering again from 0 – is a “no”. And the third instance, “overcast”, well, that’s not the first value for “outlook”, so we’ve got to specify that, so we say the 0th [attribute] is “overcast”. Then “hot”, “high”, and “yes” are all default, but “false” isn’t, so we say the 3rd attribute is “false”.
And so we go on: just specify those attributes that do not have the first value. All classifiers accept sparse data as input, but some of them just nullify the savings by expanding the sparse data internally. Others actually use sparsity to speed up the computation. Good examples are NaiveBayesMultinomial and SMO. There are a couple of filters – the StringToWordVector, for example – that produce sparse output. So if you use the StringToWordVector filter in combination with Multinomial Naive Bayes, you get a very fast system, and you probably noticed that when you were doing document classification. There are a couple of other features like weighted instances. We’ve talked now and again about instances being weighted internally to Weka.
You can specify weighted instances in ARFF files in curly brackets at the end of the instance. Again, with the weather data, we’ve got a couple of instances and the first instance has got a weight of 0.5 and the second instance has got a weight of 2.0. If weights are missing, of course, they’re assumed to be 1.0. You can specify weights explicitly in your ARFF file. There are also date attributes. I won’t go into the format. You can have relational attributes, which are really intended for multi-instance learning, which we haven’t touched upon in this course. There’s an XML version of the ARFF format called XRFF (I don’t know how to pronounce that). The Explorer can read and write XRFF files.
It’s very verbose. Here’s an example. We’ve got a header, and then at the end of the header, we’ve got the body. The header contains the ARFF header, and the body contains the data, the instances. In the header, there’s a bit for each attribute where it specifies the name of the attribute and the type of the attribute, and if it’s a nominal attribute, the possible labels for it, for each attribute.
In the body, we say <instances>, and within <instances>, we have <instance>: define the first instance, define the attribute values. Then we would follow that with another instance defined in the second <instance>, and so on. It’s the same information as in ARFF files. It’s clearly very verbose. You can have instance weights, as you can with ARFF files. You can do a little bit more than you can with ARFF files. In the XML format, you can specify which is the “class” attribute – remember, Weka assumes by default that the last attribute is the class. There’s no way to change that in an ARFF file, but there is in an XRFF file. You can also specify attribute weights to have weighted attributes.
There’s a compressed version of this: .xrff.gz. The Explorer can read and write those files, as well. So you should know about that. That’s it.
ARFF has some extra features that you didn’t know about: the sparse format, instance weights, date attributes, and relational attributes. Some filters and classifiers take advantage of the sparsity to operate more efficiently in both time and space. XRFF is an XML equivalent of ARFF, plus some additional features.
<End Transcript>

<-- 5.15 Quiz -->
Advanced ARFF
Question 1
With this dataset, how many yes and how many no instances are reported on the Preprocess panel?
Select all the answers you think are correct.
6 yes instances
7 yes instances
9 yes instances
5 no instances
7 no instances
8 no instances
---
Correct answer(s):

<-- 5.15 Quiz -->
Advanced ARFF
Question 1
With this dataset, how many yes and how many no instances are reported on the Preprocess panel?
Select all the answers you think are correct.
6 yes instances
7 yes instances
9 yes instances
5 no instances
7 no instances
8 no instances
---
Correct answer(s):
9 yes instances
5 no instances

<-- 5.15 Quiz -->
Advanced ARFF
Question 2
Process the data with any classifier and examine the confusion matrix. How many yes and how many no instances are represented in the confusion matrix?
Select all the answers you think are correct.
0 yes instances
9 yes instances
45 yes instances
0 no instances
2.5 no instances
5 no instances
---
Correct answer(s):
45 yes instances
2.5 no instances

<-- 5.15 Quiz -->
Advanced ARFF
Question 3
Examine the dataset using the Preprocess panel’s Edit button. Are the weights displayed?
Yes
No
---
Correct answer(s):
Yes
---
Feedback correct:
They’re shown in the second column, just after the instance number

<-- 5.15 Quiz -->
Advanced ARFF
Question 4
[Note: Weka 3.8.2 has a bug that will not let you save a weighted ARFF file, so please skip this question]
Save the dataset in a new ARFF file and examine it with a text editor. Does it contain the weights?
Yes
No
---
Correct answer(s):
Yes

<-- 5.15 Quiz -->
Advanced ARFF
Question 5
[Note: Weka 3.8.2 has a bug that will not let you save a weighted ARFF file, so please skip this question]
Save the data into a .csv file. Does it contain the weights?
Yes
No
---
Correct answer(s):
No

<-- 5.15 Quiz -->
Advanced ARFF
Question 6
Load the original weather.nominal.arff file and convert it to sparse format using the NonSparsetoSparse filter. Save it in a new ARFF file and take a look.
How large are the original weather.nominal file and the sparse version respectively (in bytes)?
587 and 592
589 and 598
587 and 606
592 and 612
---
Correct answer(s):
587 and 606
---
Feedback correct:
These are the sizes under Windows
---
Feedback correct:
These are the sizes on the Mac

<-- 5.15 Quiz -->
Advanced ARFF
Question 7
Word vectors for large text files are a prime candidate for sparse representation, because most words do not occur in most documents.
Use the StringtoWordVector filter to convert the ReutersCorn-train.arff file into a word vector, and save it. How big is the file?
584 KB
589 KB
592 KB
597 KB
---
Correct answer(s):
597 KB
---
Feedback correct:
This is the size on a Mac. (Macs use 1 KB = 1000 bytes.)
---
Feedback correct:
This is the size under Windows. (Windows uses 1 KB = 1024 bytes.)

<-- 5.15 Quiz -->
Advanced ARFF
Question 8
StringToWordVector produces a sparse representation, so convert it to non-sparse and save that.
How big is the non-sparse file?
6 MB
6,840 KB
7 MB
7,087 KB
---
Correct answer(s):
6,840 KB
---
Feedback correct:
This is the size under Windows
---
Feedback correct:
This is the size on a Mac

<-- 5.15 Quiz -->
Advanced ARFF
Question 9
We now compare the speed of various classifiers on sparse and non-sparse input.
For the two word-vector files you’ve just produced, use the Explorer to move the class attribute (currently the first attribute) to the end. Do this by applying the Reorder filter with parameter “2-last,1”. This moves all attributes from the second to the last (inclusive) to the beginning, i.e. starting at position 1.
In the Experimenter, classify these two files with Naive Bayes, Multinomial Naive Bayes, J48, and SMO. This took 15–20 mins on my computer using the Experimenter’s default settings, but you’ll get much the same result far more quickly if you reduce the number of repetitions from 10 to 1.
How does the User CPU time for training Naive Bayes on the sparse representation compare with training it on the non-sparse representation?
About the same
A bit slower
2–3 times slower
More than 3 times faster
None of the above
---
Correct answer(s):
2–3 times slower
---
Feedback correct:
Naive Bayes does not take advantage of the sparse representation

<-- 5.15 Quiz -->
Advanced ARFF
Question 10
How does the User CPU time for training Multinomial Naive Bayes on the sparse representation compare with training it on the non-sparse representation?
About the same
A bit slower
2–3 times slower
More than 3 times faster
None of the above
---
Correct answer(s):
More than 3 times faster
---
Feedback correct:
Multinomial Naive Bayes takes advantage of the sparse representation

<-- 5.15 Quiz -->
Advanced ARFF
Question 11
How does the User CPU time for training J48 on the sparse representation compare with training it on the non-sparse representation?
About the same
A bit slower
2–3 times slower
More than 3 times faster
None of the above
---
Correct answer(s):
A bit slower
---
Feedback correct:
J48 does not take advantage of the sparse representation

<-- 5.15 Quiz -->
Advanced ARFF
Question 12
How does the User CPU time for training SMO on the sparse representation compare with training it on the non-sparse representation?
About the same
A bit slower
2–3 times slower
More than 3 times faster
None of the above
---
Correct answer(s):
More than 3 times faster
---
Feedback correct:
SMO takes advantage of the sparse representation.
In fact, if you’re applying Multinomial Naive Bayes or SMO to the non-sparse file you have just produced, it’s 2 to 5 times faster to use the FilteredClassifier to convert it into sparse representation and apply the classifier to that – even though with 10-fold cross-validation this runs the filter 10 separate times.

<-- 5.16 Video -->
Summary
There’s no magic in data mining – no universal “best” method. It’s an experimental science. This video reviews what this course has covered, and points out many things that it hasn’t covered. Finally, data mining is a powerful technology – please use it wisely.
© University of Waikato, New Zealand. CC Creative Commons Attribution 4.0 International License.
<Start Transcript>
Hi! Well, they say all good things come to an end, and this is the end of More Data Mining
with Weka: the last class. Let’s just summarize a few things here. This summary is actually from the previous course, Data Mining with Weka. These are the main messages I wanted to convey there, and it’s the same main messages this time. There’s no magic in data mining. There’s no single universal “best method”. It’s an experimental science. Weka makes it easy to experiment, especially now that you know how to use the Experimenter. But there are many pitfalls, and many ways to go wrong. You really need to understand what it is that you’re doing, and be focused a lot on evaluation and statistical significance using the Experimenter. I talked more about all of these points at the end of the last course.
You could go back and look at that video if you’d like some more expansion on these. This slide is also from the last course.
This is what we missed from Data Mining with Weka: filtered classifiers, working with cost matrices, selecting attributes, clustering, association rules, text classification, and using the Experimenter. These should all sound very familiar to you, because we’ve talked about them all extensively in this course. Plus more besides! We talked about big data. You experienced big data. We talked about the Command Line Interface; the Knowledge Flow Interface; streaming data using the Command Line Interface through NaiveBayesUpdateable; discretization and discretization filters; the difference between rules and trees – the similarities and differences; Multinomial Naive Bayes for text classification.
We had a little look at neural networks: the simple Perceptron and the Multilayer Perceptron. We learned about ROC curves, and learning curves, and some more stuff about the ARFF format and the XML version of it. We’ve done a lot. You’ve done a lot, actually, and I congratulate you on having got this far. This has been pretty intensive stuff. You’ve learned a lot about a lot of important things. Of course, there’s always more!
Time series analysis is a really important area: how to do data mining on time series.
Stream-oriented algorithms: NaiveBayesUpdateable is stream-oriented, but there exist stream-oriented versions of other algorithms, like decision tree methods. They’re in a MOA package, Massive Online Analysis, also from the University of Waikato. Multi-instance learning, where it’s not single instances, but bags containing several instances that are labeled positive or negative. One-class classification, where you don’t have any information about the negative class, just about the positive class. That makes things very difficult, but there are some things you can do. Other data mining packages. There’s a package called R, which has a lot of excellent resources. Actually, you can interface to this from Weka, so Weka can take advantage of those resources.
Also, there’s the LibSVM package for support vector machines and the LibLinear package for linear classification. They can all be reached through the Weka interface with the appropriate wrapper package. There’s a distributed version of Weka with the Hadoop system for distributing processing. Finally, there’s a technique called “latent semantic analysis” that you really need to know about to work on text classification. All of these things are available as packages for Weka. Here are just a few, final remarks. Data mining is really important. They’re talking about data as the new oil. The economic and social importance of data mining will rival that of the oil economy – some people say by 2020; it might be happening as we speak. You’re right in there.
Data mining is a wonderful thing to know about. It’s an exploding field. It will continue to explode. Personal data is becoming a new economic asset class. You know, it used to be that the data revolution, the internet revolution, was about our ability to learn stuff from the internet. You know, Wikipedia and all the things you can learn. But a lot of it now is about personal data, our own personal data and the economic importance of that. We need a lot more trust than we have at the moment between individuals and governments and the private sector in order to take full advantage of this new, economic asset. We had a lesson on ethics in the last course.
We haven’t had a lesson on ethics here, but it’s just as important. I would urge you to think ethically whenever you’re working with data. “A person without ethics is a wild beast loosed upon this world,” Albert Camus said. I don’t want to loose a whole bunch of wild beasts through this course. So please think of ethics and what is ethical and the right kind of thing to do when you’re working with other people’s data.
Finally, wisdom: you know, the value attached to knowledge. This is the really important thing.
Jimi Hendrix is supposed to have said: “knowledge speaks, but wisdom listens”, which is worth pondering. I’ve enjoyed giving this course, and I hope maybe I’ll meet you again in another version of this course. But for now, I’m just going to relax and play some music while you do the assessment. Bye for now!
<End Transcript>

<-- 5.17 Quiz -->
How to cheat: A case study
Question 1
The sentiment.arff dataset contains 4995 tweets, classified as pos (2481 instances) and neg (2514 instances) according to whether they were followed by a positive smiley, e.g. :-), or a negative one, e.g. :-(. It has already been processed by the StringToWordVector filter; the attributes are word occurrences (1 or 0) and the last attribute (sentiment) is the class.
It is hard to improve much on the baseline accuracy of 50%; Multinomial Naive Bayes achieves 64%. In this series of questions you will produce a manipulated dataset of exactly the same size on which ZeroR and OneR perform the same as before, but Multinomial Naive Bayes achieves a substantially higher success rate.
Imagine a corrupt data miner, encouraged to improve upon the above-mentioned 64% under pressure from his boss. To do this he cheats by using the RemoveMisclassified filter, setting maxIterations to 1, the classifier to NaiveBayesMultinomial, and leaving the other parameters at their default values. This reduces the dataset to 3233 instances.
What is the performance of Multinomial Naive Bayes on this reduced dataset?
57%
64%
84%
89%
95%
99%
100%
---
Correct answer(s):
95%

<-- 5.17 Quiz -->
How to cheat: A case study
Question 2
Further lured by promises of lucrative bonuses, our (anti-)hero re-applies RemoveMisclassified with maxIterations set to 0, which reduces the dataset to 3056 instances.
What is the performance of Multinomial Naive Bayes now?
57%
64%
84%
89%
95%
99%
100%
---
Correct answer(s):
99%

<-- 5.17 Quiz -->
How to cheat: A case study
Question 3
Fearing that his boss will smell a rat because the new dataset is much smaller than the one he was given, our (anti-)hero uses the Resample filter with a carefully calculated sampleSizePercent to restore the dataset to the original 4995 instances (he has to think about how to set the noReplacement parameter to achieve this).
How many pos instances are there in this dataset?
1528
1882
2896
3056
---
Correct answer(s):
1882

<-- 5.17 Quiz -->
How to cheat: A case study
Question 4
ZeroR and OneR yield 62% and 67% accuracy on this new dataset, very different from the 50% and 52% that they achieved on the original one, and our (anti-)hero fears that, noting the phenomenal accuracy of Multinomial Naive Bayes, his boss will run benchmark tests and notice the discrepancy.
He hits on a plan to create a new dataset with exactly 2481 pos and 2514 neg instances, just as in the original dataset, which will therefore yield the exact same result from ZeroR. He starts with the 3056-instance dataset generated in Q.2, uses a suitable filter to remove all neg instances and then another filter to increase the instances to the desired number, and saves the dataset. He repeats the procedure, this time removing all pos instances; and then combines the two datasets using a text editor.
Applying ZeroR to the resulting dataset yields exactly the same result as for the original one. But OneR does not. What is its performance?
57%
64%
84%
89%
95%
99%
100%
---
Correct answer(s):
57%

<-- 5.17 Quiz -->
How to cheat: A case study
Question 5
To reduce OneR’s performance to that on the original dataset, our (anti-)hero decides to identify the attribute that OneR chooses for the new dataset, remove it, and continue until its performance (rounded to the nearest integer) is 52%.
In fact, he has to remove 8 attributes. What are they (in order of removal)?
good, user, url, love, happy, lol, great, awesome
lol, good, great, url, happy, love, work, awesome
great, work, lol, awesome, user, love, happy, url
work, happy, user, url, love, awesome, lol, great
lol, good, awesome, great, user, url, love, work
---
Correct answer(s):
good, user, url, love, happy, lol, great, awesome

<-- 5.17 Quiz -->
How to cheat: A case study
Question 6
Multinomial Naive Bayes’s performance deteriorates substantially on this latest dataset – though it’s still a great deal better than the original 64%.
What is it?
69%
73%
79%
84%
89%
95%
---
Correct answer(s):
84%

<-- 5.17 Quiz -->
How to cheat: A case study
Question 6
Multinomial Naive Bayes’s performance deteriorates substantially on this latest dataset – though it’s still a great deal better than the original 64%.
What is it?
69%
73%
79%
84%
89%
95%
---
Correct answer(s):

<-- 5.18 Discussion -->
Corruption and cheating
I do not advocate cheating.
Quite the reverse! – as I hope I have made clear by urging ethics and integrity in the previous video. But cheating has been mentioned several times in this course because you need to be aware, and guard against it.
The preceding quiz demonstrated how easy it is for dataminers to cheat, and conceal the traces of what they’ve done. The problem was to classify tweets as positive and negative, and an appropriate dataset was supplied. It’s hard to improve much on the baseline accuracy of 50%. Multinomial Naive Bayes achieves 64%, and ZeroR and OneR get 50% and 52% respectively (using cross-validation  throughout). The scenario envisaged a corrupt data miner, encouraged to improve upon 64% under pressure from his boss and seduced by lucrative bonuses.
If you completed the Quiz, you ended up with a manipulated dataset of exactly the same size on which ZeroR and OneR perform the same as before, but Multinomial Naive Bayes scores a substantially higher success rate of 84%.
Will this fool his boss? Probably. The dataset size and ZeroR and OneR baseline checks provide strong evidence that the dataset is unchanged. (However, if the boss were to sort it he might notice many duplicates, because of how the Resample filter was used. If our anti-hero had anticipated this, could you advise him how this flaw might be overcome?)
Last week also contained a discussion on cheating.
What I invite you to do now is contribute your thoughts on the kind of cheating discussed last week and what we saw in the preceding Quiz. Similarities and differences? Which is likely to be more dangerous?

<-- 5.19 Discussion -->
Reflect on this week's Big Questions
The big questions this week are, “What are neural networks and how can I use them?”, and (in general, not just for neural networks) “How much training data do I need?” and “How do I optimize all those parameters?”
We promised that by the end you’d know exactly what the individual units of so-called “neural networks” do and how they are connected together. You’d know where the learning comes in, and  what decision boundaries these systems can create. And you’d be able to set up neural networks in Weka and evaluate how well they work. You’d also know that you’ll need to be patient! – multilayer networks take a long time to run. (I hope you also agree that the sex appeal of the brain analogy diminishes radically when the details are revealed.)
We also promised that you’d know how to estimate how much training data is necessary to achieve a certain level of performance by examining how performance improves as the volume of data increases. And you’d be able to use “wrapper” metalearners that Weka provides to optimize parameters for best performance.
Finally, you’d also have a richer understanding of what is possible in the ARFF file format in which datasets are represented.
Well, now it’s time to take up our final challenge to explain some of these things to a colleague.
Tell your fellow learners how you get on!

<-- 5.20 Quiz -->
Post-course assessment
Question 1
In the Explorer, run OneR on the weather.nominal.arff data, evaluated using 10-fold cross-validation, and output the predictions, along with the value of the first attribute (outlook).
[Hint: Under More options …, change Output Predictions from Null to PlainText, and then click on the word “PlainText” and set the attributes field to 1. This prints the value of the first attribute alongside each prediction.]
Number these predictions from 1 to 14 in the order that they are output. Ignore the column headed actual.
The output also shows the classifier model generated from the full training set, which branches on the outlook attribute, so for each prediction you can deduce what that model would predict.
Four of the 14 predictions made by the cross-validation models do not agree with the predictions made by the classifier model for the full training set. Which ones are they?
Select all the answers you think are correct.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
---
Correct answer(s):
6
8
9
12

<-- 5.20 Quiz -->
Post-course assessment
Question 2
The reason that four predictions do not agree with the model generated from the full training set is that OneR makes different models in different folds of the cross-validation.
Use the KnowledgeFlow interface to determine the 10 cross-validation models that OneR makes for the weather.nominal.arff dataset. Model numbers appear in the Result list in parentheses (at the end of each line). Four models are different from the one generated from the full training set. Which ones?
Select all the answers you think are correct.
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
---
Correct answer(s):
(3)
(4)
(5)
(8)

<-- 5.20 Quiz -->
Post-course assessment
Question 3
In order to relate the predictions printed by the Explorer to instances in the dataset, the AddID filter can be used to add an identification attribute to each instance.
Unfortunately, this often causes the classifier to produce a different model (e.g., try it with OneR on the nominal weather data).
However, you can effectively disable an attribute for the purposes of classification by using the FilteredClassifier with the Remove filter, appropriately configured.
Now for the question: with the iris.arff dataset, J48 makes one error on the first cross-validation fold (it is the 15th instance in the “output predictions” list). Use the above technique to determine the instance number in the original data set that this error corresponds to.
44
71
78
84
107
---
Correct answer(s):
84

<-- 5.20 Quiz -->
Post-course assessment
Question 4
Using the Explorer in Week 4, you achieved 99.7% cross-validation accuracy on the ReutersCorn-train.arff dataset using Naive Bayes and the FilteredClassifier, with the StringToWordVector filter with case-folding enabled (lowerCaseTokens = true), and selecting the top 2 attributes using the Gain Ratio attribute evaluator combined with the Ranker search method, wrapped up inside the AttributeSelectedClassifier.
What is the corresponding accuracy for the ReutersGrain-train.arff dataset? (You are advised to check the classifier configuration first by repeating the earlier experiment.)
96%
97%
98%
99%
99.5%
99.7%
---
Correct answer(s):
98%

<-- 5.20 Quiz -->
Post-course assessment
Question 5
Repeat the experiment of Question 4 on the same dataset but retaining the top 4 attributes instead of 2. What is the accuracy now?
96%
97%
98%
99%
99.5%
99.7%
---
Correct answer(s):
99%

<-- 5.20 Quiz -->
Post-course assessment
Question 6
In the Explorer, cluster the diabetes.arff dataset using SimpleKMeans (with default parameters); ensure that you exclude the class attribute to get a realistic clustering.
What is the within-cluster sum of squared errors?
91
115
121
150
---
Correct answer(s):
121

<-- 5.20 Quiz -->
Post-course assessment
Question 7
In the Explorer, do a classification-by-clustering evaluation of SimpleKMeans, XMeans, and EM on the diabetes dataset, constraining the number of clusters to 2, and compare with ZeroR as a baseline. (You will need to install the classificationViaClustering and XMeans packages if you haven’t already done so.)
In terms of classification accuracy, what order do the methods come in?
ZeroR < EM < SimpleKMeans = XMeans
ZeroR < SimpleKMeans < XMeans = EM
EM < ZeroR < SimpleKMeans < XMeans
EM = XMeans < SimpleKMeans < ZeroR
SimpleKMeans = ZeroR < XMeans = EM
---
Correct answer(s):
EM = XMeans < SimpleKMeans < ZeroR

<-- 5.20 Quiz -->
Post-course assessment
Question 8
Load the bankdata.arff dataset into the Explorer.
Prepare it for association rule mining by Apriori, which does not handle numeric attributes, by converting the children attribute using the NumericToNominal filter, and discretizing (unsupervised discretization) both age and income into three equal-width ranges, expressed in terms of binary attributes.
Apply Apriori. Which of the original attributes do the top 10 rules predict?
Select all the answers you think are correct.
age
income
married
pep
sex
---
Correct answer(s):
age
income

<-- 5.20 Quiz -->
Post-course assessment
Question 9
Suppose that in the diabetes dataset, misclassifying a tested_positive instance as tested_negative is twice as costly as misclassifying a tested_negative instance as tested_positive.
Use cost-sensitive evaluation with:
    (a) the Logistic classifier
    (b) the Logistic classifier using cost-sensitive classification (minimizeExpectedCost = true)
    (c) the Logistic classifier using cost-sensitive learning (minimizeExpectedCost = false).
What are the three costs?
Select all the answers you think are correct.
(a) 175
(a) 256
(a) 261
(a) 290
(a) 635
(b) 175
(b) 256
(b) 261
(b) 290
(b) 635
(c) 175
(c) 256
(c) 261
(c) 290
(c) 635
---
Correct answer(s):
(a) 290
(b) 256
(c) 261

<-- 5.20 Quiz -->
Post-course assessment
Question 10
Use the Explorer’s dataset generator to create 5 sets of LED24 data with 100, 200, 300, 400, and 500 instances respectively. Then use the Experimenter to compare the performance of J48, Naive Bayes, and SMO on them (with the default 10-fold cross-validation repeated 10 times).
Which of these statements do you agree with?
Select all the answers you think are correct.
Naive Bayes significantly outperforms J48 on 1 dataset
Naive Bayes significantly outperforms J48 on 2 datasets
Naive Bayes significantly outperforms J48 on 3 datasets
Naive Bayes outperforms J48 on 4 datasets
Naive Bayes outperforms J48 on 5 datasets
SMO significantly outperforms J48 on no datasets
SMO significantly outperforms J48 on 1 dataset
SMO significantly outperforms J48 on 2 datasets
SMO outperforms J48 on 3 datasets
SMO outperforms J48 on 4 datasets
SMO outperforms J48 on 5 datasets
---
Correct answer(s):
Naive Bayes significantly outperforms J48 on 1 dataset
Naive Bayes outperforms J48 on 4 datasets
SMO significantly outperforms J48 on no datasets
SMO outperforms J48 on 3 datasets

<-- 5.21 Article -->
Farewell
Thanks for taking this course. We hope you’ve enjoyed it.
This course has extended your knowledge and experience of practical data mining, following on from Data Mining with Weka.
We’ve shown you how to run experiments in Weka’s Experimenter; how to set up knowledge flows; how to deal with datasets of unlimited size in the command line interface. You’ve learned about mining text, filtering using supervised and unsupervised filters, discretization and sampling, attribute selection, classification rules, rules vs. trees, association rules, clustering, cost-sensitive evaluation and classification.
We claimed that after completing the course you’d be able to mine your own data using even more more powerful methods, in more subtle and sophisticated ways – and understand what it is that you are doing!
What do you think? How did you get on? What did you learn? What did you struggle with? Have your views on data mining changed? What do you want to learn next? Please let us know by filling in the post-course survey. We welcome your suggestions on what could be improved!
This course will run again soon. Please tell anyone you think might benefit from it.
No matter where you go, there you are. (Sometimes attributed to Confucius)

<-- 5.22 Article -->
Index
(A full index to the course appears at the end of Week 1.)
      Topic
      Step
      Datasets
      Breast-cancer
      5.2, 5.3, 5.4, 5.5, 5.11, 5.13
      Credit-g
      5.2, 5.3, 5.4, 5.5, 5.11, 5.13
      Diabetes
      5.2, 5.3, 5.4, 5.5, 5.11, 5.12, 5.13
      Glass
      5.4, 5.5, 5.10, 5.11, 5.13
      Ionosphere
      5.2, 5.3, 5.4, 5.5, 5.11, 5.13
      Iris
      5.3, 5.4, 5.5
      Reuters-Corn-train/test
      5.15
      Sentiment
      5.17
      Weather
      5.4, 5.14, 5.15
      Classifiers
      IBk
      5.4, 5.11, 5.13
      J48
      5.3, 5.4, 5.10, 5.11, 5.12, 5.15
      MultilayerPerceptron
      5.4, 5.5
      NaiveBayes
      5.3, 5.4, 5.11, 5.15
      NaiveBayesMultinomial
      5.14, 5.15, 5.17
      OneR
      5.4, 5.17
      SMO
      5.2, 5.3, 5.4, 5.14, 5.15
      VotedPerceptron
      5.2, 5.3, 5.4
      ZeroR
      5.4, 5.10, 5.17
      Metalearners
      AdaBoostM1
      5.4
      CVParameterSelection
      5.12, 5.13
      FilteredClassifier
      5.10, 5.11
      GridSearch
      5.12
      ThresholdSelector
      5.12
      Filters
      NonSparseToSparse
      5.14, 5.15
      RemoveMisclassified
      5.17
      RemoveWithValues
      5.3
      Resample
      5.10, 5.11, 5.17
      SparseToNonSparse
      5.14
      StringToWordVector
      5.14, 5.15
      Plus …
      Experimenter interface
      5.3, 5.4, 5.5, 5.10, 5.13, 5.15

