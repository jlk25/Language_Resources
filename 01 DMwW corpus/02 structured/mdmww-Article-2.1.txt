How can you discretize numeric attributes?
Converting numeric attributes to nominal is called “discretization”.
But wait! Why would you want to do this? Well, for one thing, some machine learning methods only work on nominal attributes. For another, a model like a decision tree that branches on nominal values like very_big, big, medium, small, very_small may be easier to understand than one that uses numbers. Also, it may actually be better to determine split-points using global information from the whole dataset rather than individually for each branch of the tree.
Supervised discretization is when you take class information into account when determining the split-points. This might produce better split points. But it introduces a subtle but crucial issue concerning the use of class information when creating a model, for then what should you do when faced with with test data that is (of course) completely unlabelled? This important issue transcends discretization, but it’s easier to grasp in the specific context of discretization.
At the end of this week you will be able to explain various discretization strategies: equal width and equal frequency; unsupervised and supervised. You will be able to discretize in a way that preserves the ordering information inherent in numeric attributes, even though the resulting nominal attributes have no intrinsic ordering. You will appreciate why pre-discretization might be better than building the same discretization method into a classifier—and why it might work the other way round! And you will be able to use Weka’s FilteredClassifier to fairly evaluate a classification method that involves supervised discretization.
